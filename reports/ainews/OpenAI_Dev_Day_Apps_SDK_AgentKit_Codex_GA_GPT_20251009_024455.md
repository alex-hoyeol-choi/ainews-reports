# OpenAI Dev Day: Apps SDK, AgentKit, Codex GA, GPT‑5 Pro and Sora 2 APIs | AINews
**원문 URL**: https://news.smol.ai/issues/25-10-06-devday
**번역일**: 2025-10-09 02:44:55

## 📋 메타데이터
- **발행일**: 2025-10-07T05:44:39.731Z
- **설명**: **OpenAI** showcased major product launches at their DevDay including the **Apps SDK**, **AgentKit**, and **Codex** now generally available with SDK and enterprise features. They introduced new models such as **gpt-5-pro**, **gpt-realtime-mini-2025-10-06**, **gpt-audio-mini-2025-10-06**, **gpt-image-1-mini**, and **sora-2** with a pro variant. The Apps SDK enables embedding interactive apps inside ChatGPT with partners like **Canva**, **Figma**, **Zillow**, and **Coursera**. AgentKit offers a full stack for building and deploying production agents with tools like ChatKit and Guardrails. Codex supports speech and controller-driven coding, credited with high internal shipping velocity. Pricing for GPT-5 Pro was revealed at $15 input and $120 output per million tokens. *"OpenAI turned ChatGPT into an application platform"* and *"AgentKit built a working agent in under 8 minutes"* were highlights.

---

## 📰 번역된 내용

OpenAI면 충분할지도 모릅니다.
> 2025년 10월 3일~10월 6일 AI 뉴스입니다. 저희는 12개의 subreddit, 544개의 Twitter 계정, 23개의 Discord (196개 채널, 20085개 메시지)를 확인했습니다. 절약된 예상 독서 시간 (분당 200단어 기준): 1496분입니다. 저희의 새로운 웹사이트가 이제 오픈했습니다. 모든 과거 이슈에 대한 전체 메타데이터 검색과 아름다운 'vibe coded' 프레젠테이션을 제공합니다. 전체 뉴스 분석은 https://news.smol.ai/ 에서 확인하시고, @smol_ai 로 피드백을 보내주십시오!
오늘 OpenAI의 GPT5가 OpenAI 자체 DevDay 이벤트를 너무나 잘 요약해 주었기에, 저희는 GPT5가 제안한 제목을 오늘 이메일에 그대로 사용했습니다. 더 자세한 분석은 내일 팀과 함께하는 Latent Space 팟캐스트에서 다루겠습니다. 그 외 놓치지 말아야 할 링크들은 다음과 같습니다:

## OpenAI DevDay Product/API/SDK 출시
- Sama와 함께하는 50분 오프닝 키노트: https://www.youtube.com/watch?v=hS1YqcewH0c&t=1382s
- Andrew Mayne과 함께하는 1시간 OAI 팟캐스트: https://www.youtube.com/watch?v=QIdUllqmuls
- 웹사이트: https://openai.com/devday
- ChatGPT에 앱 도입 및 새로운 Apps SDK (블로그): https://openai.com/index/introducing-apps-in-chatgpt

Apps SDK (문서): https://developers.openai.com/apps-sdk
1분 YouTube 트레일러: https://www.youtube.com/watch?v=2C4Cs6503gw
- AgentKit 소개 (블로그): https://openai.com/index/introducing-agentkit

5분 Agent Builder 소개: https://www.youtube.com/watch?v=44eFf-tRiSg
Agents (문서): https://platform.openai.com/docs/guides/agents/agent-builder
ChatKit Studio (앱): https://chatkit.studio/ (playground, 위젯 빌더, 데모)
ChatKit 문서: https://platform.openai.com/docs/guides/chatkit
ChatKit Python: https://openai.github.io/chatkit-python/
ChatKit JS: https://openai.github.io/chatkit-js/
Guardrails (문서): https://guardrails.openai.com/docs
Evals (문서): http://platform.openai.com/docs/guides/evaluation-getting-started
- Codex가 이제 정식 출시되었습니다 (블로그): https://openai.com/index/codex-now-generally-available

Codex SDK (문서): https://developers.openai.com/codex/sdk
- 서비스 상태 대시보드: https://platform.openai.com/settings/organization/service-health
- GitHub 프로젝트: https://github.com/orgs/openai/repositories?q=apps-sdk+OR+chatkit+OR+guardrails (apps, chatkit, guardrails)

## 새로운 모델 출시
- gpt-5 pro (모델): https://platform.openai.com/docs/models/gpt-5-pro
- gpt-realtime-mini-2025-10-06 (모델): https://platform.openai.com/docs/models/gpt-realtime-mini (70% 더 저렴합니다)
- gpt-audio-mini-2025-10-06 (모델): https://platform.openai.com/docs/models/gpt-audio-mini
- gpt-image-1-mini (모델): https://platform.openai.com/docs/models/gpt-image-1-mini (80% 더 저렴합니다)
- Sora를 이용한 비디오 생성 (문서): https://platform.openai.com/docs/guides/video-generation
- Sora 2: 프롬프팅 가이드 (쿡북): https://github.com/openai/openai-cookbook/blob/16686d05abf16db88aef8815ebde5c46c9a1282a/examples/sora/sora2_prompting_guide.ipynb#L7
- sora-2 (모델): https://platform.openai.com/docs/models/sora-2
- sora-2-pro (모델): https://platform.openai.com/docs/models/sora-2-pro

---

# AI 트위터 요약
OpenAI DevDay: Apps SDK, AgentKit, Codex GA, GPT-5 Pro 및 Sora 2 API
- OpenAI는 ChatGPT를 애플리케이션 플랫폼으로 전환했습니다. 새로운 Apps SDK(MCP 기반으로 구축)를 통해 파트너는 맞춤형 UI, 액션 및 향후 수익 창출 기능을 사용하여 완전한 대화형 앱을 ChatGPT에 직접 임베딩할 수 있습니다. 초기 파트너로는 Canva, Figma, Zillow, Coursera가 있습니다. OpenAI 기조연설에서 공개된 출시 및 라이브 데모를 확인하세요: ChatGPT 내 앱 @OpenAI, SDK 프리뷰 @OpenAIDevs, 그리고 “DevDay ships” 요약 @edwinarbus.
- AgentKit은 OpenAI의 엔드투엔드 에이전트 스택으로, 시각적 Agent Builder, ChatKit UI, Guardrails, Evals, Connectors를 포함하여 프로덕션 에이전트를 구축, 배포 및 강화하는 데 사용됩니다. 무대 위 라이브 시연에서 OpenAI는 8분 이내에 작동하는 에이전트를 구축했습니다 @gdb. 문서 및 발표: AgentKit, 블로그. 특히, 내장된 프롬프트 옵티마이저는 커뮤니티 모범 사례(예: GEPA)와 일치합니다 @dbreunig.
- Codex는 이제 SDK, Slack 통합 및 코드 검토와 CLI/IDE 워크플로우를 위한 엔터프라이즈 제어/분석 기능을 갖춘 GA 상태입니다 (GA 게시물). 라이브 데모에서는 Codex를 사용한 음성+컨트롤러 기반 코딩을 시연했습니다 @gdb. 팀들은 Codex 덕분에 출시 속도가 빨라졌다고 평가합니다 (일부 내부 빌드에서 PR의 80%가 작성됨) @stevenheidel.
- 새로운 모델/API 및 규모 통계:

GPT‑5 Pro는 API에서 더 고도화된 추론을 위해 제공됩니다. 가격은 무대에서 그리고 커뮤니티 관찰자들에 의해 공유되었습니다: 100만 Token당 입력 $15 / 출력 $120입니다. @OpenAIDevs, @scaling01.
gpt‑realtime‑mini는 gpt‑realtime보다 약 70% 낮은 비용으로 speech‑to‑speech 기능을 제공합니다. @juberti.
Sora 2와 Sora 2 Pro는 이제 API를 통해 접근 가능합니다 (사운드, 리믹싱, 길이 제어 포함). 가격 예시: Sora 2는 초당 $0.10 (720p); Pro는 초당 $0.30 (720p) / 초당 $0.50 (1024p)입니다. @scaling01. Mattel은 이미 sketch‑to‑concept 루프에 Sora 2를 사용하고 있습니다. @gdb.
플랫폼 지표: 400만 명의 개발자, 주간 ChatGPT 사용자 8억 명, API를 통해 분당 60억 개 이상의 Token이 처리됩니다. @kevinweil, @nickaturley. 새로운 서비스 상태 대시보드와 약 40% 더 빠른 GPT‑5 응답을 제공하는 우선순위 티어가 제공됩니다. @OpenAIDevs.
컴퓨팅 및 추론 인프라: OpenAI × AMD, NVIDIA 스택, 그리고 vLLM
- OpenAI와 AMD는 6 GW 규모의 Instinct GPU를 배포하기 위한 다년간의 계획을 발표했으며, AMD는 OpenAI에 배포/가격 이정표 달성 시 최대 1억 6천만 주를 행사할 수 있는 워런트를 발행했습니다. 이 소식에 AMD 주가는 급등했으며, OpenAI는 이것이 현재 진행 중인 NVIDIA 구매와는 별개로 추가적인 것이라고 강조했습니다. @sama, @LisaSu, @TheRundownAI, @gdb.
- NVIDIA의 B200은 이제 Hugging Face Inference Endpoints에서 사용할 수 있습니다. @ClementDelangue. NVIDIA의 TensorRT‑LLM은 PyTorch‑native 코어, CUDA Graphs, speculative decoding, 그리고 GB200 지원을 통해 v1.0을 달성했으며, 현재 Llama3, DeepSeek V3/R1, Qwen3 등을 지원합니다. @ZhihuFrontier.
- vLLM은 최첨단 RL 루프(예: in‑flight 가중치

# AI Reddit Recap

## /r/LocalLlama + /r/localLLM Recap

### 1. Community Provider Appreciation Post (Image)
- 현재 커뮤니티에 가장 큰 기여를 하는 제공자들에게 감사를 표하는 게시물 (활동: 2530): 기술적이지 않은 밈 게시물로, 중국 기반의 LLM 제공자들인 GLM (ZhipuAI/THUDM), Alibaba의 Qwen, 그리고 DeepSeek을 현재 커뮤니티에 가장 크게 기여하는 존재로 칭찬합니다. 이들은 유능한 모델과 저렴한 접근성을 제공하며, 더 폐쇄적이고 고비용인 서구권 제공자들과 대조를 이룹니다. 댓글의 맥락은 이 그룹들이 OpenAI의 역사적으로 불투명한 접근 방식과 제품화에 대비하여 AI 접근을 민주화하고 있다고 설명합니다. 게시물에는 벤치마크나 구현 세부 정보는 제공되지 않습니다. 상위 댓글들은 GLM/Qwen/DeepSeek을 "인류에게 주는 선물"이라고 칭찬하며, OpenAI가 안전이라는 명목 아래 기밀성을 우선시했다고 주장하고, 이들 제공자가 없었다면 개발자들이 GPT와 유사한 접근을 위해 훨씬 더 많은 비용을 지불했을 것이라고 말합니다.

댓글 작성자들은 GLM (THUDM/ZhipuAI), Qwen (Alibaba), 그리고 DeepSeek과 같은 중국의 오픈 웨이트 모델 패밀리들이 웨이트 공개, 상세한 model cards, 그리고 경쟁력 있는 벤치마크 덕분에 현재 커뮤니티의 주력 모델이라고 강조합니다. 이 모델들은 폐쇄형 API에 대한 강력한 오픈 대안으로 커뮤니티 리더보드 (MMLU/GSM8K/HumanEval)에 자주 인용됩니다. Open LLM Leaderboard: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 및 GLM (https://github.com/THUDM/ChatGLM3), Qwen (https://github.com/QwenLM/Qwen2.5), DeepSeek (https://huggingface.co/deepseek-ai)의 모델 허브를 참조하십시오.
반복되는 기술적 주제는 비용 및 배포입니다. 7B–14B 모델 (종종 4-/8-비트 양자화된)을 셀프 호스팅하면 vLLM (https://github.com/vllm-project/vllm) 또는 llama.cpp (https://github.com/ggerganov/llama.cpp)와 같은 런타임을 사용하여 약 8–24 GB VRAM을 가진 소비자용 GPU에서 실행할 수 있으며, 토큰당 API 요금을 피할 수 있습니다. 이는 예측 가능한 TCO, 오프라인/엣지 배포, 그리고 독점 계층 (예: GPT‑3.5/4)에서는 비용이 너무 많이 드는 맞춤형 가드레일/파인튜닝 파이프라인을 가능하게 합니다.
OpenAI의 폐쇄적 출시 관행 (GPT‑3 이후 제한된 훈련 세부 정보)에 대한 기술적 비판이 있으며, 이는 이들 그룹의 개방성 (웨이트, 훈련/평가 레시피, 추론 스택)과 대조됩니다. 이러한 개방성은 독립적인 벤치마킹과 재현성을 가능하게 합니다. 참고 자료: Qwen 문서/논문 및 model cards (https://huggingface.co/Qwen), DeepSeek 릴리스 (https://github.com/deepseek-ai), GLM/ChatGLM 자료 (https://huggingface.co/THUDM).

## Less Technical AI Subreddit Recap
> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo

### 1. DeepMind CodeMender 및 Gemini 3 Tool-Use 업데이트
- Google DeepMind는 코드 보안을 위한 새로운 AI 에이전트인 CodeMender를 소개합니다. CodeMender는 코드 취약점을 자동으로 찾아 수정하며, 이미 주요 오픈 소스 프로젝트에 72개의 고품질 수정 사항을 제출했습니다(아직 공개 액세스는 불가능하지만 곧 제공될 예정입니다). (활동: 396): Google DeepMind는 저장소에서 취약점을 자율적으로 감지하고 수정 사항을 제안/제출하는 AI 코드 보안 에이전트인 CodeMender를 발표했으며, 주요 오픈 소스 프로젝트에 72개의 고품질 패치를 기여했다고 주장합니다. 공개 액세스는 아직 제공되지 않습니다(게시물). 제공된 링크 콘텐츠는 사이트 탐색만 포함하고 있으므로, 모델 제품군, 훈련/평가 데이터셋, 지원 언어, 취약점 클래스, CI/CD 통합, 검토 워크플로, 안전/가드레일 메커니즘에 대한 구체적인 내용은 이 입력에서 공개되지 않았습니다. 상위 댓글들은 이러한 에이전트가 조직 워크플로에서 지속적으로 실행될지 여부에 의문을 제기하며, 인적 요소 위험(예: 자격 증명 위생)에 대한 한계를 지적하고, 과도한 개입/오탐(예: .env/secrets 삭제)에 대한 우려를 제기합니다. 이는 엄격한 범위, 가드레일, 검토를 거친 자동화의 필요성을 시사합니다.

참가자들은 코드 수정 에이전트가 더 광범위한 운영 보안 위험을 해결하지 못한다고 지적합니다. 완벽한 정적/동적 개선(remediation)조차도 부실한 관행(예: 포스트잇에 적힌 비밀번호, 웹캠 노출)으로 인한 자격 증명(credential) 유출을 막을 수 없습니다. 이는 자동화된 취약점(vuln) 수정 에이전트와 더불어 코드 외적인 심층 방어(defense-in-depth)(비밀 관리(secret hygiene), 최소 권한(least-privilege) 접근, 엔드포인트 강화(endpoint hardening), 사용자 교육)의 필요성을 강조합니다.
자동으로 리포지토리를 수정하는 것에 대한 회의론은 엄격한 안전장치(guardrails)의 필요성을 부각합니다. 에이전트는 deny/allow 리스트를 통해 민감한 아티팩트(예: .env)를 건드리지 않아야 하며, 패치 전후에 비밀 스캐닝(secret scanning)을 통합하고, 파괴적인 변경이나 우발적인 비밀 노출을 방지하기 위해 보호된 브랜치(protected branches), 필수 코드 리뷰(mandatory code review), 드라이런(dry-run) diff, 쉬운 롤백(rollback)을 요구해야 합니다.
"항상 켜져 있는(always-on)" 배포에 대해, 댓글 작성자들은 암묵적으로 운영상의 우려를 제기합니다. 지속적으로 실행되는 에이전트는 PR(Pull Request) 시점 및/또는 예약된 스캔(야간/주간)에 통합되어야 하며, 공급망 무결성을 유지하고 노이즈와 리포지토리(repo) 변경(churn)을 최소화하기 위해 속도 제한(rate limits), 비용/거버넌스 제어(cost/governance controls), 범위가 지정된 토큰(scoped tokens), 상세한 감사 로그(audit logs)를 갖춰야 합니다.
- Gemini 3는 툴을 호출할 수 있습니다 (Activity: 533): 해당 게시물은 "Gemini 3가 툴을 호출할 수 있을 것"이라고 주장합니다. 즉, 외부 API를 호출하고 그 출력을 소비하기 위한 구조화된 함수/툴 호출을 지원하여 검색(retrieval), 코드 실행(code execution) 및 기타 에이전트 스타일의 액션(action)을 가능하게 합니다. 기술적으로 이는 기존 LLM 생태계(유형화된 인수(typed arguments)/JSON과 유사한 스키마(schemas) 및 툴 선택을 포함하는 함수 호출 인터페이스)와의 기본적인 동등성(table-stakes parity)이며, 순수한 자유 텍스트 프롬프팅(free-text prompting)에 비해 신뢰성과 통합성을 향상시킵니다. 댓글 작성자들은 대부분 이를 기본 기능("요즘은 기본적으로 필수 아닌가요?")이라고 언급하며, 게시물의 진지함에 대한 회의적인 시각도 있어, 이 발표가 참신하기보다는 사소한 것임을 암시합니다.

"Calling tools"는 LLM이 시작하는 함수 호출을 의미하며, 모델이 도구 이름을 선택하고 런타임이 실행하는 구조화된 인수(일반적으로 JSON)를 내보내는 방식입니다(예: 웹 검색, DB 쿼리, 코드 실행). 모델은 그 결과를 다음 턴에 통합하며, 이는 ReAct 스타일 루프와 유사합니다. 이는 OpenAI ([docs](https://platform.openai.com/docs/guides/function-calling)) 및 Google Gemini ([docs](https://ai.google.dev/docs/guides/function_calling))와 Anthropic Claude의 "Tool Use" ([docs](https://docs.anthropic.com/claude/docs/tool-use))와 같은 다른 스택에서 함수 호출 / 도구로 노출되는 동일한 기능입니다. 이것은 순수 텍스트 생성 이상의 그라운딩, 최신 데이터 액세스 및 정밀한 작업을 가능하게 합니다.

도구 사용은 프로덕션 LLM 앱의 기본 기능으로 점점 더 중요해지고 있습니다. 이는 RAG (검색), 계산기/코더, 통합(앱/API)을 지원하여 할루시네이션을 크게 줄이고 기능을 확장하기 때문입니다. 대부분의 경쟁력 있는 스택(GPT-4/4o (Assistants API tools), Claude 3.5 (Tool Use), 오픈소스 에이전트 프레임워크)은 도구 호출을 일급 기능으로 취급하므로, 신뢰할 수 있는 도구 사용의 부족은 경쟁력에 불리하게 작용합니다. 실제로 이는 강력한 스키마 준수, 도구 선택/라우팅, 다단계 계획/실행 충실도에 달려 있습니다.

몇몇 논평가들은 Gemini가 동종 모델에 비해 도구 사용의 역사적 신뢰성이 약하다고 언급하며, 스키마 불일치 인수, 잘못된 도구 선택, 취약한 다중 턴 계획으로 인한 실패(예: 엄격한 API에서 발생하는 4xx 오류)와 같은 문제들을 지적합니다. 실무자들은 종종 더 엄격한 JSON 스키마, 유효성 검사, 도구 선택 게이팅, 분해된 계획으로 이를 완화하지만, 커뮤니티 테스트에서 기본 성공률은 GPT-4o/Claude 3.5에 뒤처지는 것으로 알려져 있습니다. 만약 "Gemini 3"가 도구 라우팅, 인수 형성, 반복적 계획을 개선한다면, 에이전트 워크로드에서 이 격차를 줄일 수 있을 것입니다.

- Gemini 3 (활동: 503): "Gemini 3"라는 제목의 게시물은 (볼 수 없는) 이미지를 공유하는데, 논의에 따르면 Gemini의 도구 사용 기능과 플랫폼 통합에 중점을 둡니다. 논평가들은 Gemini가 현재 Gemini Apps 샌드박스를 넘어 더 광범위한 외부 도구/API를 호출할 필요성을 강조하고, 안정성/일관성을 최우선 과제로 삼아야 한다고 강조하며, 신뢰성과 생태계 폭의 격차를 시사합니다. 주목할 만한 논쟁은 "도구"가 무엇을 의미하는지(온디바이스 액션(휴대폰) 대 크로스 디바이스/데스크톱 명령어 실행)를 명확히 하는데, 적어도 한 명의 사용자는 Gemini가 노트북에서 명령어를 실행할 수 있다고 보고하여, 불균일하거나 컨텍스트에 따라 달라지는 도구 지원을 암시합니다.

더 넓은 tool-use와 개방형 상호운용성에 대한 요구: 댓글 작성자들은 Gemini Apps의 현재 제한적인 "Actions"를 넘어선 지원을 요청하며, 한 명은 "MCP compatibility is necessary atp"라고 언급했습니다. Model Context Protocol (MCP)을 채택하면 vendor-agnostic tool, 표준화된 discovery/schema, 그리고 어시스턴트 전반에 걸친 permission/logging flow를 가능하게 하여 Gemini가 다른 ecosystem에서 지원하는 것과 동일한 서드파티 기능을 활용할 수 있게 합니다 (modelcontextprotocol.io, github). 이는 plugin fragmentation을 줄이고 filesystem, HTTP, code, 그리고 맞춤형 enterprise tool을 균일한 서버로 가져오는 것을 더 쉽게 만들 것입니다.

교차 기기 실행 동등성 및 OS 제약: 한 사용자는 Gemini가 "execute commands on my laptop"할 수 있지만 휴대폰에서는 제약이 있다고 보고하며 플랫폼 차이를 강조했습니다. 데스크톱 에이전트는 native app, shell 또는 browser extension을 활용할 수 있는 반면, 모바일 OS는 background task와 inter-app automation을 제한합니다. 이러한 격차를 해소하려면 Android Intents, iOS App Intents/Shortcuts, foreground service와의 심층적인 통합과 명시적인 user permission을 가진 tool로서 device capability를 노출하기 위한 로컬 RPC bridge가 필요할 것으로 보입니다. 명확한 permissioning과 sandboxing은 안정성을 유지하면서 안전한 on-device action을 위해 필수적입니다.

최고의 엔지니어링 우선순위로서의 안정성/일관성: 댓글 작성자들은 "consistency and reliability above everything else"를 강조하며, 이는 tool-call success rate, end-to-end action completion rate, 그리고 deterministic planning과 같은 구체적인 목표로 이어집니다. 기술에는 schema-constrained function calling, planning/tool selection을 위한 temperature=0, exponential backoff 및 timeout을 사용한 재시도, idempotency token, 그리고 auditability를 위한 구조화된 error handling/logging이 포함됩니다. 견고한 eval (예: 기기 전반의 action success)과 prompt의 caching/stability는 분산과 사용자에게 보이는 flakiness를 실질적으로 줄일 수 있습니다.

### 2. TTS 음성 악센트 불만 (스코틀랜드 악센트)
- 스코틀랜드 악센트를 시도조차 하지 않았습니다. (활동: 494): 제목과 댓글로 미루어 보아, 이 게시물은 AI 음성 클론이 스코틀랜드 악센트와 관련된 대사(아마도 브레이브하트의 "Freedom")를 마이클 잭슨 스타일의 목소리로 렌더링하는 것을 보여주는 것으로 보입니다. 시스템은 MJ의 음색과 개인어(예: "hee-hee", "shamone")를 보존하지만, 악센트 전송에는 실패합니다. 이는 zero-shot TTS/음성 클로닝의 일반적인 한계를 보여줍니다. 모델은 종종 화자 식별 및 운율에 최적화되지만, 악센트 조건부 훈련 또는 파인튜닝 없이는 지역 악센트에 대한 제어력이 약합니다 (YourTTS 또는 VALL-E와 같은 다화자 TTS 참조). 최고 댓글들은 MJ의 과장된 표현인 "Free-hee-hee-dom"과 "The SHAMOOOONE"을 언급하며, 모델이 스코틀랜드 악센트를 놓치면서도 스타일적인 특징을 포착했음을 암시합니다. 독자들은 이를 문제라기보다는 재미있게 여깁니다.
- 스코틀랜드 악센트를 시도조차 하지 않았습니다. (활동: 492): Reddit에 연결된 미디어(v.redd.it/qu1ek8f8jetf1)는 HTTP 403 Forbidden 차단 페이지를 반환하며, 이는 Reddit의 CDN/게이트웨이가 자산을 검색하기 위해 인증된 액세스(OAuth 또는 개발자 자격 증명)를 요구함을 나타냅니다. 제목과 댓글 단서로 미루어 보아, 이 클립은 스코틀랜드 악센트 맥락에 적용된 AI 생성 마이클 잭슨 스타일의 목소리(음성 클로닝/VC 또는 TTS)를 특징으로 할 가능성이 높지만, 모델, 파이프라인 또는 품질 지표는 공개되지 않았으며 접근 제한으로 인해 콘텐츠를 확인할 수 없습니다. 논평은 주로 비기술적이며, "AI MJ" 목소리에 대한 재미를 표현하고 MJ의 보컬 틱(예: "hee-hee", "shamone")을 언급하며, 실질적인 기술적 논쟁은 없습니다.

### 3. AI 커뮤니티 정서: 밈, 분위기, 그리고 검열 논쟁
- 현재 분위기 (활동: 416): 참조된 Reddit 게시물에 접근할 수 없습니다. 요청이 HTTP 403 Forbidden("클라이언트가 네트워크 보안에 의해 차단됨")을 반환했으며 인증이 필요하다고 표시됩니다. 해당 콘텐츠는 v.redd.it(https://v.redd.it/mgss3gugtitf1)에 호스팅된 동영상으로 보입니다. 접근할 수 없으므로 기술적 세부 정보, 벤치마크 또는 구현 노트는 추출할 수 없습니다. 해결 방법으로는 Reddit Login을 통해 로그인하거나, 개발자 Token을 사용하거나, 지원 티켓을 제출하는 것이 있습니다. 상위 댓글들은 비기술적입니다. 농장에서 "이것"을 보고 싶다는 관심, 즐거움("I’m LMAO"), 그리고 "boomer FB slop"을 다음 단계로 끌어올릴 수 있다는 농담이 있습니다. 이는 저품질 자동 생성 콘텐츠의 확장에 대한 우려를 암시합니다.
- 흥미로운 미래가 될 거야, 얘들아! (활동: 629): v.redd.it/hjy60pum5jtf1의 링크된 미디어는 HTTP 403 Forbidden을 반환합니다. 이는 Reddit의 인증 계층 또는 엣지(CDN/WAF)에서 접근 제어가 적용되었음을 나타내며, 자산을 검색하려면 사용자 로그인 또는 개발자 Token이 필요합니다. 콘텐츠 자체에 접근할 수 없으므로, 유일한 구체적인 기술적 시사점은 생성형 AI의 모델, 벤치마크 또는 구현 세부 정보가 아닌 Reddit 미디어 CDN의 전달 및 게이트키핑에 관한 것입니다. 댓글 작성자들은 생성형 AI 사용이 엔터테인먼트/NSFW 콘텐츠("50% dank memes and 50% porn") 쪽으로 크게 치우칠 것이라고 추측하며, 역사적 제복의 정확성("히틀러가 영국 제복을 입고 있나?")에 대한 의문을 제기하여 잠재적인 deepfake/stylization을 암시합니다. 다른 댓글 작성자는 비관적인 정서("Welp it’s been a fun ride")를 표현합니다. 기술적 증거 또는 벤치마크는 제공되지 않습니다.
- 만약 ChatGPT가 신원 증명을 강요할 계획이라면, 18세 이상임을 확인한 경우 SFW "안전 장치(guard rails)"를 제거하는 것이 좋습니다 (활동: 527): 사용자들은 최근 ChatGPT의 SFW 안전 필터가 강화되었다고 보고합니다. 이 필터는 이제 합의된, 가상의 성인 콘텐츠를 포함하는 기존 스레드를 차단하거나 잠급니다. 모델은 종종 설명을 요청한 다음 거부하여, 이전에 작동 가능했던 창작 글쓰기 워크플로우를 방해합니다. OP(원 게시자)는 OpenAI가 ChatGPT에 신원/연령 확인(KYC)을 추가한다면, 확인된 18세 이상 사용자는 NSFW 안전 장치를 우회할 수 있어야 한다고 제안합니다. 이는 사용자 연령과 관계없이 노골적인 성적 콘텐츠 생성을 허용하지 않는 현재 OpenAI의 성적 콘텐츠 안전 사양(참조: https://platform.openai.com/docs/guides/safety-specifications/sexual-content)과 상충됩니다. 댓글 작성자들은 롤백 또는 18세 이상 "우회" 모드를 요구합니다. 한 명은 기본 "GPT‑5" 동작이 지나치게 정렬되어 있고 "형편없다"고 말하며, 다른 한 명은 이러한 제한이 미성년자 보호에 관한 것이 아니라고 주장합니다. 실질적인 우려는 NSFW 감지에서 증가된 오탐(false positives)이 이전 스레드와의 하위 호환성을 깨뜨리고 합법적인 성인 소설 사용 사례를 방해한다는 것입니다.

여러 사용자들이 최근 NSFW 안전 필터가 강화되어, 명백히 성인용인 가상의 캐릭터에 대해서도 이제는 여러 단계의 "명확화" 프롬프트가 트리거되고 거부로 이어진다고 보고합니다. 이러한 동작은 반복적인 글쓰기 워크플로우(예: 프로젝트 후 "만약 ~라면" 시나리오 생성)를 방해하고, 심지어 기존 대화 스레드에서 사용자들을 잠가버리기도 합니다. 이는 기본 모델 자체의 한계라기보다는 플랫폼 수준의 중재 래퍼에 변화가 있었음을 시사합니다.

한 댓글 작성자는 "기본 상태의 GPT-5"가 극도로 제한적이라고 특징지으며, 이는 원시 모델의 능력보다는 기본 시스템 프롬프트/안전 레이어가 창의적/NSFW 결과물의 병목 현상임을 암시합니다. 이는 기본 모델과 배포된, 정책이 적용된 구성 간의 차이를 강조하며, 여기서 기본 안전 스캐폴딩은 생성 품질을 크게 저하시킬 수 있습니다.

한 가지 대안이 제안됩니다. Mistral의 "Le Chat"을 사용하는 것입니다. 이는 GPT-4o와 유사한 기능을 가지고 있다고 주장되며, 동시에 다른 (더 유연한) 정책 체제하에서 작동하여 OpenAI의 가드레일을 피할 수 있습니다. 참고 자료: Le Chat (https://chat.mistral.ai) 및 OpenAI의 GPT-4o 개요 (https://openai.com/index/hello-gpt-4o/).

- 커뮤니티를 위한 최대 제공자 감사 (활동: 1034): 이 이미지는 중국 연구소들이 Alibaba/Qwen (HF), 01.AI/Yi (HF), DeepSeek (HF), InternLM (HF) 등 오픈 웨이트 릴리스를 통해 현재 오픈 소스 AI 커뮤니티에 "최대 제공자"임을 암시하는 밈 "감사" 게시물입니다. 댓글에서 언급된 기술적 뉘앙스: 이들은 "오픈 웨이트"이며, 완전히 "자유/오픈 소스"는 아닙니다. 라이선스에는 사용 제한이 포함될 수 있으며, 다운스트림 생태계 지원(LoRA 파인튜닝, 툴링)은 PonyXL/Illustrious와 같은 인기 있는 서구 기반 모델에 비해 뒤처집니다. 댓글 작성자들은 "무료가 아니다"(라이선스 뉘앙스)라고 주장하며, 미국/EU 기업들이 폐쇄적인 태도를 유지하는 동안 중국은 웨이트를 공개함으로써 시장 점유율을 확보한다고 말합니다. 다른 이들은 중국 모델에 대한 커뮤니티 LoRA가 거의 없다고 지적하며, 이는 하드웨어 한계 때문일 가능성이 높습니다.

여러 평론가들은 "무료" 액세스와 open-weights 릴리스가 별개라고 명확히 합니다. open weights는 다운로드 및 로컬 사용/파인튜닝을 허용하지만, 완전한 FOSS 코드와는 다르게 여전히 compute 비용을 발생시키고 라이선싱 제약이 있을 수 있습니다. 실질적으로 open weights는 오프라인 인퍼런스, 퀀타이제이션, 그리고 LoRA training을 가능하게 하는데, 이는 closed API가 제공하지 않는 이점이며, 동시에 비용을 사용자의 하드웨어와 전기로 전가합니다. 이러한 미묘한 차이는 모델 실행 비용이 무료가 아니더라도 커뮤니티 벤치마크와 재현성을 가능하게 함으로써 생태계 건강에 영향을 미칩니다.

모델 채택과 관련하여, 중국 open 모델의 주요 장애물은 LoRA ecosystem입니다. 사용자들은 PonyXL 또는 Illustrious에 비해 커뮤니티 LoRA가 적다고 지적하는데, 이는 XL 규모의 파인튜닝을 위한 하드웨어 한계 때문일 가능성이 높습니다. SDXL급 LoRA training은 일반적으로 소비자용 GPU에 부담을 줍니다. 8–12 GB VRAM을 사용하는 많은 취미 사용자들은 작은 batch size 또는 공격적인 메모리 최적화를 사용해야 하는 반면, 더 원활한 training은 종종 16–24 GB 이상의 VRAM에서 이점을 얻습니다. 이는 잘 지원되는 ecosystem에 비해 커뮤니티 LoRA의 양/품질을 감소시킵니다. kohya-ss (https://github.com/bmaltais/kohya_ss)와 같은 도구들이 도움이 되지만, XL 모델은 SD1.5보다 여전히 더 많은 리소스를 필요로 합니다.

- Brett Adcock: “이번 주, Figure는 BMW X3 차체 공장 생산 라인에서 5개월 이상 가동되었습니다. 우리는 매 생산일마다 하루 10시간씩 가동했습니다! Figure와 BMW가 휴머노이드 로봇으로 이러한 일을 한 세계 최초라고 생각합니다.” (Activity: 1153): Brett Adcock은 Figure 휴머노이드 로봇이 BMW X3 차체 공장 생산 라인에서 약 5개월 동안 매 생산일마다 약 10시간씩 작동해왔다고 보고하며, 이는 자동차 제조 분야에서 최초의 지속적인 휴머노이드 배치라고 주장됩니다. 해당 게시물은 작업 범위, MTBF/가동 시간, 오류율, 안전 사고 또는 처리량 영향에 대한 정량적인 세부 정보를 제공하지 않으며, 참조된 클립은 접근이 제한되어 있습니다 (video, Figure). 기술에 정통한 평론가들은 반복적인 작업 스테이션에 휴머노이드 형태를 선택한 것에 대해 목적에 맞게 제작된 또는 바퀴 달린 플랫폼과 더 간단한 end-effector에 비해 의문을 제기하며, 하루 10시간은 일반적인 산업용 로봇의 작동 주기(종종 연속/24×7)에 비해 적다고 지적하면서, 이는 여전히 제한적이거나 시범적인 배치일 수 있음을 시사합니다. 다른 이들은 더 광범위한 채택 시기(예: 2035년까지 로봇이 "모든 곳에")에 대해 추측합니다.

폼 팩터 논쟁 (휴머노이드 vs 바퀴형/특수 목적): 댓글 작성자들은 고정된 로봇 셀이나 바퀴형 모바일 매니퓰레이터가 더 간단하고 신뢰성이 높을 수 있는데, 반복적이고 순환적인 작업에 왜 휴머노이드가 필요한지에 대해 의문을 제기합니다. 강조된 기술적 트레이드오프는 휴머노이드가 사람이 설계한 작업 셀(도달 범위, 도구 형상, 고정 장치)에 바로 투입될 수 있고, 재도구화 없이 사람의 도구를 사용할 수 있다는 점입니다. 하지만 다리와 5지 손은 복잡성, 비용, 잠재적인 고장 모드를 추가합니다. 많은 공장 작업은 2지 또는 3지 그리퍼나 평행 조 엔드 이펙터가 장착된 바퀴형 베이스로 처리할 수 있습니다. 내재된 최적화 문제는 민첩성/커버리지 대 가동 시간(uptime)/MTBF 및 통합 비용이며, 휴머노이드는 더 간단하고 신뢰성이 높은 전용 자동화를 희생하여 유연성을 제공합니다.

가동 시간(uptime) 및 듀티 사이클 회의론: 5개월 동안 하루 10시간이라는 주장은 산업용 로봇이 일반적으로 다중 교대 또는 24/7 운영을 목표로 한다는 논의를 촉발합니다. 따라서 10시간으로 제한하는 것은 통합/안전 제약, 인간 교대 근무 정렬, 충전/열 제한 또는 신뢰성 번인(burn-in)을 반영할 가능성이 높습니다. 기술에 정통한 독자들은 OEE/MTBF/MTTR 및 개입 간 평균 주기(mean cycles between intervention)와 같은 지표가 달력 시간보다 더 의미 있다고 지적하며, 생산 준비 상태를 평가하기 위해 개입 빈도, 복구 시간 및 자율 오류 처리(autonomous error handling)에 대한 데이터가 필요하다고 제안합니다.

기존 산업용 암봇(armbot)과의 비교: 이미 셀에 대규모 6-DoF 암봇이 있는 상황에서, 댓글 작성자들은 휴머노이드가 어떤 고유한 가치를 추가하는지 묻습니다. 기술적 주제는 고정된 암봇이 고도로 제약되고 고정된 작업(예: 용접, 재료 이송)에는 탁월하지만, 사람과 유사한 도달 범위, 자세 및 다중 접촉 조작(multi-contact manipulation)이 맞춤형 고정 장치(custom fixturing) 및 전환 시간(changeover time)을 줄일 수 있는 비정형적이거나 가변적인 하위 작업(임시 처리, 도구 픽업, 검사, 케이블 라우팅)에서는 어려움을 겪는다는 것입니다. 트레이드오프는 전용 셀의 처리량(throughput) 및 단순성 대 엣지 케이스(edge case) 또는 다품종 소량 생산(high-mix/low-volume) 작업을 위한 재구성 가능성(reconfigurability) 및 낮은 재도구화(retooling) 비용입니다.

# AI Discord Recap
> gpt-5가 요약한 요약의 요약

1. Sora 2 Video Gen: 데모, 제한 사항 및 반응
- Duckumentary가 눈길을 사로잡고 밈이 확산되다: OpenAI는 X에 30초짜리 Sora 2 단편 'The Quack: Part 1'을 공개하며 Sora 2의 창의적 충실도와 티저와 함께 공유된 초대 코드 FRIYAY에 대한 관심을 증폭시켰습니다. 이번 공개는 잘 다듬어지고 밈 친화적인 짧은 영상으로 AI 비디오 생성 분야의 빠른 발전을 조명하며, 제한된 장면에서 prompt-to-video 일관성을 테스트합니다.

커뮤니티 회원들은 빠르게 향상되는 gen-video 품질을 환영했으며, @elder_plinius가 언급한 저중력 '우주비행사 위 말' 개그(Sora 2 Pro의 도약)와 같은 다른 최근 Sora 2 클립들과 비교를 공유했습니다. 많은 이들은 이번 공개를 더욱 신뢰할 수 있는 스토리보드 준수와 영화적 타이밍을 향한 가시적인 진전으로 평가했습니다.
- Sora, 밤새 IP 문을 닫다: 크리에이터들은 Sora 2에서 갑작스러운 prompt 재작성과 저작권이 있는 콘텐츠에 대한 전면적인 금지를 보고했으며, 이는 처음에는 강력해 보였던 애니메이션 테스트 이후 X의 Andrew Curran을 인용한 것입니다. 이러한 변화는 직접적인 참조(예: 특정 프랜차이즈 이름)를 제한하고, 보호되는 캐릭터와 세계에 대한 설명적인 우회책을 강제합니다.

사용자들은 이 경험을 "speedrunning enshittification"이라고 묘사했으며, 모델이 이제 prompt와 결과물을 적극적으로 검열하여 팬 스타일 비디오의 창의적 범위를 축소하고 있다고 언급했습니다. 논의는 이러한 정책 변경이 프로덕션 파이프라인에 미치는 영향과 스타일만으로 된 설명이 여전히 검열을 통과할 수 있는지 여부에 집중되었습니다.
2. Local/Edge Inference: LM Studio 호환성 및 DIY 처리량
- LM Studio, OpenAI v1 응답 지원: LM Studio 0.3.29는 OpenAI /v1/responses 호환성을 추가하여, 표준 OpenAI API 형식을 기대하는 앱들이 로컬 모델에 직접 연결할 수 있도록 했습니다. 또한 이 릴리스에서는 로컬 모델 변형을 나열하는 CLI 헬퍼 lms ls --variants를 선보여, 다중 변형 개발 워크플로우를 간소화했습니다.

엔지니어들은 OpenAI 스타일 클라이언트와의 더 원활한 드롭인 통합과 터미널에서 강력한 변형 발견 덕분에 더 빠른 반복이 가능했다고 보고했습니다. 이는 로컬 실험과 /v1/responses 시맨틱스를 가정하는 프로덕션 프로토타입 간의 격차를 좁힙니다.
- Wi-Fi 팜, GLM에 23 tok/s로 공급: 한 설정은 Wi-Fi를 통해 8개의 RTX 3090이 장착된 3개의 노드에서 분산 추론을 실행하여, OpenRouter/Z.ai에서 GLM 4.5 Air (무료)를 통해 사용할 수 있는 모델을 사용하여 8비트 GLM 4.5 Air에서 약 5.5k 프롬프트 처리와 약 23 tok/s를 달성했습니다. 운영자는 부품이 도착하면 처리량을 대략 두 배로 늘리기 위해 2개 노드(4/4)로 재조정할 계획입니다.

이 보고서는 신중한 샤딩, 정밀도 선택, 그리고 인터커넥트가 저렴하게 로컬 클러스터 처리량을 높일 수 있는 방법을 강조합니다. 또한 GLM 4.5 Air를 분산 서빙 스트레스 테스트를 위한 신뢰할 수 있고, 속도 제한에 친화적인 기준선으로 주목합니다.
3. OpenRouter 액세스: iFlow 해킹, 모델 스왑, 그리고 Seed LLM
- iFlow 플립, 무료 GLM-4.6 호출 잠금 해제: 한 회원이 iFlow를 리버스 엔지니어링하여 단순히 Python 스크립트를 실행함으로써 모든 OpenAI 호환 클라이언트에서 무료 GLM-4.6 요청을 라우팅했습니다. 이 기술은 Docker를 피하고 Qwen/Gemini 스택과도 작동한다고 보고되었습니다.

채팅은 이를 기존 OpenAI SDK 흐름에 연결하는 데 중점을 두었지만, 신뢰성과 서비스 약관(Terms-of-Service) 위험에 대해 경고했습니다. 이 해킹은 어댑터 레이어가 타사 모델 엔드포인트에 무임승차하기 위해 어떻게 악용될 수 있는지를 보여줍니다.
- DeepSeek 중단; GLM Air 유지: 공급자가 deepseek-v3.1-base 호스팅을 중단(HTTP 404)한 후, 사용자들은 GLM 4.6 및 OpenRouter/Z.ai를 통한 무료 GLM 4.5 Air 티어와 같은 대안으로 전환했습니다. 이 전환은 OpenAI 스타일 API에 의존하던 다운스트림 앱을 안정화했습니다.

Grok 4 Fast가 더 이상 무료가 아니게 되면서, GLM의 무료 티어는 테스트 중 속도 제한 문제(rate-limit headaches)를 피하기 위한 주요 선택지가 되었습니다. 스레드에서는 프로토타입이 막히지 않도록 임시방편들 간의 지연 시간, Token 가격, 그리고 신뢰성을 비교했습니다.
- Seed 모델, 저렴하게 프론티어 성능을 예고: 빌더들은 OpenRouter에 ByteDance의 Seed LLM(예: Seed 1.6)을 추가해 달라고 요청했으며, Volcengine Ark를 통해 호스팅되는 강력한 결과와 약 $0.11/$0.28 mtok의 저렴한 가격을 언급했습니다. 이러한 관심은 프론티어와 유사한 성능과 공격적인 비용 곡선의 조합에서 비롯됩니다.

중국 호스팅 컨트롤 플레인에 대한 우려에도 불구하고, 참석자들은 품질 대비 가격 비율을 검증하기 위한 실험에 찬성했습니다. 합의된 내용은 접근성 및 정책 명확성이 개선된다면 Seed가 주류 가격에 압력을 가할 수 있다는 것이었습니다.
4. GPU 시스템: 새로운 dtypes, Multi-GPU 컴파일러 및 NVLink 인사이트
- Arm, 6비트 MXFP6로 AI 역량 강화: Arm은 Arm Architecture Developments 2025에 자세히 설명된 A-프로파일 로드맵의 새로운 SVE/SME 명령어와 함께 OCP MXFP6 포맷을 통한 6비트 AI 데이터타입 지원을 발표했습니다. 이러한 움직임은 엣지/임베디드 AI의 메모리 풋프린트 및 대역폭 감소를 목표로 합니다.

엔지니어들은 MXFP6가 메모리 대역폭이 지배적인 양자화 친화적 모델의 처리량을 향상시킬 것으로 기대합니다. 이번 업데이트는 하드웨어 네이티브 커널을 사용한 8비트 미만 추론을 향한 광범위한 산업 모멘텀을 시사합니다.
- Mercury, Multi-GPU 컴파일을 빠르게 만듭니다: Mercury 논문은 원격 GPU 메모리를 메모리 계층 구조의 관리형 확장으로 취급하여 Multi-GPU 연산자를 컴파일하는 CommIR을 소개합니다. 보고된 결과에 따르면 수작업으로 만든 기준선 대비 평균 1.56배의 속도 향상과 실제 LLM 워크로드에서 최대 1.62배의 성능 향상을 보였습니다.

Mercury는 데이터 배치 및 장치 간 트래픽을 명시적으로 모델링하여 3D 병렬 학습을 괴롭히는 교차 GPU 지연을 최소화합니다. 논의에서는 CommIR의 루프 중심 접근 방식과 NVLink/NVSwitch 토폴로지용 벤더 라이브러리를 비교했습니다.
- NVLink Copy Engine, 높은 대역폭을 기록합니다: NVLink copy engine 및 구성을 벤치마킹하는 공개 실험 결과는 여기에 게시되었습니다: NVLink bandwidth experiments (copy engines). 결과는 측정된 대역폭이 플랫폼 배선 및 복사 경로에 따라 달라진다는 점을 강조했습니다.

엔지니어들은 TMA 대 로드/스토어 대 memcpy 경로를 대조했으며 일부 B200 Multi-GPU 테스트에서 놀라운 cudaMemcpy 헤드룸을 발견했습니다. 핵심: 커널 전략을 확정하기 전에 정확한 패브릭 + 드라이버 조합에서 프로파일링하십시오.
5. 에이전트 툴링: AppsSDK 대 MCP, 새로운 DSPy 모듈 및 게이트웨이
- AppsSDK, MCP의 파티를 망치다: OpenAI의 AppsSDK는 앱용 ChatGPT 내 UI를 도입했으며 (론칭 파트너: TheFork), Cloudflare의 Code Mode는 에이전트 툴 호출을 Workers로 전환하는 방안을 제안했습니다. MCP 기여자들은 AppsSDK UI와 MCP-UI 간의 중복, 그리고 Code Mode가 단순한 툴 호출을 과도하게 엔지니어링하는지에 대해 논의했습니다.

일부는 AppsSDK가 에이전트 작업의 턴 수와 지연 시간을 줄일 수 있다고 주장했으며, 다른 이들은 일반 웹 API/SDK에 비해 성능(perf)과 복잡성을 지적했습니다. 해당 스레드에서는 생태계의 일관성을 유지하기 위해 MCP 검색/기능을 앱 스타일 트랜잭션과 일치시킬 것을 촉구했습니다.
- DSPy ReAct Machina, 다중 턴 에이전트 구동: 커뮤니티 모듈인 DSPy‑ReAct‑Machina가 PyPI에 출시되어 대화를 위한 단일의 확장 가능한 컨텍스트 버퍼로 다중 턴 ReAct를 가능하게 합니다. 저자는 동반 게시물에서 설계 트레이드오프를 자세히 설명했습니다: [Dev.to blog](Dev.to blog).

빌더들은 트라젝토리 저장, 전체 ReAct 체인에 대한 리플렉션, 그리고 이를 기존 DSPy 프로그램에 연결하는 것에 대해 논의했습니다. 관심은 툴 호출 쓰레시를 줄이고 더 긴 계획을 안정화하는 데 집중되었습니다.
- Neosantara, 무료 LLM Gateway 공개: Neosantara AI는 DSPy 통합을 갖춘 무료 LLM Gateway Platform을 출시했으며, 관련 문서는 여기에서 확인할 수 있습니다: [Neosantara DSPy docs](Neosantara DSPy docs). 신규 사용자는 월 1만 개의 consume Token을 받으며, 공개된 연락처로 피드백을 보낼 수 있습니다.

이 Gateway는 단일 제공업체에 종속되지 않고 빠른 앱 스캐폴딩을 목표로 하며, 데모 및 비용에 민감한 프로토타입에 유용합니다. 얼리 어답터들은 출시 전에 Rate Limit과 Fallback을 테스트할 것을 강조했습니다.

---

# Discord: 주요 Discord 요약

## LMArena Discord
- Perplexity 프로모션, 추천인 경쟁 촉발: Perplexity 학생 프로모션이 공개되었는데, 이는 활성 학생 신분을 요구하며, 사용자들이 투명하게 추천 보너스를 요청하는 상황으로 이어졌습니다.

이 프로모션은 접근성과 할인을 위해 사용자들이 얼마나 노력할지에 대한 논의를 불러일으켰습니다.
- Comet Browser: 에듀테크(EdTech)인가 치트웨어(CheatWare)인가?: Comet Browser가 검토되었는데, 한 멤버는 이를 공부하기 좋은 곳이라고 설명했고, 다른 멤버는 농담 삼아 부정행위를 하기 좋은 곳이라고 불렀습니다.

Comet의 어시스턴트에 대한 의견은 엇갈렸는데, 일부는 ChatGPT나 Gemini보다 못하다고 평가했고, 다른 일부는 음성 모드를 높이 평가했습니다.
- 목마른 AI, 지역 수자원 고갈: 멤버들은 AI의 환경적 영향, 특히 데이터 센터 냉각을 위한 물 사용량에 대해 논의했으며, 한 멤버는 ChatGPT의 답변 20개가 약 0.5리터의 물을 소비한다고 언급했습니다.

한 멤버는 AI 데이터 센터 인근 주민들이 어려움을 겪고 있다고 주장했고, 다른 멤버는 열 방출을 위한 미래의 우주 기반 데이터 센터에 대해 추측했습니다.
- Gemini 3 출시 유출: 2025년 10월 9일?: 채팅에서는 초기 테스터와 업계 관계자들의 보고서를 인용하여, Gemini 3가 AI Studio와 같은 플랫폼에서의 테스트를 거쳐 2025년 10월 9일에 공식 출시될 예정이라고 밝혔습니다.

한 회원은 AI Ultra 사용자 전용으로 제공되기를 선호한다고 밝혔습니다.
- Hunyuan 3 Image Gen 출시: 회원들은 Tencent의 새로운 이미지 생성기인 Hunyuan 3에 대한 첫인상을 공유했으며, 한 회원은 Qwen 또는 Flux보다 우수하고 Imagen과 동등하다고 평가했습니다.

또 다른 회원은 이러한 중국 모델들이 영어로 번역할 때 번역 및 미묘한 문화적 세부 사항과 맥락에서 종종 어려움을 겪는다고 언급했습니다.

---

## OpenAI Discord
- GPT-5, 즉각적인 위기 감지 목표: GPT-5 Instant는 위기에 처한 사용자를 더 잘 인식하고 지원하기 위해 업데이트되고 있으며, 민감한 대화를 즉각적인 지원을 제공하도록 라우팅합니다.

ChatGPT는 어떤 모델이 활성화되어 있는지 사용자에게 계속 알려줄 것이며, ChatGPT가 GPT Instant만 사용하도록 강제하는 방법에 대한 논의가 진행 중입니다.
- 무료 ChatGPT Business 플랜: 사용자들은 직접 링크를 통해 최대 5개의 시트를 포함하는 ChatGPT Business 1개월 무료 이용권을 얻고 있으며, 이후에는 시트당 $30가 청구됩니다.

이러한 무료 액세스는 무제한 계정 생성과 비즈니스 플랜 내에서 Codex의 유용성에 대한 논의를 촉발했습니다.
- Arduino의 AI MCU, 신경망 마법 모방: Arduino는 NPU 기반 ARM과 유사한 AI 지원 마이크로컨트롤러를 개발 중이며, 더 작은 Hailo-8과 같은 장치일 것으로 추정됩니다.

한 사용자는 Raspberry Pi에 Hailo-8L을 사용하여 2와트 미만의 전력으로 2000+ FPS의 객체 인식을 달성했으며, 이를 UGV Waveshare 장난감 로봇 탱크에 장착했다고 보고했습니다.
- Sora 2, 조악한 결과물로 비난받아: 사용자들은 Sora 2의 낮은 해상도 품질과 워터마크를 비난하며, 저렴한 느낌을 준다고 말합니다.

많은 이들은 OSS 20B 모델이 더 좋았다고 주장하며, 건강 연구 인포그래픽을 위한 일관된 JSON 컨텍스트 프로필을 달성하기 어렵다고 말합니다.
- 저작권, DBZ 콘텐츠 생성을 검열: 회원들은 새로운 저작권 제한으로 인해 Dragon Ball Z와 같은 저작권이 있는 콘텐츠 생성이 불가능하며, 사용자들은 직접적인 언급 없이 캐릭터와 설정을 묘사해야 한다고 지적합니다.

사용자들은 Goku와 Vegeta, 세계, 애니메이션 스타일, 의상, 톤을 묘사해야 하지만, DBZ와는 크게 닮지 않을 수 있다고 경고했습니다.

---

## Unsloth AI (Daniel Han) Discord
- B200 온디맨드 액션 누락!: 사용자들은 DeepInfra에서 단일 B200 온디맨드 렌탈이 사라졌으며, 이제 8x B200 설정만 제공된다고 보고했습니다.

Modal 및 Thundercompute와 같은 대안이 제안되었지만, 이들 또한 사용 불가능하다고 보고했습니다.
- Discord 보안 침해 발생!: Discord는 9월 20일 보안 사고를 공개했으며, 타사 고객 서비스 시스템에 대한 무단 접근으로 개인 데이터 및 정부 신분증 이미지가 노출되었을 가능성이 있다고 밝혔습니다.

이번 침해는 Trust & Safety 팀에 대한 소셜 엔지니어링과 관련이 있을 가능성이 있으며, 디지털 ID 및 데이터 저장 관행에 대한 논의를 촉발했습니다.
- 데이터셋 다운그레이드가 TTS의 구원자!: Orpheus 4B TTS 모델의 파인튜닝을 디버깅하던 한 사용자가 datasets를 버전 3.6.0으로 다운그레이드하여 데이터셋 로딩 오류를 해결했습니다.

‘torchcodec.decoders.AudioDecoder’ 객체가 subscriptable하지 않다는 오류가 성공적인 데이터셋 로딩 및 훈련을 방해했으며, 이는 버전 호환성 문제를 부각합니다.
- Qwen Moe, 6개 모델 매시업 공개!: 한 멤버가 사용자 정의로 구축된 6B Qwen 3 아키텍처를 사용하여 각각 2개의 데이터셋으로 6개의 개별 모델을 훈련하는 것을 선보였으며, 이후 Unsloth를 사용하여 이들을 6X6B Qwen Moe, 36B 모델(27B 압축)로 "moe"했습니다.

Unsloth, quants를 담당한 Team Mradermacher, 그리고 MLX 및 이 모델 프로젝트의 여러 부분에서 협력한 Team Nightmedia에 특별한 감사를 표했습니다. 데이터셋은 자체적으로 생성되었으며, [DavidAU/Qwen3-MOE-6x6B-Star-Trek-Universe-Alpha-D-256k-ctx-36B](https://huggingface.co/DavidAU/Qwen3-MOE-6x6B-Star-Trek-Universe-Alpha-D-256k-ctx-36B)에서 이용할 수 있습니다.
- Sequence Packing + Flash Attention으로 파인튜닝 속도 향상!: 한 멤버가 Sequence Packing과 Flash Attention을 사용하여 에폭 훈련 시간을 약 30분에서 약 4-5분(약 500 mio 토큰)으로 단축했으며, CPU 중단 없이 100% GPU 활용률을 달성했습니다.

Flash Attention은 수동 컴파일이 필요했으며, 이는 워크스테이션의 메모리 사용량을 최고치로 끌어올렸습니다.

---

## LM Studio Discord
- LM Studio, v1 응답으로 OpenAI 호환성 확보: LM Studio 0.3.29는 OpenAI /v1/responses 호환성 API를 도입하여, 개발자가 표준 OpenAI API 응답 형식을 기대하는 애플리케이션과 LM Studio를 통합할 수 있도록 합니다.

최신 LM Studio CLI 기능인 `lms ls --variants`는 사용자가 터미널에서 로컬 모델 변형을 쉽게 나열할 수 있도록 합니다.
- AI 버블 토크: 멤버들은 Nvidia, Oracle, OpenAI의 자금 순환에 대해 논의하며, 이들 중 하나라도 실패하면 눈덩이 효과를 일으킬 수 있다고 제안합니다.

다른 멤버는 자신의 로컬 사용에는 toolcalls가 가치가 없다고 언급했지만, toolcalls 없이 예를 들어 system prompt에 지식 관련 내용을 주입하는 플러그인이 있다면 좋을 것이라고 덧붙였습니다.
- GPT-OSS-120B 프레임워크, 128k Context로 출시: GPT-OSS-120B 프레임워크가 출시되었으며, 128k context window를 자랑하고 Ryzen AI max+ 하드웨어에서 초당 19.76 Token의 속도로 실행됩니다.

다른 멤버들은 정확한 하드웨어 사양에 대해 후속 질문을 했습니다.
- 분산 추론 설정, 성공적으로 가동: 한 멤버가 3개의 노드에 걸쳐 WiFi를 통해 8개의 3090을 사용한 성공적인 분산 추론 설정을 공유했으며, full 8-bit precision에서 GLM 4.5 air에 대해 약 5.5k prompt 처리와 초당 23 Token의 출력을 달성했습니다.

그들은 새로운 부품이 도착하면 4/4 구성으로 2개의 노드를 사용하여 속도를 두 배로 늘릴 계획입니다.
- 구형 GPU에서 Vulkan이 CUDA보다 우세?: 한 멤버는 LM Studio에서 qwen/qwen3-30b-a3b-2507 (Q4_K_M) 모델을 사용하여 NVIDIA P40에서 Vulkan이 CUDA보다 훨씬 뛰어난 성능(초당 63.02 Token 대 초당 23.31 Token)을 보인다는 것을 발견했습니다.

멤버들은 P40의 오래된 드라이버와 구형 카드에 대한 Vulkan의 더 나은 지원이 그 이유일 수 있다고 제안합니다.

---

## OpenRouter Discord
- iFlow 리버싱으로 무료 GLM-4.6 노출: 한 멤버가 iFlow를 리버스 엔지니어링하여 해당 폴더에서 python 파일을 실행함으로써 모든 OpenAI 호환 도구에서 무료 GLM-4.6 요청을 가능하게 했습니다.

커뮤니티는 Docker 없이 Qwen/Gemini와 함께 이것을 사용하는 것에 대해 논의했습니다.
- Deepseek 3.1 서비스 중단: 제공자가 deepseek-v3.1-base 호스팅을 중단한 후 사용자들은 404 오류를 보고했지만, 멤버들은 deepseek v3.1 deepinfra와 GLM 4.6을 실행 가능한 대안으로 제안했습니다.

Grok 4 Fast가 더 이상 무료가 아니게 되면서 사용자들은 대안을 찾고 있었고, rate limit 오류를 우회하기 위해 Z.ai를 통한 GLM 4.5 Air (무료)가 제안되었습니다.
- OpenRouter의 BYOK 이해 명확화: 사용자들은 BYOK (Bring Your Own Key)에 대한 명확성을 요구했으며, OpenRouter가 API (예: OpenAI)에 대한 프록시 역할을 하며 사용자들은 API 제공자로부터 직접 요금을 청구받는다고 설명되었습니다.

OpenRouter는 BYOK에 대해 5%의 추가 요금을 면제하며, 지출 통제 및 대체 옵션과 같은 이점을 제공한다고 언급되었습니다.
- Sora 2의 가격 정책 공개: Sora 2의 가격이 공개되었습니다: 프로는 비디오 초당 0.3달러, 비프로는 초당 0.1달러입니다.

이는 딥페이크를 쉽게 생성하는 것의 함의와 워터마크 및 암호화를 우회하는 방법에 대한 논의를 촉발했습니다.
- 바이트댄스의 Seed 모델이 관심을 불러일으켰습니다: 회원들은 Seed 1.6과 같은 바이트댄스의 Seed LLM 모델이 최첨단 성능과 저렴한 가격(토큰당 $0.11 / $0.28)을 제공한다는 점을 언급하며 OpenRouter에 포함되는 것에 관심을 표명했습니다.

주요 호스트가 중국 플랫폼인 volcengine.com이라는 점에 대한 우려가 제기되었지만, 이 모델들의 잠재력은 여전히 가치 있는 것으로 평가됩니다.

---

## Cursor 커뮤니티 Discord
- GPT-5와 Claude 4.5가 벤치마크에서 경쟁합니다: 사용자들은 GPT-5가 솔루션을 과도하게 설계하고 프롬프트에 의존하는 경향이 있다고 생각하는 반면, Claude는 많은 작업에서 더 나은 성능과 빠른 속도를 보인다고 말합니다. 하지만 의견은 엇갈립니다.

일부 사용자들은 Sonnet 4가 GPT-5보다 열등하다고 생각했지만, 다른 사용자들은 Ultra를 선호했습니다.
- 스텔스 Cheetah 모델이 등장했습니다: 회원들은 Cheetah라는 새로운 유료 모델(아마도 Gemini 3.0)을 발견했으며, 그 속도를 칭찬했습니다. 한 사용자는 5초 이내에 이미지 생성이 가능하다고 보고했습니다.

한 사용자는 이를 테스트하고 거의 즉시 문제를 해결했다고 말했으며, 다른 사용자는 그 속도를 높이 평가했습니다.
- GPT-5 Pro는 더 비싼 요금을 청구합니다: 새로 출시된 GPT-5 Pro는 Opus와 같은 다른 모델보다 더 비싸며, 사용자들은 Cursor CLI에서 이 모델이 어떻게 작동하는지 테스트하고 있습니다.

한 사용자는 GPT-5 Codex가 지속적으로 제로 diff 편집을 한다고 보고했습니다.
- Cursor UI 변경으로 사용자들이 혼란스러워합니다: 사용자들은 최근 Cursor UI 변경에서 사라진 채팅 닫기 버튼에 대해 논의했습니다.

한 회원은 상단의 Cursor 로고가 창을 닫을 수 있다는 것을 깨닫지 못하고 이를 에이전트 버튼으로 착각했습니다.
- 백그라운드 에이전트가 버그에 시달리고 있습니다!: 여러 사용자가 백그라운드 에이전트가 실패하고 있으며 조사가 진행 중이라고 보고했습니다. 한 사용자는 다른 노드 버전이 사용되고 있다고 언급했습니다.

한 사용자는 API를 사용하여 에이전트를 실행하는 것이 성공할지 궁금해하고 있습니다.

---

## GPU MODE Discord
- Arm, AI를 위한 6비트 미래를 선보입니다: Arm은 OCP MXFP6 형식의 AI 지원을 위한 6비트 데이터 타입을 발표했으며, 새로운 Scalable Vector Extension (SVE) 및 Scalable Matrix Extension (SME) 명령어를 통합했습니다.

이러한 추가 사항들은 메모리 사용을 최적화하고 대역폭 요구 사항을 줄임으로써 AI 모델 효율성을 높이는 것을 목표로 하며, 더 강력한 edge 및 embedded AI 애플리케이션을 위한 기반을 마련합니다.
- All2All AMD Kernel 튜닝: 한 사용자가 누락된 custom_kernel 함수를 구현하여 제출물을 수정했습니다. 이는 baseline all2all kernel을 실행하기 위한 진입점입니다.

타임아웃 문제는 코드 최적화를 통해 해결되었으며, 특히 gemm-rs의 경우 계산 및 통신 오버랩 개선으로 1.25배의 속도 향상을 달성했습니다.
- worklist를 사용하여 반복적으로 CUDA graph를 처리합니다: 한 개발자가 while 조건과 worklist 스와핑을 사용하여 CUDA graph 관리를 개선하고자 했으며, 이는 추가적인 device pointer 할당을 줄이는 것을 목표로 합니다.

목표는 CUDA while node를 사용하여 kernel 계산을 최적화하고, 반복적인 graph 실행 중 불필요한 pointer 오버헤드를 피하는 것입니다.
- NVSHMEM Symmetric Heap pointer 논의: nvshmem_malloc()은 collective operation을 수행하여 메모리를 예약하고 symmetric pointer를 반환하며, 이는 특정 패턴 없이 효율적인 원격 메모리 접근을 가능하게 합니다.

이는 GPU당 하나의 process를 필요로 한다는 점에서 NCCL과 다르며(NCCL의 단일 process 지원과 대조됨), symmetric heap은 processing element 전반에 걸쳐 동일하지 않은 pointer 값을 사용하여 접근 효율성을 향상시킵니다.
- 안정적인 프로파일링을 위한 GPU 클럭 고정: 회원들은 nvidia-smi --lock-gpu-clocks를 사용하여 안정적인 프로파일링 결과를 보장하기 위해 GPU 클럭을 고정할 것을 권장합니다.

기본 TDP 주파수를 사용하면 전체 부하에서 안정성이 보장되어 과열로 인한 속도 저하를 방지합니다. 이 접근 방식은 GPU가 가열될 때 불안정성을 유발할 수 있는 최대 주파수 사용과 대조됩니다.

## HuggingFace Discord
- ONNX 커뮤니티, Image-to-Text 모델 변환: 회원들은 ONNX 커뮤니티의 도움과 지도를 받아 VLM과 multimodal 모델을 ONNX로 변환하고 있습니다.

공유된 변환 팁은 HF Discord 채널에서 찾을 수 있습니다.
- Gemma 1B, 오프라인 프롬프트 지원: PromptVault Android 앱은 이제 MediaPipe Tasks를 통해 Gemma 1B IT 모델을 사용하여 on-device AI를 실행하며, 오프라인 프롬프트 생성 및 개선을 지원합니다. 이 앱은 Google Play 스토어에서 이용 가능합니다.

이 앱은 로컬 및 Google Drive (암호화된) 백업, 오프라인 모드, 그리고 제목, 설명, 프롬프트 텍스트 작성을 위한 실험적인 AI 액션 기능을 제공합니다.
- BERT 모델, AI 에세이 분류: 한 회원이 Kaggle의 데이터셋을 사용하여 AI 생성 텍스트와 사람이 작성한 텍스트를 분류하기 위해 BERT 모델을 훈련했습니다.

한 멤버가 현재 데이터셋이 최적의 상태가 아니기 때문에 더 큰 데이터셋에서 훈련 속도를 높이기 위한 아이디어를 찾고 있습니다.
- TrackIO 버전이 DPOTrainer 문제를 일으킵니다: 멤버들은 예시를 따를 때 발생하는 관련 문제인 DPOTrainer/DPOConfig의 문제를 trackio를 버전 0.4.0으로 고정하면 해결된다는 것을 발견했습니다.

이 해결책은 버전 0.5.1에 문제가 있음을 시사합니다.
- GAIA 오류로 인해 클론 대신 스페이스 복제: GAIA 실습 중에 한 사용자가 스페이스를 클론하려고 할 때 500 오류를 겪었고, 대신 스페이스를 복제하라는 조언을 받았습니다.

해당 과정은 사용자들에게 클론 대신 스페이스를 복제하도록 명시적으로 지시했지만, 그 이유는 명시하지 않았습니다.

---

## Modular (Mojo 🔥) Discord
- Pixi가 문제를 해결했습니다: 한 사용자가 nightly 채널 대신 max-nightly 채널로 전환함으로써 Pixi를 사용하여 'Access denied' 오류를 해결했습니다.

Pixi는 uv와 빠른 conda 구현의 조합과 같습니다.
- Mojo의 MAX, TensorFlow 및 PyTorch를 대체하는 것을 목표로 합니다: 한 멤버가 MAX가 잠재적으로 TensorFlow와 PyTorch를 대체할 수 있다고 언급하며, 현재 동등성(parity)과 오픈소스 상태에 대해 문의했습니다.

다른 멤버는 MAX가 추론 성능에서 거의 동등하거나 앞서 있지만, 부분적으로만 오픈소스이며 내년 초에 추가로 오픈소스화할 계획이 있다고 답변했습니다.
- Mojo GPU 컴퓨팅 기능, CUDA와 정면 대결: 한 멤버가 Mojo가 CUDA에서 사용 가능한 모든 기능을 재현할 수 있는지 문의했습니다.

다른 멤버는 Mojo가 현재 GPU에서 그래픽 하드웨어와 상호 작용하는 최적의 방법이 부족하지만, 사실상 모든 계산 작업은 달성 가능하다고 답변했습니다. 충족되지 않은 요구 사항에 대한 기능 요청은 환영합니다.
- 난관 극복: C Libraries와 Mojo: 한 멤버가 Mojo 내에서 pthreads와 같은 C Libraries를 활용할 가능성에 대해 문의했습니다.

한 멤버는 C Libraries가 일반적으로 잘 통합되지만, pthreads는 Mojo의 런타임 환경과 표준 라이브러리 함수에 미치는 영향 때문에 이상적이지 않을 수 있다고 답변하며, Mojo의 현재 동시성 모델이 불완전하다는 점을 언급했습니다.

---

## Nous Research AI Discord
- Sora 2, 저작권이 있는 콘텐츠 금지: 좋은 애니메이션 결과물로 멤버들을 놀라게 한 후, Sora 2 video generation model은 prompts를 다시 작성하고 밤새 저작권이 있는 콘텐츠를 전면적으로 금지하기 시작했으며, 한 트윗에 따르면 이는 사실상 창의적 잠재력을 마비시키는 것입니다.

회원들은 그 경험을 "speedrunning enshittification"이라고 묘사하며 실망감을 표했습니다.
- LPDDR5X, Ryzen AI Mini PC를 압도합니다: AI 테스트를 위해 128GB LPDDR5X를 탑재한 HP G1a R-AI-Max+ Pro 395를 확보했습니다. 이 LPDDR5X는 온보드에 납땜되어 교체할 수 없지만, 일반적으로 DDR5보다 훨씬 빠르고 전력 소비량이 적습니다.

이 Samsung Semiconductor 링크에 따르면, LPDDR5X는 DGX Spark와 인기 있는 Ryzen AI Mini PC가 8000 Mhz RAM 속도와 250GB/s 이상의 버스 속도를 달성할 수 있도록 합니다.
- Qwen VL, 클럭 부정확성 경험: 회원들은 Qwen 2.5 VL의 특이한 점에 대해 논의했습니다. 이 모델은 비전 작업에서 더 작은 모델이 때때로 더 큰 모델보다 뛰어난 성능을 보이지만, 텍스트를 제공했을 때는 lost-in-the-middle 현상을 겪습니다.

한 클럭 판독 테스트에서, 이 모델은 두 개의 클럭을 완전히 놓치고 대부분 틀린 것으로 나타났습니다. 반면 로컬 vllm 실행은 3/5를 맞혔고, 뒤집힌 숫자를 제외하고는 대부분 정확했습니다.
- Transformer, 다단계 손실 함수 학습: 한 회원이 다단계 Transformer 손실을 사용한 실험에 대해 설명했습니다. 이는 Transformer의 중간 레이어 중 일부에서 hidden vector를 선택하여 입력 vector에 추가하고, 또 다른 forward pass를 수행하는 방식입니다.

그들은 이 실험이 매우 단순한 gemma 3 270 mit 모델에서 수행되었다고 언급했습니다. 하지만 cli agent가 훈련 및 알고리즘 조작 실험에 대한 접근을 가능하게 하는 것이 정말 놀랍다고 덧붙였습니다.
- Dragon Hatchling 논문, 아이디어 제시: 한 회원이 "THE DRAGON HATCHLING: THE MISSING LINK BETWEEN THE TRANSFORMER AND MODELS OF THE BRAIN"이라는 제목의 논문(Arxiv link) 링크를 공유했습니다. 이 논문은 Transformer와 뇌 모델 간의 잠재적 연결을 탐구합니다.

뇌 모델과 Transformer 아키텍처에 대한 논의가 진행 중입니다.

---

## Latent Space Discord
- DeepSeek, CUDA의 중국 장악력 완화: DeepSeek은 FP8 spec과 TileLang 언어를 통해 Nvidia의 CUDA 록인(lock-in)에 도전하고 있습니다. 이는 중국 칩 재고를 위한 공유 표준과 프로그래밍 브리지를 육성하는 것을 목표로 합니다. 자세한 내용은 X에서 확인하세요.

이러한 움직임은 중국에게 전략적 제휴를 의미하지만, CUDA와 비교했을 때 여전히 남아있는 성능 격차에 대한 의구심은 남아 있습니다.
- Sora 2 다큐멘터리 공개: OpenAI는 Sora 2를 사용하여 제작된 30초 분량의 AI 생성 단편 "The Quack: Part 1"을 공개했습니다. 이는 초대 코드(FRIYAY)를 포함한 오리 테마 밈과 함께 큰 기대감을 불러일으켰으며, X에 링크되어 있습니다.

이 발표는 AI 비디오 생성 능력의 급속한 발전을 강조합니다.
- 에이전틱 브라우저가 AI 격전지가 되다: AI 기업들은 현재 에이전틱 브라우저 분야에서 경쟁하고 있으며, 이는 지난 금요일 AIIA 통화에서 Claude의 새로운 브라우저를 통해 입증되었습니다. 초기 사용자들은 그 기능을 주목하고 있습니다.

이러한 추세는 더욱 통합되고 AI 기반의 브라우징 경험으로의 중요한 변화를 보여줍니다.
- Sora의 우주비행사 승마가 Gen-AI를 혁신하다: Pliny는 Sora 2 Pro가 작년의 불가능했던 정지 이미지 프롬프트에서 인상적인 저중력 비디오로 발전한 것을 높이 평가했습니다. 이 비디오는 우주비행사 위에 올라탄 사실적인 말과 즉흥적이고 자발적으로 생성된 미션 컨트롤 유머를 담고 있습니다 (xcancel.com).

이 논의는 빠른 Gen-AI 발전, 벤치마크, 그리고 내부 농담을 강조합니다.
- OpenAI의 Medal 인수 시도가 중단되다: 보도에 따르면, OpenAI는 훈련용 영상을 수집하기 위해 게이머 비디오 플랫폼인 Medal을 인수하고자 5억 달러를 제안했지만, 거래는 무산되었습니다 (xcancel.com).

Medal은 자체 AI 부서인 General Intuition을 출범하고 있으며, 현재 1억 달러 규모의 펀딩 라운드를 마무리하고 있습니다.

---

## Yannick Kilcher Discord
- 로보-로맨스가 미래를 예고하다: AI 섹스 로봇이 언제쯤 덜 어색해질지에 대한 논의는 서투르지만 6천 달러의 저렴한 로봇인 Unitree R1의 도입과 동시에 이루어졌습니다 (X link).

이 대화는 영국의 연령 확인 요건을 다루었으며, 일부 회원들은 특이점(singularity)과 로봇 아내를 예상했습니다.
- Discord 데이터 대혼란이 공개되다: Discord 고객 서비스 데이터 유출이 The Verge에 의해 보고되었습니다 (The Verge).

이는 Dan Chollet의 안전 벤치마크 (Tweet 1, Tweet 2)와 DeepSeek AI 모델에 대한 NIST 평가 (NIST report)가 잠재적인 단점과 위험을 드러낸 시점에 발생했습니다.
- 새로운 논문이 Low-Rank Gradients를 탐구하다: 한 회원이 새로운 논문인 Low Rank Gradients를 공유하며, 이 논문이 상당한 LA (Linear Algebra)를 포함한다고 언급하여 관심과 가능한 탐구 세션을 불러일으켰습니다.

다른 회원은 복잡한 수학이 관련될 때 검은 고양이 사진을 가진 특정 회원을 영입하는 것을 좋아한다고 농담하며, 그가 훌륭한 공동 조종사라고 말하고 고양이 비디오를 공유했습니다.
- Diffusion 모델이 신호 강화를 요구하다: 한 회원은 문제를 Diffusion 프로세스로 구성할 것을 제안하며, conditioning signal이 과소 평가될 수 있다고 언급하고 conditioning/guidance weight를 늘릴 것을 권장했습니다.

다른 멤버도 모델이 배경에 대해 underfitting하고 있다고 언급하며, 더 강력한 conditioning signal의 필요성을 입증했습니다.
- GPT-5의 수학적 능력 예측: 수학자들이 GPT-5가 수학 문제 해결을 돕는다는 주장이 [Tweet 1](Tweet 1)과 [Tweet 2](Tweet 2) 링크와 함께 공유되고 있습니다.

한 멤버는 첫 번째 트윗이 2025년 8월 1일에 게시되었다고 언급하며, 이는 미래의 주장임을 암시한다고 말했습니다.

---

## Eleuther Discord
- 인간이 FVD보다 Diffusion Model을 더 잘 평가합니다: 멤버들은 Diffusion Model 평가에 대해 논의하며, 이미지에는 FID/CLIPScore, 수동 인간 평가, 비디오에는 FVD와 같은 자동화된 지표를 사용한다고 언급했습니다.

한 멤버는 비디오 평가에 대한 호기심을 표현하며, Sora 2와 함께 이미지 평가 방법과 비교했을 때 비디오 평가가 상대적으로 원시적이라고 언급했습니다.
- Gemma의 아키텍처: 성공적이지 않음: LM Arena에서 강력한 성능을 보였음에도 불구하고, Gemma의 아키텍처는 Qwen의 아키텍처만큼 널리 채택되지 않았습니다.

한 멤버는 LLM 성능에서 아키텍처 단독보다 학습 데이터와 파인튜닝 분포가 더 중요한 요소라고 주장했습니다.
- Synaptik Core, 검증 가능한 AI를 약속합니다: Synaptik Core의 Janay가 AI 시스템에서 검증 가능하고 장기적인 메모리 및 감사 가능성을 위한 툴체인인 Synaptik Core를 소개했습니다.

그녀는 AI 에이전트와 OpenAI Open Model Hackathon을 앞둔 자신의 스프린트를 보여주는 [LinkedIn 게시물](link)을 공유했습니다.
- 더 쉬운 학습을 위한 AO3 서브셋 등장: 멤버들은 TinyStories와 유사하게 더 쉬운 학습을 위해 더 간단한 문법 구조를 가진 AO3 스토리 서브셋을 만드는 것을 탐색했습니다.

그룹은 가독성 점수를 사용하여 데이터를 필터링하는 것을 고려했으며, 동시에 잠재적인 노이즈 제거 트레이드오프도 인정했습니다.
- nanoGPT 스피드런 기록 경신: 한 멤버는 LessWrong 게시물을 공유하며 nanoGPT 스피드런 세계 기록이 3개월 만에 20% 하락했다는 점을 강조했습니다.

이는 더 작은 규모에서도 빠른 발전이 여전히 일어나고 있음을 시사합니다.

---

## MCP Contributors (Official) Discord
- GitHub 팀, Infrastructure-As-Code 채택: 팀은 GitHub 팀 관리를 infrastructure-as-code로 마이그레이션하여, [modelcontextprotocol/access](modelcontextprotocol/access)에서 코드를 통해 멤버십 및 리포지토리 권한을 관리합니다.

마이그레이션은 커뮤니티 소유권, 투명성, 감사 추적, AI 친화적인 액세스 관리를 목표로 하며, 배포 중 짧은 액세스 중단이 예상됩니다.
- MCP Tools의 버전 관리 문제: Intuit 엔지니어 한 분이 MCP Servers에서 MCP Tools의 버전 관리 문제에 직면하고 있으며, 특히 대규모 종속성 관리 및 호환성 문제에 대해 협력자를 찾고 있습니다.

그들은 잠재적인 해결책을 담은 SEP 초안을 작성했으며, modelcontextprotocol/modelcontextprotocol#1575에서 확인할 수 있고 피드백을 구하고 있습니다.
- Cloudflare의 Code Mode, 과도한 엔지니어링 비난에 직면: Cloudflare의 Code Mode가 논의되었으며, 일부는 이 블로그 게시물에 따라 Code Mode가 MCP를 오해하거나, 툴 호출을 Cloudflare worker에 대한 요청으로 과도하게 엔지니어링했다고 주장했습니다.

일부 회원들은 Code Mode가 에이전트가 결과를 전달하는 데 필요한 턴 수를 줄이는지 여부에 대해 논의한 반면, 다른 회원들은 성능에 대한 우려를 표명하고 이 프로토타입을 기반으로 웹 API나 클라이언트 SDK와 비교하여 좋지 않게 평가했습니다.
- AppsSDK, MCP-UI 중복 논쟁 촉발: OpenAI는 그들의 발표에 따라 TheFork를 론칭 파트너로 삼아 ChatGPT에 MCP와 함께 UI를 도입하며 AppsSDK를 출시했습니다.

회원들은 MCP-UI와 연동하는 것이 더 나은 움직임이었을지 궁금해하지만, OpenAI는 이들이 자연스럽게 어우러지도록 할 계획이며 ACP를 사용하는 앱의 트랜잭션을 전적으로 지원할 것입니다.
- MCP 기능 지원 매트릭스 해독: 한 회원이 Model Context Protocol의 Feature Support Matrix에서 Discovery의 의미에 대해 문의했습니다.

다른 회원은 Discovery가 서버 기능과 툴 변경 사항을 전달하는 능력을 의미한다고 설명했습니다.

---

## Moonshot AI (Kimi K-2) Discord
- Kimi-latest는 Kimi-K2가 아닙니다: ‘Kimi-latest’라는 별칭은 Kimi 어시스턴트를 구동하는 비공개 프로덕션 Kimi LLM을 의미하며, 반면 ‘Kimi-K2’는 오픈 웨이트 moe 패밀리(예: k2-0905)를 나타냅니다.

moonshot.ai에 있는 ‘proprietary llm’ 문구는 K2가 UI에서 두드러지게 나타남에도 불구하고 K2와는 별개의 비공개 스택에 해당합니다.
- Em Dash에 대한 의견 분분: 한 사용자가 AI와 상호작용하지 않을 때 em dash가 포함된 메시지를 무시하는지 다른 사람들에게 물었습니다.

한 응답자는 봇으로 오해받는 것을 피하기 위해 자연스러운 em dash 사용을 자제한다고 인정하며, 이미지 매크로를 통해 Kimi에 대한 애정을 표현했습니다.
- 번역 중 Kimi 검열 문제: 한 회원이 Kimi의 검열로 인해 출력을 지우고 사과문으로 대체될 수 있다고 보고했습니다.

그들은 백만 Token 컨텍스트 윈도우와 우수한 번역 기능 때문에 번역에 Qwen을 활용할 것을 권장했습니다.
- 주주, 재미와 이익을 추구하다: 한 사용자가 Alibaba 주식 비중이 8%인 펀드에 100달러 이상을 소유하고 있다고 밝혔습니다. 이 펀드는 Moonshot에 약 35%의 지분을 가지고 있습니다.

해당 멤버는 삶의 목적이 주주 가치를 극대화하는 것이라고 농담조로 주장하여 다른 사람들의 웃음을 자아냈습니다.

---

## aider (Paul Gauthier) Discord
- Deepseek, Claude로 브라우저 테스트: 한 멤버가 Deepseek 브라우저를 Claude CLI 및 Chrome DevTools MCP로 테스트하는 것에 대한 블로그 게시물을 공유했습니다.

그들은 도구 작업에서 더 나은 성능을 보이기 때문에 Claude Code와 Opencode에서 Anthropic API를 통해 Deepseek을 사용하기도 합니다.
- Aider에 수동 제어가 절실히 필요합니다: 한 멤버는 Aider에서 가장 그리운 기능이 /tokens, /add, /remove, /clear와 같은 컨텍스트에 대한 수동 제어라고 말했습니다.

그들은 대규모 코드베이스의 경우 Aider가 이러한 기능 없이는 경쟁력이 없으며, 다른 어떤 도구도 아직 이를 구현하지 않았다고 주장합니다.
- Agentic Grep이 Aider에 이점을 줄 수 있습니다: 멤버들은 agentic 도구가 regex grep을 사용하여 필요한 부분을 찾고 주변 라인을 확인하는 방식에 대해 논의했으며, Aider에 ripgrep agentic 핸들러가 없다는 점을 언급했습니다.

다른 멤버도 agentic grep이 Aider를 현재 세대의 도구들과 경쟁력 있게 만드는 데 정말 도움이 될 것이라고 말하며 동의했습니다.
- Aider 프롬프트 캐시 기본 활성화: Aider의 models.py는 이제 Z.aialso 제공자에 대해 `cache_control: bool = True`와 `caches_by_default: bool = True`를 설정하며, 환영 메시지에 "prompt cache"를 추가합니다.

새로운 환영 메시지는 "Main model: openrouter/deepseek/deepseek-v3.2-exp with diff edit format, 8k think tokens, prompt cache"와 유사할 것입니다.
- 오픈 vs 클로즈드 가중치: 가격 우려 부상: 논의는 오픈소스와 클로즈드소스 AI 모델 가중치를 비교하는 방향으로 전환되었으며, 클로즈드 가중치 소유자들이 마음대로 가격을 인상할 수 있다는 우려가 강조되었습니다.

지속적인 논의는 Aider 도구의 최신 업데이트 및 기능에 대한 정보를 유지하고, 사용자들이 그 잠재력을 최대한 활용할 수 있도록 보장하는 데 중점을 둡니다.

---

## DSPy Discord
- Neosantara AI, LLM Gateway 오픈: Neosantara AI는 AI 앱 구축을 위한 새로운 LLM Gateway Platform을 출시했으며, DSPy 통합을 위한 무료 액세스 및 문서를 제공합니다.

신규 사용자는 가입 시 매월 1만 개의 consume Token을 받으며, 피드백은 halo@neosantara.xyz로 보내주시면 됩니다.
- DSPy 로드맵 요청 (Issues 외): 한 멤버가 기존 GitHub issues 및 변경 로그(changelog)를 넘어선 DSPy 로드맵을 요청했습니다. 이는 X/Twitter에서 최근 언급된 내용을 인용하며 제기되었습니다.

Drew Houston의 게시물과 Huyen Chip의 ReAct+Reflection에 대한 블로그 게시물 링크가 커뮤니티 참여의 예시로 공유되었습니다.
- Elysia, Agent의 ReAct 지원: 더 긴 Agent 단계(steps)를 위해 ReAct 궤적(trajectories)을 디스크에 저장할지 메모리에 저장할지에 대한 논의가 있었으며, Weaviate의 Elysia를 사용하자는 제안이 있었습니다. Elysia는 Decision Tree를 포함하는 DSPy 프로젝트입니다.

한 멤버는 ReAct 부분의 전체 궤적(trajectory)을 반영하기 위해 ReAct+Reflection을 구현하는 것을 고려하고 있습니다.
- Fallback 문제 해결: 한 멤버가 DSPy에서 fallback 동작을 수정하는 것에 대해 문의했습니다.

현재 하드코딩된 fallback 메커니즘은 변경할 수 없습니다.
- DSPy-ReAct-Machina 등장: 한 멤버가 DSPy-ReAct-Machina를 출시했습니다. 이는 단일하고 성장하는 context history를 통해 다중 턴(multi-turn) 대화를 용이하게 하는 DSPy를 위한 대체 ReAct 구현이며, PyPI에서 사용할 수 있습니다.

그들은 또한 동기(motivation)와 아키텍처(architecture)를 상세히 설명하는 블로그 게시물을 공유하며 커뮤니티의 의견을 구했습니다.

---

## tinygrad (George Hotz) Discord
- tinygrad 개발자, 현상금 프로그램 참여: tinygrad에 합류하려는 사람들은 자신의 기술을 입증하기 위해 현상금 프로그램(bounty program)에 참여해야 합니다.

tinygrad 팀은 개인 면접이나 직접 채용을 진행하지 않는다고 명확히 밝혔습니다.
- tinygrad NIR Backend 검토 준비 완료: NIR backend가 이제 검토 준비를 마쳤습니다 (PR #12089).

엔지니어들은 backend의 기능을 감사하고 테스트하는 것을 환영합니다.
- tinygrad, match statement 탐색 예정: 팀은 패턴 매처(pattern matcher)를 컴파일할 때 match statement를 사용하는 것을 고려하고 있습니다.

이는 반복되는 if statement를 대체하여 렌더링된 코드(rendered code)를 개선할 것입니다.
- 3dgs Repo, tinygrad 포팅 고려: 3dgs repo (LichtFeld-Studio)의 관리자는 libtorch를 제거할 계획이며, inference 및 CUDA 지원을 위해 tinygrad를 C++로 포팅하는 것을 고려하고 있습니다.

한 멤버는 모델을 CUDA 또는 C kernel로 컴파일한 다음 해당 kernel이 포함된 C 코드를 내보내고 EfficientNet C 예시로 연결하는 것을 제안했습니다.

## Manus.im Discord Discord
- 리드 생성 MCP 서버 활성화: 도매 리드 생성을 위한 MCP 서버가 wrangler/Cloudflare를 사용하여 배포되었습니다. 이 서버는 특정 정부 웹사이트에서 저평가된 부동산과 적극적인 판매자를 찾아내고, 최종 구매 제안을 위한 분석을 세분화하는 데 사용되며, [wholesale-lead-generator-frontend.pages.dev](wholesale-lead-generator-frontend.pages.dev)에서 이용할 수 있습니다.

  한 사용자가 "삭제되기 전에 멀웨어를 잡으세요"라고 농담조로 말했습니다.
- Manus iOS 클라이언트 충돌 버그 해결: Manus iOS 클라이언트에서 예약된 작업 인터페이스에 텍스트를 입력하기 위해 선택할 때 100% 프리즈/충돌을 일으키는 버그가 보고되었습니다.

  해결책으로 "별도의 앱에 명령어나 텍스트를 작성하세요... 해당 필드 내에서 텍스트를 선택하거나 편집하는 것을 피하세요"라는 방법과 함께, Apple의 내장 Shortcuts 앱 및 키보드의 내장 클립보드 관리자를 활용하는 방법이 제시되었습니다.
- 경량 상황 인지 AI 도구 탐색: 한 멤버가 개인적인 맥락이 부족하여 사용자가 선택한 데이터에 연결하지 못하고, 특정 질문으로 지식 격차를 파악하며, 어떤 도구에서든 사용될 수 있도록 사용자 목표를 이해하는 AI 도구를 위한 경량 솔루션을 탐색하고 있습니다.

  이 멤버는 솔루션을 개선하고 테스트하는 데 도움을 줄 Bay Area 내의 협력자와 얼리 어답터를 적극적으로 찾고 있습니다.

---

## MLOps @Chipro Discord
- AI 서밋, Feature Store 집중 조명: Feature Store Summit은 10월 14일 오전 8시 30분 PT (오후 5시 30분 CET)에 예정되어 있으며, Uber, Pinterest, Zalando, Lyft, Coinbase의 연사들이 참여합니다.

  논의는 AI, ML을 위한 인프라, 대규모 확장성과 실시간 성능을 요구하는 애플리케이션, 그리고 2025년 Feature Store를 형성할 트렌드를 포함할 예정입니다. 등록은 [이 링크](this link)를 통해 가능합니다.
- AI 서밋, 대규모 실시간 엔지니어링 탐구: 다가오는 서밋의 강연에서는 대규모 실시간 Feature Engineering, Vector Database, 그리고 프로덕션 환경에서의 Generative AI에 대해 심층적으로 다룰 예정입니다.

  추가적으로, 배치 및 실시간 워크플로우의 균형에 대해서도 탐구할 것입니다.
- AGI: 무의미한 포괄적 용어인가?: 한 멤버가 AGI의 정의에 의문을 제기하며, 명확한 일반성 기준이 없는 무의미한 포괄적 용어라고 일축했습니다.

  그들은 또한 인간 지능에 대한 신뢰할 수 있는 측정 기준이 없다는 점도 언급했습니다.

---
LLM Agents (Berkeley MOOC) Discord에는 새로운 메시지가 없습니다. 이 길드가 너무 오랫동안 조용했다면 저희에게 알려주십시오. 그러면 저희가 이를 제거하겠습니다.

Windsurf Discord에 새 메시지가 없습니다. 이 길드가 너무 오랫동안 조용했다면 저희에게 알려주십시오. 그러면 저희가 제거하겠습니다.

---
이 이메일은 귀하가 저희 사이트를 통해 수신을 동의하셨기 때문에 발송되었습니다.
이 이메일을 받는 방식을 변경하고 싶으신가요?
이 목록에서 구독을 취소할 수 있습니다.

---

# Discord: 채널별 상세 요약 및 링크

### LMArena ▷ #general (1252개 메시지🔥🔥🔥):
> Perplexity 학생 할인, Comet Browser, ChatGPT 부정행위, 학습용 Gemini, AI의 물 사용량
- 학생들이 Perplexity의 매력적인 보조금을 얻기 위해 경쟁합니다: 한 회원이 Perplexity 학생 할인을 공유했지만, 활성 학생 신분이 필요하다고 경고했으며, 이는 추천 보너스를 얻기 위한 투명한 시도로 이어졌습니다.
- Comet Browser: 학습 도우미인가, 아니면 컨닝 페이퍼인가?: 회원들은 Comet Browser에 대해 논의했으며, 한 회원은 이를 공부하기 좋은 곳이라고 설명했고, 다른 회원은 농담으로 컨닝하기 좋은 곳이라고 불렀습니다.

Comet의 어시스턴트에 대한 의견은 엇갈렸는데, 일부는 ChatGPT 또는 Gemini에 비해 부족하다고 평가했고, 다른 일부는 음성 모드를 칭찬했습니다.
- AI는 목마른가? 물 소비량 논의: 회원들은 AI의 환경적 영향, 특히 데이터 센터 냉각을 위한 물 사용량에 대해 논의했으며, 한 회원은 ChatGPT의 20개 답변이 약 0.5리터의 물을 소비한다고 제안했습니다.

한 회원은 AI 데이터 센터 근처의 지역 주민들이 어려움을 겪고 있다고 말했고, 다른 회원은 미래에는 열을 방출하기 위해 데이터 센터를 우주에 배치하는 것이 포함될 것이라고 말했습니다.
- Gemini 3 유출: 10월 9일 출시?: 채팅 회원들은 초기 테스터 및 업계 내부자들의 보고서를 인용하여, AI Studio와 같은 플랫폼에서의 철저한 테스트를 거쳐 2025년 10월 9일이 공식 출시일이 될 가능성이 높다고 언급했습니다.

한 회원은 AI Ultra 사용자에게만 제공되기를 바랐습니다.
- Hunyuan 3: 첫인상: 한 사용자가 Tencent의 새로운 이미지 생성기인 Hunyuan 3에 대한 첫인상을 밝혔는데, Qwen 또는 Flux보다 낫고 Imagen과 비슷하다고 평가했습니다.

다른 회원은 이러한 중국 모델들이 영어로 번역할 때 미묘한 문화적 세부 사항과 맥락을 번역하고 정확하게 묘사하는 데 종종 어려움을 겪는다고 언급했습니다.

---

### OpenAI ▷ #announcements (4개 메시지):
> GPT-5 Instant, Sam Altman 기조연설, OpenAI 도구를 사용하는 스타트업
- GPT-5 Instant, 고통 인식 기능 강화: OpenAI는 사용자가 고통을 겪는 순간을 더 잘 인식하고 지원하기 위해 GPT-5 Instant를 업데이트하고 있습니다.

대화의 민감한 부분은 이제 GPT-5 Instant로 라우팅되어 신속하게 유용한 답변을 제공합니다. ChatGPT는 요청 시 어떤 모델이 활성화되어 있는지 사용자에게 계속 알려줄 것입니다.
- Sam Altman의 기조연설 라이브 스트리밍: Sam Altman의 기조연설이 라이브 스트리밍되고 있으며, 제공된 OpenAI 웹사이트 링크를 통해 접속할 수 있습니다.

DevDay [2025]는 내일 오전 10시(PT)에 시작합니다.
- 스타트업, OpenAI 도구 활용: @Cursor_ai, @getSchoolAI, @AbridgeHQ, @Jamdotdev와 같은 스타트업들이 OpenAI와 함께 OpenAI 도구를 사용하여 각자의 산업을 어떻게 혁신하고 있는지 공유합니다.

행사는 오전 11시 25분(PT)에 시작하며 OpenAI 팟캐스트 링크를 통해 시청할 수 있습니다.

---

### OpenAI ▷ #ai-discussions (1074 messages🔥🔥🔥):
> Gemma 3 12B multimodal, Arduino AI 마이크로컨트롤러, ChatGPT Business 무료, AI 음성 복제, Sora 2 저해상도 품질
- Arduino, AI 마이크로컨트롤러 개발: Arduino는 NPU 기반 ARM과 유사한 AI 지원 마이크로컨트롤러를 개발하고 있으며, 더 작은 Hailo-8과 같은 장치일 것으로 추정됩니다.

Raspberry Pi에 Hailo-8L을 사용하여 한 사용자는 UGV Waveshare 장난감 로봇 탱크에 장착하여 2와트 미만의 전력으로 2000+ FPS의 객체 인식을 시연했습니다.
- ChatGPT Business 무료 이용 가능: 사용자들은 해당 제안에 대한 직접 링크를 사용하여 최대 5개의 시트를 가진 ChatGPT Business를 한 달간 무료로 이용할 수 있다는 것을 발견하고 있으며, 그 후에는 시트당 30달러가 청구됩니다.

무료 액세스는 무제한 계정 생성에 대한 논의와 비즈니스 플랜 내 Codex의 가치에 대한 논의로 이어졌습니다.
- OpenAI 음성 업그레이드 및 AI 음성 복제: 회원들은 OpenAI가 음성 모델에 인터넷 검색 기능을 추가하여 더 빠르고 심층적으로 만들었다고 언급했으며, 다른 사용자들은 AI 음성 복제 클립을 공유했는데, ElvenLabs가 최고로 꼽혔습니다.

논의는 딥페이크 시대에 대한 우려와 무엇이든 생성될 수 있다는 인식의 필요성을 포함했습니다. 한 사용자는 매일 AI Chris Hitchens의 음성을 듣는다고 했고, 다른 사용자들은 인터넷에서 보는 모든 것을 믿는 사람들이 많기 때문에 불안해했습니다.
- Sora 2의 조악한 결과물: 사용자들은 Sora 2의 낮은 해상도 품질과 워터마크가 저렴하게 느껴지게 한다고 언급했습니다.

많은 이들이 OSS 20B 모델이 더 낫다고 주장합니다.
- Grok, Waifu 및 비디오로 입지 강화: SuperGrok 구독자들은 모바일에서 Ani와 대화할 수 있으며 (waifu 기능), 놀라운 품질을 보여줍니다.

사용자들은 Grok이 생성한 비디오를 공유했습니다 (Sora에서 금지당한 것을 알게 된 스폰지밥을 상상해 보세요라는 프롬프트로 생성됨). 비록 일부는 형편없고, 저작권을 침해하며, 일관된 스토리가 부족하다고 평가했지만 말입니다.

### OpenAI ▷ #gpt-4-discussions (19 messages🔥):
> GPT Instant, Error in the stream, GPT-4o switch, GPT publishing review time, NSFW filter
- GPT Instant 선호도 유도: 한 멤버가 ChatGPT가 GPT Instant만 사용하고 'thinking mini'를 사용하지 않도록 강제하는 방법을 물었습니다.

다른 멤버는 농담으로 "협박하세요"라고 제안했습니다.
- 스트림 오류가 GPT-5 사용자들을 괴롭히다: 한 멤버가 GPT-5에 메시지를 보낼 때 GPT-4에서는 발생하지 않는 이상한 'Error in the stream' 메시지를 받았다고 보고했습니다.

'Error in the stream' 메시지에 대한 해결책은 제시되지 않았습니다.
- GPT-4o 토글 문제: 한 멤버가 GPT-4o로 전환할 수 없는 것 같다고 보고하며, 해당 토글을 일종의 플라시보라고 묘사했습니다.

GPT-4o 플라시보 토글에 대한 해결책은 제시되지 않았습니다.
- 사용자, NSFW 필터 우회 실패 경험: 한 멤버가 ChatGPT가 갑자기 NSFW 필터 우회를 거부하는지 문의했습니다.

한 멤버는 안전 메커니즘을 우회하는 것은 TOS에서 금지되어 있다고 답변했습니다.
- iPhone에서 ChatGPT 앱이 제대로 작동하지 않음: 한 멤버가 iPhone에서 GPT 앱을 사용할 때 제대로 작동하지 않고 응답이 없다고 보고했습니다.

그들은 앱을 삭제하고 다시 설치해 보았지만, 웹에서만 작동한다고 보고했습니다.

---

### OpenAI ▷ #prompt-engineering (57 messages🔥🔥):
> Sora Prompts for Infographics, Copyright Restrictions on Generating DBZ Content, Realistic POV Horror Video Prompts, Tailoring Resume for ATS with AI, Minimalistic Communication Style for ChatGPT
- Sora, JSON 컨텍스트 프로필 생성에 어려움 겪어: 한 멤버가 건강 연구를 위해 Sora로 만든 인포그래픽에 대한 일관된 JSON 컨텍스트 프로필을 만들려고 합니다.

그들은 또한 Pokemon, Demon Slayer, Hunter x Hunter, Jujutsu Kaisen 및 'video ia de Makita'에 대한 프롬프트를 요청하고 있습니다.
- 저작권, DBZ 콘텐츠 생성 제한: 한 사용자가 Dragon Ball Z (DBZ) 프롬프트 생성을 도와달라고 요청했지만, 한 멤버는 새로운 저작권 제한으로 인해 저작권이 있는 콘텐츠 생성이 불가능하다고 답변했습니다.

그들은 Goku와 Vegeta, 세계, 애니메이션 스타일, 의상, 톤을 묘사하라고 조언했지만, DBZ와는 크게 닮지 않을 수 있다고 경고했습니다.
- POV 공포 비디오 프롬프트 제작: 한 사용자가 현재 프롬프트로는 비디오 게임과 같은 품질의 결과물이 나와 현실적인 1인칭 시점(POV) 공포 비디오를 만드는 데 도움을 요청하고 있습니다.

회원 한 분이 원치 않는 비디오 게임/만화 스타일을 피하기 위해 원하는 이미지 유형을 명시할 것을 제안했습니다.
- **AI로 맞춤 제작된 ATS 친화적인 이력서:** 한 회원이 Applicant Tracking System (ATS) 점수가 높은 직무에 맞춰 이력서를 작성하기 위한 강력한 프롬프트 작성에 도움을 요청했습니다.

다른 회원은 이력서를 맞춤 제작하기 위한 흥미로운 프롬프트 엔지니어링 프레임워크인 “[The_ATS_Resonance_Engine.md](The_ATS_Resonance_Engine.md)”를 공유했습니다.
- **ChatGPT에 미니멀리즘 지시:** 한 회원이 ChatGPT가 친근함, 상세한 설명 또는 비공식적인 상호작용을 배제하고 엄격하고 미니멀리즘적인 커뮤니케이션 스타일을 채택하도록 지시하는 프롬프트를 공유했습니다.

목표는 ChatGPT가 불필요한 대화 없이 직접적이고 정확한 답변을 제공하는 데만 집중하는 차갑고 간결한 어시스턴트 역할을 하는 것입니다.

---

### OpenAI ▷ #api-discussions (57 messages🔥🔥):
> Art Prompts 저장, Sora 및 인포그래픽용 JSON Context, 저작권 콘텐츠 생성, 비디오 품질 개선, AI를 활용한 이력서 맞춤 제작
- **Art Prompts 정리 강점:** 회원들은 Google Sheets, 로컬 markdown 파일, ChatGPT 내 채팅 스레드 등 Art Prompts를 저장하는 다양한 방법에 대해 논의했습니다.

한 사용자는 ChatGPT의 프로젝트 폴더를 사용하여 다른 렌더링 그룹을 분리한다고 언급했습니다.
- **Sora의 과학적 스타일 고군분투:** 한 사용자가 Sora를 사용하여 건강 연구 인포그래픽에 대한 일관된 JSON context 프로필을 달성하는 데 어려움을 겪고 있다고 보고했습니다.

다른 사용자는 ChatGPT에 Sora 프롬프트를 요청할 때 카메라 앵글을 구체적으로 지정할 것을 제안했습니다.
- **저작권 콘텐츠 생성 제한:** 사용자들은 저작권이 있는 콘텐츠 생성이 제한되었으며, 이는 법적 위협 때문일 가능성이 높고, 사용자들이 저작권이 있는 캐릭터나 이름을 언급하지 않고 아이디어를 설명해야 한다고 언급했습니다.

한 사용자는 이러한 제한에 대해 한탄하며, 미래에 이러한 제한을 우회할 도구에 대한 희망을 표했습니다.
- **공포 비디오 헬프 데스크:** 한 사용자가 사실적인 POV 공포 비디오 생성에 도움을 요청했습니다. 출력물이 때때로 비디오 게임과 같은 품질이었기 때문입니다.

한 회원은 모델이 추측하여 비디오 게임/만화 스타일을 선택할 가능성을 방지하기 위해 원하는 이미지 유형을 명시할 것을 제안했습니다.
- **ATS 상승 에이스 어시스턴트:** 한 사용자가 더 높은 ATS 점수를 가진 직무에 맞춰 이력서를 작성하기 위한 강력한 프롬프트 작성에 도움을 요청했습니다.

다른 사용자는 "[The ATS Resonance Engine.md](The_ATS_Resonance_Engine.md)" 파일 링크로 응답했습니다.

---

### Unsloth AI (Daniel Han) ▷ #general (842 messages🔥🔥🔥):
> GLM 4.6 on 5090, B200 rental, Model Quality vs Precision, Platform Sidebars
- GLM 4.6은 5090에 맞지만 느리게 작동합니다: 한 사용자가 200GB RAM이 장착된 5090에서 GLM 4.6을 실행할 수 있는지 문의했으며, Q4_K_M 이하 버전은 맞지만 느리게 작동할 것이라고 제안했습니다.

다른 사용자는 맞을 것이지만 느릴 것이라고 확인했습니다.
- 단일 B200 온디맨드 옵션이 사라졌습니다: 사용자들은 DeepInfra가 단일 B200 온디맨드 렌탈 옵션을 제거하고 이제 8x B200 설정만 제공하면서, 해당 옵션을 이용할 수 없게 된 것에 대해 논의했습니다.

한 사용자는 단일 B200 온디맨드 옵션을 위해 Modal을 살펴보라고 제안했으며, 다른 사용자는 Thundercompute에서의 가용성에 대해 문의했습니다.
- 양자화 품질에 대한 의문 제기: 한 사용자가 양자화 후에도 모델이 동일한 지능을 유지하는지, 특히 Q4_K_M 품질과 관련하여 질문했습니다.

한 사용자는 풀 프리시전(full precision)에 비해 품질이 저하되지만, Q4_K_M은 모델과 사용 사례에 따라 다르지만 상당히 좋다고 답변했습니다.
- 새로운 Unsloth 모더레이터가 임명되었습니다: 새로운 모더레이터가 발표되었고, 멤버들은 새로운 역할과 채널 권한에 대해 반응했습니다.

한 사용자는 "비밀 채널이 있을 줄 알았는데, 실망이 컸습니다."라고 말했습니다.
- 사용자가 문맥 인식(Context-Awareness)에 대한 조언을 요청했습니다: 한 사용자가 AI 통계 앱을 구축 중이며 문맥 인식 모델에 대한 조언을 구했습니다.

사용자는 앱에 로컬 데이터베이스 접근 권한을 제공하고 모델이 데이터베이스와 관련된 질문에 답변하기를 원했습니다. 그들은 RAG (Retrieval-Augmented Generation)를 연구하고 ibm-granite/granite-4.0-h-tiny와 같은 모델을 사용할 것을 조언받았습니다.

---

### Unsloth AI (Daniel Han) ▷ #introduce-yourself (12 messages🔥):
> Support channel permissions, Auto moderator issues, Introduction of AI-ML researcher
- 지원 채널의 게이트키핑(Gatekeeping) 탐색: 한 사용자가 지원 채널에서 게시 권한을 얻는 것에 대해 문의했으나, 명백한 접근 권한에도 불구하고 시도가 차단되는 것을 발견했습니다.

자동 모더레이터가 사용자의 게시를 막고 있음이 빠르게 확인되었고, 다른 멤버의 개입으로 이어졌습니다.
- 지원 채널에 대한 자동 모더레이터의 강력한 통제: 멤버들은 지원 채널의 게시 제한 원인이 자동 모더레이터임을 발견했습니다.

한 멤버가 자동 모더레이터를 범인으로 지목한 후 사용자의 질문을 전달하기 위해 나섰습니다.
- AI-ML 연구원 채팅 참여: Generative AI 모델, 클라우드 통합 및 확장 가능한 플랫폼을 활용하여 비즈니스 운영을 최적화하는 한 독립 AI-ML 연구원 겸 개발자가 자신을 소개했습니다.

이 사람은 채널의 다른 멤버들에게 환영받았습니다.

---

### Unsloth AI (Daniel Han) ▷ #off-topic (800 messages🔥🔥🔥):
> AI Music Detection, AI teaching Music, Suno and Udio music AI, Discord Security Incident, GPTS Agent
- AI 음악 탐정 필요: 한 멤버가 AI가 AI 음악과 인간이 만든 음악을 구별할 수 있는 방식과 유사하게, AI를 사용하여 명확한 J-pop과 애니메이션 노래를 구별하는 것이 가능한지 문의했습니다.
- Discord 보안 사고 발생: Discord는 9월 20일 보안 사고가 발생했음을 사용자들에게 알렸으며, 이 사고로 인해 승인되지 않은 당사자가 제3자 고객 서비스 시스템에 제한적으로 접근하여 개인 데이터 및 정부 ID 이미지가 노출되었을 가능성이 있습니다.

이 침해는 Trust & Safety 팀에 대한 소셜 엔지니어링과 관련이 있을 수 있으며, 이는 디지털 ID 및 데이터 저장 관행에 대한 논의를 촉발했습니다.
- 데이터 품질 딜레마: 머신러닝을 위한 고품질 데이터를 정의하는 복잡성에 대한 논의가 있었으며, 한 멤버는 졸업 규범이 흔들리는 것이 노이즈가 많은 데이터셋을 나타낸다고 언급했습니다.

오디오 처리에서 노이즈가 많은 데이터를 식별하고 처리하는 기술이 탐색되었으며, 신뢰할 수 있는 벤치마크를 설정하는 어려움이 언급되었습니다.
- Meta와 Google, ChatGPT의 앱 통합에 반응: 멤버들은 OpenAI의 ChatGPT 앱 통합에 대해 논의하며 Meta와 Google의 반응을 예상했습니다.

일부는 이러한 움직임이 다른 사람들이 단순히 GPT 래퍼를 만드는 것을 막을 수 있다고 제안했으며, 반면 다른 이들은 Apple과 Google의 잠재적인 OS 수준 통합에 대해 추측했습니다.
- Unsloth에 새 모더레이터 합류: 커뮤니티는 새 모더레이터의 합류를 축하하며, 그들의 새로운 역할에 대해 축하를 전했습니다.

---

### Unsloth AI (Daniel Han) ▷ #help (221 messages🔥🔥):
> llama.cpp Compilation, GGUF Output, vLLM with Gemma, Axolotl Fix, FA3
- Unsloth의 아미고, llama.cpp 컴파일: 실제로는 AI에 입문한 C++ 엔지니어인 한 사용자가 안내에 따라 새로운 cmake 지침을 사용하여 llama.cpp를 성공적으로 컴파일했으며, 거의 다 왔다고 말했습니다.

다른 멤버는 추가 컴파일 문제를 피하기 위해 `maximum_memory_usage`와 관련한 문서를 확인할 것을 권장했습니다.
- FA3 효율성 및 다른 트레이닝 프레임워크: 한 팀 멤버는 다른 트레이닝 프레임워크를 사용할 수 있지만, 사용자는 FA3를 비활성화해야 하며, `Unsloth`가 가장 효율적인 옵션으로 남아있다고 언급했습니다.

또 다른 팀 멤버는 "네, 어느 정도 그렇습니다. 다른 트레이닝 프레임워크의 경우 FA3를 비활성화해야 하지만, 그럼에도 불구하고 저희는 여전히 가장 효율적입니다."라고 말했습니다.
- Orpheus TTS 파인튜닝 디버깅 마라톤: 한 사용자가 `Orpheus 4B TTS` 모델 파인튜닝을 위해 로컬 데이터셋 로딩에 문제가 있었고, '`torchcodec.decoders.AudioDecoder` object is not subscriptable' 오류가 발생하여 데이터셋 로딩 문제 해결을 위해 커뮤니티의 도움을 요청했습니다.

광범위한 디버깅 끝에, 사용자는 `datasets`를 버전 3.6.0으로 다운그레이드하는 것이 문제를 해결하여 성공적인 데이터셋 로딩 및 트레이닝을 가능하게 했다는 것을 정확히 찾아냈습니다.
- 비전 모델 이미지 리사이징 문제: 한 사용자는 `UnslothVisionDataCollator`가 비전 모델에서 예기치 않게 토큰을 잘라내고 있었다는 것을 발견했습니다. 이는 콜레이터가 이미지를 512로 리사이징했기 때문이며, 수동 처리 단계에서는 그렇게 하지 않았습니다.

한 멤버는 512로 리사이징하는 것은 모델의 `config`에 기본 이미지 크기가 없기 때문이라고 명확히 했고, 이 문제를 피하기 위해 원하는 크기를 전달하도록 조언했습니다.
- `WSL` 및 `Docker`로 해결된 GPU 문제: `Ubuntu 5090` 사용자가 트레이닝 중 `steps/s` 성능 저하를 경험했으며, 한 팀 멤버는 `Unsloth`를 위해 `WSL` 또는 `Docker` 사용을 제안했습니다.

팀 멤버는 `Docker` 설정 지침 링크를 공유했으며, 추가 지원을 위해 사용자에게 상세 로그를 제공하도록 권장했습니다.

---

### Unsloth AI (Daniel Han) ▷ #showcase (12 messages🔥):
> Advanced AI Safety Notebook, SFT + GRPO, Structured outputs, Qwen Moe, TNG dataset
- AI Safety Notebook 토크 공유!: 한 멤버가 DM 채팅에 이어 고급 `AI safety notebook`에 대한 `GitHub` 토론을 공유하며, 이것이 `SFT` + `GRPO`를 탐색하거나 구조화된 출력을 작업하는 다른 사람들에게 유용하고 실행 가능한 예시가 되기를 바란다고 말했습니다.
- 6개 모델 Qwen Moe 공개!: 한 멤버는 커스텀 빌드된 6B `Qwen 3` 아키텍처(55 레이어, 607 텐서)를 사용하여 각각 2개의 데이터셋에 대해 6개의 개별 모델 트레이닝을 선보인 후, `Unsloth`를 사용하여 구축된 6X6B `Qwen Moe`, 36B 모델(27B 압축)로 "moe"했습니다.

Unsloth(현재 30개 이상의 모델이 파인튜닝되었으며 계속 늘어나고 있습니다), quants를 담당한 Team Mradermacher, 그리고 MLX와 이 모델 프로젝트의 여러 부분에서 협력해 주신 Team Nightmedia에 특별한 감사를 드립니다.
- 데이터셋 세부 정보 공개!: 광범위한 테스트 후 최대 품질/모델 튜닝 결과를 보장하기 위해 데이터셋은 자체적으로 제작되었으며, 모든 훈련은 로컬 머신에서 Unsloth를 사용하여 진행되었습니다. 훈련용 "BASE" 6B 모델(JanV1, Qwen 3, 256k context, W adapter)은 DavidAU/Qwen3-MOE-6x6B-Star-Trek-Universe-Alpha-D-256k-ctx-36B에서 이용 가능합니다.
- TNG 데이터셋 튜닝 성공!: TNG 데이터셋 사용은 모델의 코딩 능력을 향상시켰으며, 모델은 튜닝 테스트뿐만 아니라 비표적 개선 사항을 테스트하기 위해 가능한 경우 여러 metrics에 대해 벤치마크되고 테스트되었습니다.

---

### Unsloth AI (Daniel Han) ▷ #research (16 messages🔥):
> Qwen MoE, Unsloth Dynamic 4bit, Sequence Packing, Flash Attention, torch.compile
- MoE-Money MoE Problems?: 한 멤버가 Unsloth dynamic 4bit를 사용하여 파인튜닝할 16B parameter MoE 모델을 제안하면서, token choice로 훈련할 때 높은 sparsity의 MoE가 흔하다는 점을 언급했습니다.

이 논의는 Qwen 모델 아키텍처에만 해당된다는 것을 암시하는 듯했습니다.
- Sequence Packing + Flash Attention: 파인튜닝 혁신!: 한 멤버가 sequence packing과 Flash Attention을 사용하여 epoch 훈련 시간을 약 30분에서 약 4-5분(약 5억 tokens)으로 단축했으며, 그 결과 CPU 중단 없이 100% GPU 활용률을 달성했습니다.

Flash Attention은 수동 컴파일을 필요로 했으며, 워크스테이션의 메모리 사용량을 최고치로 끌어올렸습니다.
- Torch Compile 문제로 인한 당혹감: 한 멤버가 torch.compile 관련 문제를 보고했으며, Python 3.12와 3.10 모두에서 문제를 겪다가 결국 비활성화했습니다.

inductor에 대한 덜 공격적인 대안으로 aot_eager_decomp_partition을 시도해 볼 것을 제안했습니다.
- ml-cross-entropy, 기대에 못 미치고 훨씬 느림: 한 멤버는 ml-cross-entropy가 GPU 메모리 대역폭 사용량을 줄이는 것을 목표로 했음에도 불구하고, 자체 PyTorch 기반 linear cross-entropy 구현보다 3-4배 느리다는 것을 발견했습니다.

그들은 몇 MB를 절약하는 것이 일부 계산 오버헤드를 상쇄할 수 있기를 바랐습니다.
- Apex의 부상: Fused Optimizers의 승리: 한 멤버가 fused-adam과 FusedRMSNorm을 사용하는 apex에서 성공을 보고했으며, 이들이 성능에 상당한 향상을 제공했다고 언급했습니다.

그들은 또한 linear cross-entropy 구현을 공유했습니다.

---

### LM Studio ▷ #announcements (1 messages):
> LM Studio 0.3.29, OpenAI /v1/responses 호환성, LM Studio CLI
- LM Studio, v1 Responses를 통한 OpenAI 호환성 지원: LM Studio 0.3.29는 OpenAI /v1/responses 호환성 API를 도입했습니다.

이를 통해 개발자들은 표준 OpenAI API 응답 형식을 기대하는 애플리케이션에 LM Studio를 통합할 수 있습니다.
- LM Studio CLI에 Variant Listing 기능 추가: `lms ls --variants` 명령어를 사용하는 새로운 CLI 기능이 LM Studio에 추가되었습니다.

이는 사용자들이 터미널에서 로컬 모델의 variant들을 쉽게 나열할 수 있도록 합니다.

---

### LM Studio ▷ #general (709 messages🔥🔥🔥):
> AGI, Qwen3, LM Studio 과부하 보호, Sentience 시뮬레이션, Mark Zuckerberg 위치
- LM Studio의 과부하 보호 기능: LM Studio는 시스템 과부하에 대한 내장 보호 기능을 갖추고 있어, 컴퓨터가 AI 모델을 처리할 수 없는 경우에도 아무런 문제가 발생하지 않습니다.

보호 기능을 비활성화하더라도 시스템 충돌만 발생할 뿐 브릭(bricking) 현상은 일어나지 않으며, 한 사용자는 농담으로 컴퓨터가 불타기를 바랐습니다.
- 여러 LLM 레이어를 사용한 Sentience 시뮬레이션 논의: 한 사용자가 input 모델, looping 모델, emotional response 모델, inner monologue 모델, reasoning 모델, 그리고 여러 vector database를 포함한 다중 LLM 레이어를 사용하여 sentience를 시뮬레이션하는 것에 대해 문의했습니다.

다른 사용자는 이미 이를 시뮬레이션하고 있으며, 한 가지 작업을 위해 네 가지 다른 모델이 필요하지 않다고 답변했습니다.
- LM Studio가 사용자 지정 root certificate로 Hugging Face에서 모델을 가져올 수 없음: 한 사용자가 LM Studio가 사용자 지정 root certificate로 Hugging Face에서 모델을 가져올 수 없는 이유에 대해 질문했습니다.

다른 사용자는 앱 설정에서 프록시를 시도해 보라고 제안했습니다.
- AI 버블 붕괴를 예상하는 회원들: 한 회원은 AI가 닷컴 버블을 아무것도 아닌 것처럼 보이게 할 버블 붕괴를 겪을 것이라고 말하며, Nvidia, Oracle, OpenAI의 자금 순환에 관한 비디오를 언급했습니다.

다른 사용자는 요약하자면 Nvidia가 OpenAI에 돈을 주고, OpenAI는 Oracle로부터 compute를 구매하며, Oracle은 Nvidia로부터 구매하는 구조라고 답하며, 이들 중 하나라도 실패하면 눈덩이처럼 불어날 수 있다고 덧붙였습니다.
- 지식 주입을 위한 Memento MCP: 한 사용자가 Neo4j에 연결되는 Memento MCP에 대해 질문하며, 더 발전된 것처럼 보이지만 상당한 오버헤드가 있다고 언급했습니다.

다른 멤버는 자신의 로컬 사용에는 toolcalls를 사용하는 것이 가치가 없다고 말했지만, 예를 들어 toolcalls 없이 시스템 프롬프트에 지식 관련 내용을 주입하는 플러그인이 있다면 좋을 것이라고 덧붙였습니다.

---

### LM Studio ▷ #hardware-discussion (328 messages🔥🔥):
> GPT-OSS-120B, Ryzen AI 최대 성능, 분산 추론 설정, AMD vs NVIDIA, AMD용 Vulkan 대 CUDA
- 128k Context를 갖춘 GPT-OSS-120B 프레임워크 출시: 한 멤버는 GPT-OSS-120B 프레임워크가 출시되었으며, 128k context window를 자랑하고 Ryzen AI max+ 하드웨어에서 19.76 tokens/s로 실행된다고 보고했습니다.

다른 멤버들은 정확한 하드웨어 사양에 대해 후속 질문을 했습니다.
- 분산 추론 설정의 성공: 한 멤버는 3개 노드에 걸쳐 WiFi를 통해 8개의 3090을 사용한 성공적인 분산 추론 설정을 공유했으며, GLM 4.5 air 모델에 대해 풀 8-bit precision으로 약 5.5k prompt 처리와 23 tokens/s 출력을 달성했습니다.

그들은 새 부품이 도착하면 4/4 구성으로 2개 노드를 사용하여 속도를 두 배로 늘릴 계획입니다.
- AMD GPU는 여전히 개선이 필요합니다: 멤버들은 AMD GPU와 NVIDIA의 가격 및 성능에 대해 논쟁했으며, AMD가 소프트웨어 지원에서 뒤처져 있는지에 대한 논의가 있었습니다. 한 멤버는 AMD가 게임 외 다른 용도로 GPU 가속기를 사용할 수 있는 괜찮은 소프트웨어를 만들고, 그것이 제대로 작동하도록 하는 것부터 시작해야 한다고 말했습니다.

MI250을 사용해 볼 수 있는 이 링크가 참조되었습니다.
- 노트북 AI의 한계에 대한 아쉬움: 멤버들은 AI 작업에 노트북을 사용하는 것의 한계를 논의하며, 더 약한 하드웨어(낮은 대역폭, 적은 VRAM, 부족한 쿨링)를 언급했습니다.

일반적인 조언은 가능한 한 많은 VRAM을 확보하는 것이며, 여전히 최고의 선택은 Macbook Pro라고 합니다.
- 오래된 GPU에서 Vulkan이 CUDA보다 우세?: 한 멤버는 LM Studio에서 qwen/qwen3-30b-a3b-2507 (Q4_K_M) 모델을 사용하여 NVIDIA P40에서 Vulkan이 CUDA보다 훨씬 뛰어난 성능(63.02 tokens/s 대 23.31 tokens/s)을 보인다는 것을 발견했습니다.

멤버들은 P40의 오래된 드라이버와 오래된 카드에 대한 Vulkan의 더 나은 지원이 그 이유일 수 있다고 추측했습니다.

---

### OpenRouter ▷ #app-showcase (4 messages):
> iFlow 리버스 엔지니어링, GLM-4.6 요청, Docker 없이 Qwen/Gemini
- iFlow 리버스 엔지니어링으로 무료 GLM-4.6 공개: 한 멤버가 iFlow를 리버스 엔지니어링하여 모든 OpenAI 호환 도구에서 무료 GLM-4.6 요청을 가능하게 했습니다.

멤버가 폴더에서 python 파일을 실행하면 작동할 것이라고 확인했습니다.
- Qwen/Gemini, Docker 없이 사용 가능?: 한 멤버가 리버스 엔지니어링된 iFlow를 Qwen/Gemini와 함께 Docker 없이 사용하는 것에 대해 문의했습니다.

멤버가 폴더에서 python 파일을 실행하면 작동할 것이라고 확인했습니다.

---

### OpenRouter ▷ #general (392 messages🔥🔥):
> Multimodal Model Recommendations, Deepseek 3.1 availability, BYOK setup and use, Free models alternatives to Grok, Sora2 pricing
- Gemini Flash와 Llama 4 Maverick, 구조화된 출력에서 빛을 발합니다: 사용자들은 구조화된 출력을 제공하는 무료 Multimodal Model을 찾았고, Llama 4 Maverick과 Gemini Flash가 실행 가능한 옵션으로 제안되었습니다.

Kyle은 OpenAI 라이브러리를 사용하여 Gemini API에서 base64 데이터를 얻기 위한 python 코드 스니펫을 공유했습니다.
- Deepseek 3.1 서비스 중단, 사용자들은 대안을 찾습니다: 사용자들은 deepseek-v3.1-base에 대해 404 오류를 보고했고, 공급자가 호스팅을 중단했음을 통보받았습니다.

대안으로, 멤버들은 deepseek v3.1 deepinfra와 GLM 4.6을 제안했습니다.
- OpenRouter의 BYOK 이해 명확화: 사용자들은 BYOK (Bring Your Own Key)가 어떻게 작동하는지 혼란스러워했고, 다른 사용자들은 OpenRouter가 OpenAI와 같은 API의 프록시 역할을 하며 사용자는 OpenAI로부터 직접 요금을 청구받는다고 설명했습니다.

OpenRouter는 BYOK 사용 시 일반적인 5% 추가 요금을 면제하며, 편의성과 지출 제어 및 대체 옵션과 같은 기능을 제공합니다.
- Grok 4 Fast 서비스 중단, 사용자들은 대안을 찾습니다: Grok 4 Fast가 더 이상 무료가 아니게 되면서 사용자들은 대안을 찾고 있었고, stackedsilence는 Deepseek v3.1을 제안했으며, 다른 사용자들은 새로운 Deepseek 3.2 또는 GLM 4.6과 같은 비용 효율적인 선택지를 지적했습니다.

나중에 논의에서, GLM 4.5 Air (무료)가 Rate Limit 오류가 발생하지 않는 최고의 무료 모델로 제안되었으며, 특히 Z.ai를 공급자로 사용할 때 더욱 그렇습니다.
- Sora 2의 가격 정책 공개!: Sora 2의 가격이 공개되었는데, Pro 버전은 비디오 1초당 $0.3, Non-Pro 버전은 1초당 $0.1이며, 이는 딥페이크를 쉽게 생성할 수 있는 것의 함의에 대한 논의를 촉발했습니다.

워터마크와 암호화를 우회하는 방법들이 제안되었으며, 이는 오용으로 이어질 수 있습니다.

---

### OpenRouter ▷ #new-models (1 messages):
Readybot.io: OpenRouter - New Models

### OpenRouter ▷ #discussion (50 messages🔥):
> ByteDance Seed LLM models, Inference providers pivoting, GPT 5 pricing, Meta's frontier lab status, Grok fast training data
- **ByteDance의 Seed 모델에 대한 관심 증폭:** OpenRouter가 ByteDance의 Seed LLM 모델(예: Seed 1.6)을 포함할지 멤버들이 궁금해하며, 이 모델들이 잠재적으로 프론티어 수준의 성능과 저렴한 가격($0.11 / $0.28 mtok)을 가지고 있다고 언급합니다.

주요 호스트가 중국 플랫폼인 volcengine.com이라는 점에 대한 우려가 제기되었지만, 모델의 잠재력은 여전히 가치 있는 것으로 평가됩니다.
- **위태로운 Inference 제공업체들의 전환:** 한 멤버는 주로 Inference 제공에서 벗어난 Inference 제공업체들을 다른 사람들이 추적하고 있는지 궁금해하며, Kluster와 Inference.net을 예시로 들었습니다.

다른 누군가는 이 제공업체들이 "나에게는 끝났다(dead to me)"고 농담했습니다.
- **GPT-5 가격 루머가 빠르게 확산:** 잠재적인 GPT-5 가격에 대한 추측이 제기되었고, 한 사람은 Sonnet 수준의 속도를 제공하지만 추론(reasoning) 기능은 없다고 시사하는 X 게시물을 링크했습니다.

새로운 GPT-5 Checkpoint의 가능성과 그것이 정말 Gemini 3 Flash인지에 대한 논의가 있었습니다.
- **Meta, 프론티어 랩으로서의 역량 과시:** 멤버들은 Meta가 Pro/deepthink 모델처럼 병렬 테스트 시간 컴퓨팅(parallel test time compute)을 수행함으로써 프론티어 랩(frontier lab)으로 자리매김하고 있는지 논의했습니다.

일부는 회의적인 반응을 보였으며, 한 명은 Google이 일반적으로 상당한 무료 컴퓨팅(compute)을 제공하기 때문에 Flash와 같은 스텔스 모델에 대해 비용을 청구할 가능성이 낮다고 언급했습니다.
- **Sora 2 API가 OpenRouter에 출시될 예정?:** 커뮤니티는 프레젠테이션에서 일부 이미지가 게시된 후 Sora 2 API가 OpenRouter에 잠재적으로 출시될 것으로 보이는 것을 발견했습니다.

한 멤버는 "OpenRouter에 Sora 2가 나노 바나나처럼? 아니면 아님?"이라고 농담했습니다.

---

### Cursor Community ▷ #general (441 messages🔥🔥🔥):
> GPT-5 vs Claude, Cursor billing changes, Cheetah model, GPT-5 Pro, Cursor UI changes
- **GPT-5 대 Claude 4.5: 거인들의 대결:** 일부 멤버들은 GPT-5가 프롬프트에 더 많이 의존하고 솔루션을 과도하게 설계(overengineer)한다고 느꼈으며, 반면 Claude는 많은 작업에서 더 좋고 빠르게 수행한다고 느꼈습니다.

일부 멤버들은 Sonnet 4의 성능이 GPT-5보다 떨어진다고 생각하는 반면, 다른 멤버들은 Ultra 모델을 맹신합니다. 따라서 이는 사람마다, 작업마다 다릅니다.
- **Cheetah 모델 출시, 가격 공개:** 멤버들은 Cheetah라는 새로운 유료 스텔스 모델을 발견했으며, 이 모델이 매우 빠르기 때문에 Gemini 3.0일 수 있다고 추측했습니다.

한 멤버는 이를 테스트해보고 문제가 거의 즉시 해결되었다고 말했으며, 다른 멤버는 속도를 칭찬하며 한 명은 5초 이내에 이미지를 생성했다고 언급했습니다.
- GPT-5 Pro 출시, Opus보다 비싸다: 멤버들은 새로 출시된 GPT-5 Pro에 대해 논의하며 Opus와 같은 다른 모델에 비해 높은 비용을 지적했습니다.

일부는 Cursor CLI에서 GPT-5가 Claude보다 우수하다고 평가했지만, 한 사용자는 GPT-5 Codex가 지속적으로 zero diff 편집을 한다고 보고했습니다.
- Cursor UI 변경으로 사용자들 혼란: 멤버들은 Cursor의 최근 UI 변경, 특히 채팅 닫기 버튼의 사라짐에 대해 논의했습니다.

한 멤버는 상단의 Cursor 로고가 에이전트 버튼처럼 보여서 창을 닫을 수 있다는 것을 깨닫지 못했다고 인정했습니다.
- 학생 플랜 논란: .edu 도메인 필수: 멤버들은 .edu 이메일 주소로 인증해야 하는 학생 플랜의 제한 사항에 대해 논의했습니다.

한 멤버는 자신의 학교 도메인을 가진 사용자들이 NLP를 위한 Google 이벤트에 접근할 수 있도록 지원팀에 문의했다고 공유하며, 다른 사람들도 그렇게 할 것을 제안했습니다.

---

### Cursor 커뮤니티 ▷ #background-agents (5개 메시지):
> 백그라운드 에이전트 실패, 다른 node 버전 사용, 기능 보드 앱 개발, Automazeio CCMP 프레임워크, 사용자 지정 VM 스냅샷 미적용
- 백그라운드 에이전트 실패!: 여러 사용자가 백그라운드 에이전트가 실패하고 있다고 보고했으며, 이 문제는 현재 조사 중입니다.

한 사용자는 에이전트가 다른 node 버전을 사용하기 때문에 시작 명령에서 문제가 발생하며, Dockerfile을 사용하지 않고 임의의 구성으로 다른 컨테이너에서 시작되는 것 같다고 언급했습니다.
- 빠른 기능 보드 앱 개발: 한 사용자는 빠른 기능 보드 앱을 개발 중이며 git worktree의 한계를 해결하고 있습니다.

그들은 Cursor를 통한 Sonnet이 구현된 기능으로 요구 사항을 검증하기 위해 3개의 dev 에이전트, 3개의 reviewer 에이전트, 3개의 PM 에이전트를 생성하는 것을 처리할 수 있는지 고려하고 있으며, automazeio/ccpm 프레임워크를 가능한 대안으로 언급했습니다.
- 사용자 지정 VM 스냅샷 실패: 한 사용자는 자신의 백그라운드 에이전트가 사용자 지정 VM 스냅샷을 인식하지 못한다고 보고했습니다.

다른 사용자는 API를 사용하여 에이전트를 실행하는 것이 성공할지 궁금해하고 있습니다.
- API에 스냅샷 ID 누락: 한 사용자는 API를 통해 BA를 실행할 때 스냅샷 ID를 지정하는 것이 불가능해 보인다고 언급했습니다.

그들은 이러한 관찰을 뒷받침하기 위해 Background Agents OpenAPI 문서를 참조했습니다.

### GPU MODE ▷ #general (25 messages🔥):
> Hopper vs Blackwell SM quadrants, Futhark, 2:4 sparse training, VRAM hacking, CUDA skill set
- Blackwell에서 Hopper의 쿼드런트 디자인 유지 여부 논의: 이 블로그에 따르면 Hopper GPU는 SM을 각각 텐서 코어를 가진 4개의 쿼드런트로 나누며, 이를 통해 클록 사이클당 4개의 워프를 실행할 수 있습니다. 한 멤버가 이것이 Blackwell에서도 유지되는지 질문했습니다.

다른 멤버는 Ampere(그리고 아마도 Volta부터) 각 SM에는 자체 Warp Scheduler를 가진 4개의 쿼드런트가 있다고 설명했습니다.
- "Futhark" 논의 시작: 멤버들이 고성능 순수 함수형 데이터 병렬 배열 프로그래밍인 Futhark에 대한 논의를 시작했습니다.

한 멤버는 프로그래밍 언어에 관심 있는 사람들에게 흥미로울 것이라고 언급했습니다.
- 스파스 트레이닝 튜토리얼 속도 향상 필요: 한 멤버가 PyTorch에서 2:4 스파스 트레이닝을 수행하는 방법에 대한 자료를 요청했습니다. 그는 튜토리얼의 코드 예제로는 속도 향상을 보지 못했으며, torch.sparse의 SparseSemiStructuredTensor._FORCE_CUTLASS = True를 사용하여 스파스 텐서 동작을 수동으로 수정해야만 속도 향상을 얻었다고 언급했습니다.
- 1660 Super VRAM 해킹은 권장되지 않음: 한 멤버가 1660 Super의 VRAM을 12GB로 해킹하는 것에 대해 문의했습니다.

다른 멤버는 <#1349152646484987974> 채널을 제안했지만, 서버의 대부분은 하드웨어 모드보다는 소프트웨어 최적화에 중점을 둔다고 언급했습니다.
- FA3 커널 지식의 깊이는 흔치 않은 것으로 보임: 한 멤버가 FA3와 같은 것을 작동시키기 위해 필요한 지식의 깊이에 놀라움을 표하며, CS 대학원생들 사이에서도 이러한 기술 세트가 얼마나 흔한지 궁금해했습니다.

다른 멤버는 그것이 희귀한 기술 세트라고 지적하며, 저자 목록이 상당히 적었지만 많은 재능 있는 성능 엔지니어들이 대기업에서 일하고 있다고 강조했습니다. 그리고 과정(process)을 신뢰하라는 메시지를 전했습니다.

---

### GPU MODE ▷ #triton (7 messages):
> TLX Team at Meta, Triton Conference
- Meta의 TLX 팀이 주목받다: Meta의 TLX 팀 엔지니어링 매니저가 TLX에서 작업해왔다고 언급했습니다.

또한 팀의 엔지니어들은 이 Discord 채널을 자주 확인하지 않는다고 언급되었습니다.
- Triton 컨퍼런스에서 TLX를 선보일 예정: TLX 팀은 이번 10월 Triton 컨퍼런스에서 TLX를 발표할 예정입니다.

팀은 관심 있는 사람들의 TLX에 대한 질문을 환영합니다.

### GPU MODE ▷ #cuda (36 messages🔥):
PTX .aligned의 의미, Tensor Cores 매트릭스 곱셈, Vast AI GPU에서의 NCU, worklist를 사용한 CUDA 그래프, Blackwell의 256비트 로드 지원
- Warp Execution with PTX .aligned: PTX의 .aligned 지시어는 NVIDIA 문서에 따르면 전체 warp가 동시에 해당 명령어를 실행할 것으로 예상된다는 의미입니다.
- Diving into Matrix Multiply with Tensor Cores: 희소성(sparsity)을 가진 커스텀 매트릭스 곱셈에 Tensor Cores를 사용하는 방법에 대한 소개로, 멤버들은 이 게시물과 더 입문적인 다른 게시물, 그리고 이 게시물을 추천합니다.
- NCU Permission Problems on Cloud GPUs: 사용자들은 Vast AI와 같은 임대 하드웨어에서 NCU를 사용하려고 할 때 권한 오류에 직면하고 있으며, 해당 사이트에는 NCU를 사용한 프로파일링에 대한 문서가 쉽게 제공되지 않는 것으로 보입니다.

일반적으로 클라우드 벤더는 하드웨어 소유자가 NCU 및 하드웨어 카운터 사용을 활성화하지 않는 한 충분한 권한을 제공하지 않습니다. 이를 허용하도록 클라우드 벤더를 설득하거나 이미 허용하는 벤더를 식별하기 위한 워킹 그룹이 구성될 수 있습니다.
- CUDA Graphs Looping with Worklists: 한 개발자가 while 조건과 반복마다 교체되는 두 개의 worklist를 사용하여 CUDA 그래프를 관리하는 더 나은 방법을 찾고 있으며, 현재 교체를 위해 추가 device pointer를 사용하고 있습니다.

이들은 CUDA while node를 사용할 때 추가 device pointer 할당을 피하고 계산을 수행하는 kernel에 device pointer를 전달하는 더 효율적인 방법을 찾고 있습니다.
- Blackwell’s Bloated Bandwidth Bonanza: 누군가 Blackwell 아키텍처가 256비트 로드를 지원한다고 공유했습니다.

---

### GPU MODE ▷ #cool-links (3 messages):
404 오류, GPU 성능 엔지니어링, Arm Architecture Developments 2025, AI용 6비트 데이터 타입, Scalable Vector Extension (SVE)
- Harvard’s GPU Perf Blog Link Gets a Facelift: 한 멤버가 404 오류가 발생했다고 보고했고, 다른 멤버가 URL을 Harvard의 GPU 성능 엔지니어링 블로그로 수정했습니다.

수정된 링크는 GPU 성능 엔지니어링에 대한 상세한 블로그 게시물로 연결됩니다.
- Arm rolls out 6-bit data types for AI: 한 멤버가 Arm Architecture Developments 2025에 대한 링크를 공유하며, Open Compute Project의 OCP MXFP6 포맷을 통한 AI용 6비트 데이터 타입 지원을 강조했습니다.

이 업데이트에는 새로운 Scalable Vector Extension (SVE) 및 Scalable Matrix Extension (SME) 명령어가 포함되어 있으며, 메모리 사용량과 대역폭 요구 사항을 줄여 AI 모델의 효율성을 향상시킵니다.

### GPU MODE ▷ #jobs (4 messages):
> ISTA, DASLab, QUTLASS, Quartet 박사후 연구원 채용
- ISTA의 DASLab, 박사후 연구원 채용 공고: 오스트리아 과학기술원(ISTA)의 Deep Algorithms and Systems Lab (DASLab)에서 효율적인 머신러닝 시스템 발전을 위한 박사후 연구원을 모집합니다.

이 직책은 고성능 컴퓨팅 분야의 강력한 배경과 광범위한 GPU 프로그래밍 경험을 갖춘 박사 학위를 요구하며, QUTLASS 및 Quartet을 포함한 오픈소스 프로젝트에 기여하게 됩니다. 지원자는 CV와 짧은 자기소개서를 dan.alistarh@ist.ac.at로 보내야 합니다.
- ISTA 박사후 연구원 혜택 및 세부 정보 공개: ISTA의 박사후 연구원 직책에 대한 추가 정보는 [근무 조건 페이지](https://ist.ac.at/en/work-at-ista/working-conditions/)에서 확인할 수 있습니다.

한 멤버는 이를 매력적이라고 묘사했습니다.

---

### GPU MODE ▷ #beginner (12 messages🔥):
> CUDA 프로그래밍, CMake 버전, GEMM vs cuBLAS, NVIDIA 데이터 센터 장애 분석
- CMake 버전이 너무 오래되어 업데이트 필요!: 한 사용자가 자신의 CMake 버전이 너무 오래되어(3.5 이전) 업데이트가 필요하다는 오류를 겪었습니다.

다른 사용자는 Kitware의 [다운로드 페이지](https://cmake.org/download/)에서 최신 버전(예: 3.31)을 다운로드할 것을 제안했습니다.
- GEMM 프로젝트 조언: 한 멤버가 특정 아키텍처에서 cuBLAS와 경쟁할 GEMM 프로젝트를 만들 것을 제안했습니다.

그 사용자는 "그것만으로도 'CUDA를 작성할 수 있는' 사람들보다 훨씬 앞서게 될 것"이라고 말했습니다.
- CUDA 강의가 초보자에게 도움을 주었습니다: 새로운 CUDA 프로그래머가 자신에게 큰 도움이 된 강의 추천에 대해 다른 사람에게 감사하며 CUDA 학습을 위한 [GitHub 링크](https://github.com/d-k-k/cuda-lectures)를 공유했습니다.

그 사용자는 GitHub 저장소에 없는 15강과 16강의 강의 슬라이드/코드를 찾는 데 도움을 요청했습니다.
- NVIDIA 직원이 CUDA 로깅 문제 해결에 도움을 줄 수 있습니다: NVIDIA에서 Data Center Failure Analysis 업무를 하는 한 멤버가 CUDA 로깅과 관련된 개념적인 측면의 문제를 조사해 주겠다고 제안했습니다.

그 사용자는 자신이 살펴볼 수 있다고 말했습니다.

---

### GPU MODE ▷ #pmpp-book (1 messages):
> PMPP 코드 스타일, PMPP의 향후 에디션
- PMPP, 폭넓은 접근성을 위해 C 스타일 선호: 멤버들은 PMPP가 독자의 접근성을 극대화하기 위해 C 스타일 코드베이스를 유지하고 있다고 언급했습니다.

이러한 접근 방식이 다음 에디션에서는 발전할 수 있다는 추측이 멤버들 사이에서 나왔습니다.
- PMPP 에디션의 진화: 현재의 C 스타일 접근 방식이 향후 PMPP 에디션에서 재검토될 수 있다는 논의가 있었습니다.

이러한 잠재적인 변화는 접근성과 현대적인 코딩 관행의 균형을 맞추기 위한 적응 전략을 시사합니다.

### GPU MODE ▷ #torchao (1 messages):
aguunu: 감사합니다!

---

### GPU MODE ▷ #irl-meetup (1 messages):
josephtracyvoltagepark_53706: PyTorch 컨퍼런스에 갑니다! 만나 뵙고 싶습니다.

---

### GPU MODE ▷ #triton-puzzles (4 messages):
> Triton Interpreter, Numpy 버전 호환성, Triton-Puzzles
- Triton Interpreter의 Numpy 버전 문제: 한 사용자가 Triton Interpreter가 까다롭게 작동하며, numpy 버전 2.0 이하에서만 예상대로 작동한다는 것을 발견했습니다.

그들은 Triton-Puzzles 노트북에 자세히 설명된 대로 설치를 조정하여 이 문제를 해결할 것을 제안했습니다.
- Triton Interpreter에 특정 Numpy 필요: 한 사용자는 인터프리터가 numpy를 사용하므로, 인터프리터가 작동하려면 특정 numpy 버전에 대한 의존성이 있는 것으로 보인다고 설명했습니다. 이는 Triton Interpreter가 올바르지 않고 이상하게 작동하는 이유를 설명합니다.

버전 2.0이 필요한 것으로 보입니다.

---

### GPU MODE ▷ #rocm (12 messages🔥):
> MI300, rocm-compute-viewer, AMD GPU용 warp specialization, Triton의 wavefront partitioning, rocBLAS
- MI355에서 rocm-compute-viewer 검증: 한 멤버가 rocm-compute-viewer가 MI355에서 작동하지만, 아직 stochastic sampling과 통합되지 않았음을 확인했습니다.

MI300 제품군에서도 작동하는 것이 확인되었습니다.
- AMD GPU Kernels 및 Warp Specialization: 문서 부족: 한 멤버가 AMD GPU 커널용 warp specialization에 대한 공개 자료에 대해 문의했습니다.

다른 멤버는 이 블로그 게시물만큼 포괄적인 자료는 없지만, Triton 이슈 #8281에서 wavefront partitioning에 대한 지속적인 노력이 있다고 언급했습니다.
- rocBLAS는 wavefront specialization을 사용하지 않음: 한 멤버는 rocBLAS가 HIP-level에서 wavefront specialization을 사용하기 시작할 것으로 예상했습니다.

warpgroup instructions의 부재로 인해 warp specialization이 AMD에서 일반적으로 잘 작동하지 않는다는 점이 언급되었습니다.

---

### GPU MODE ▷ #lecture-qa (1 messages):
seb3523: 물론이죠 🙂

---

### GPU MODE ▷ #self-promotion (3 messages):
> Cute-Bench Package, Prefix Sum 및 Kogge-Stone Algorithm, Tiny MoE Optimization
- Cute-Bench Package, 커널 벤치마킹: 한 멤버가 `pip install -U git+https://github.com/NTT123/cute-bench.git` 명령어를 통해 커널의 쉬운 설치 및 벤치마킹을 용이하게 하는 Python 패키지인 cute-bench를 소개했습니다.

해당 패키지는 torch profiler 및 CUDA events를 사용한 벤치마킹 기능을 포함하며, 코드 예시를 통해 시연되고 커널 측정값을 반환합니다.
- Prefix Sum Algorithm 시각화: 한 멤버가 prefix sum 및 Kogge-Stone 알고리즘을 설명하는 YouTube 영상을 공유했습니다. 이 영상에는 완전한 코드 구현이 포함되어 있습니다.

영상 제작자는 영상 제작이 처음이므로 모든 비판을 환영합니다.
- Tiny MoE 최적화 프레젠테이션: 한 멤버가 0.5B 파라미터의 작은 MoE(Qwen 스타일)를 최적화했으며, 해당 내용은 이 Maven 링크에서 무료로 프레젠테이션될 예정입니다.

그들은 프로파일러, 하나의 fused GEMM 및 기타 트릭을 사용하여 60시간 이상(DDP 사용 시 23시간) 걸리던 훈련 시간을 13.4시간으로 단축했으며, CUTLASS 사용 직전까지 최적화했습니다.

---

### GPU MODE ▷ #🍿 (3개 메시지):
> LLM 생성 커널, Sakana CUDA 엔지니어
- Alexander, Popcorn Manifesto에 합류: Alexander (github.com/zanderjiang)는 LLM 생성 커널에 관심을 표명했으며 Popcorn Manifesto 프로젝트에 기여하고 싶어 합니다.

그는 신뢰할 수 있는 코드 생성, 견고한 커널 벤치마킹 및 평가, 그리고 커널 생성을 위한 에이전트/프레임워크/도구와 관련된 프로젝트를 진행해 왔습니다.
- Sakana CUDA 엔지니어 문서 찾기 시작: 한 멤버가 인터넷에서 철회된 Sakana AI CUDA 엔지니어 문서의 원본 PDF 또는 Wayback Machine 캡처본을 찾고 있습니다.

다른 멤버는 사람들이 대신 Twitter 게시물을 인용하고 있다고 언급했습니다.

---

### GPU MODE ▷ #thunderkittens (3개 메시지):
> B200 멀티 GPU cudaMemcpy, TMA 대 load/store 성능, NVLink 대역폭
- B200 cudaMemcpy 속도 향상 보고: 한 멤버가 B200 멀티 GPU cudaMemcpy에서 726GB/s를 달성했다고 보고했으며, 이는 다른 사람의 670GB/s를 약간 넘는 결과보다 높은 수치입니다.

원본 게시자는 불일치에 대한 궁금증을 표하고 더 높은 대역폭을 달성하는 데 사용된 코드에 대해 문의했으며, NCCL 테스트로는 결과를 재현하기 어렵다고 언급했습니다.
- TMA 성능 실망: 한 사용자는 TMA가 load/store 작업보다 크게 빠르지 않고 memcpy보다 약간 느릴 뿐이라는 것을 발견했으며, 32바이트 PTX 명령어(12.9에서 도입) 사용이 요인일 수 있다고 추측합니다.

그들은 NCCL 코드가 동일한 스레드에서 연속적인 load를 사용한다고 언급했으며, 이는 아직 제대로 비교해 보지 못한 기술입니다.
- NVLink 대역폭에 대한 공개 실험 공유: 한 사용자가 NVLink 대역폭에 대한 자신의 공개 실험 링크와 copy engine을 사용한 결과를 공유했습니다.

그들은 또한 대역폭 성능이 테스트에 사용된 특정 머신 구성에 따라 달라진다고 덧붙였습니다.

---

### GPU MODE ▷ #submissions (72 messages🔥🔥):
> amd-all2all Leaderboard, MI300x8 Performance, amd-ag-gemm Leaderboard, amd-gemm-rs Leaderboard, gau-nernst and Kernel GM
- AMD All2All 리더보드: amd-all2all 리더보드에 여러 번 제출되었으며, 그중 몇몇은 MI300x8에서 445 µs 및 439 µs의 기록을 포함하여 1위를 차지했습니다.

다른 주목할 만한 기록으로는 90.3 ms의 개인 최고 기록과 1100-1200 µs 내외의 여러 성공적인 제출이 있습니다.
- AG-GEMM AMD 리더보드 장악: amd-ag-gemm 리더보드에 많은 제출이 이루어졌으며, MI300x8에서 약 500 µs에서 800 µs에 이르는 기록을 보였습니다.

한 제출은 499 µs로 4위를 차지했으며, 다른 하나는 59.1 ms의 기록을 보였습니다.
- GEMM-RS AMD 리더보드: MI300x8의 amd-gemm-rs 리더보드에 여러 번 제출되었으며, 570-590 µs 내외의 기록을 보였습니다.

한 제출은 1289 µs의 기록을 달성했습니다.
- GAU-Nernst가 Kernel GM을 위협하다: 한 멤버는 Kernel GM 역할을 가진 gau-nernst가 무서운 존재라고 언급했습니다.

추가 설명은 제공되지 않았습니다.

---

### GPU MODE ▷ #hardware (38 messages🔥):
> homelabs builds, b200s, A100 32G SXM2, cursed builds, GPU direct storage
- 미친 홈랩 빌드에 대한 갈망: 한 멤버는 채널이 미친 homelab builds로 가득 차 있지 않다는 것에 실망감을 표현했으며 <:wahh:1404542087860584570>, 다른 멤버는 장비를 구축하고 라이브스트림을 고려하고 있다고 언급했습니다.

다른 멤버는 대신 저렴한 hw를 기대하고 있었다고 끼어들며 말했습니다.
- A100 32G SXM2 노드 구성 논의: 멤버들은 4x A100 32G SXM2 노드 설정에 대해 논의했으며, 한 멤버는 이것이 "cursed builds"로 알려진 사람에게 매력적일 수 있다고 제안했습니다.

논의는 이 카드들에는 전력 제한이 없다는 사실을 언급했습니다.
- PCIe 제한 및 확장 옵션 탐색: 대화는 8개의 GPU로 업그레이드를 시도할 때 발생하는 PCIe 레인 제한으로 옮겨갔으며, 듀얼 소켓 서버 마더보드 또는 PCIe switch magic과 같은 제안이 있었습니다.

한 멤버는 시스템에 64개의 GPU를 장착하기 위한 PCIe switch magic을 농담 삼아 제안했습니다.
- GPU Direct Storage: 한 멤버는 GPU가 P2P를 통해 디스크 스토리지에서 직접 데이터를 읽는 것이 가능한지 물었고, Nvidia의 GPUDirect storage API를 언급하며 비-Nvidia GPU에서도 이러한 기능이 존재하는지 궁금해했습니다.

NIXL의 Backend Guide 링크가 공유되었으며, NIXL이 사용하는 백엔드 중 하나로 GPUDirect Storage (GDS)가 언급되었습니다.

---

### GPU MODE ▷ #tpu (4 messages):
> TPU VM, TPU Pods, TPU Slices, TPU Workers
- TPU 초보자가 가이드를 요청합니다: 한 멤버가 tpu-starter GitHub의 지침을 따랐지만, TPU VM 생성 시 구성 문제에 직면했습니다. 그들은 TPU Pods, TPU Slices, TPU Workers를 학습할 자료를 찾고 있습니다.
- TPU 설정에 대한 추가 정보: 사용자가 자신의 TPU 구성 이미지를 첨부했으며, 이미 Pods, Slices, Workers에 대해 읽었음을 밝혔습니다. 그들은 설정 과정에서 놓친 부분이 있는지 묻고 있습니다.

---

### GPU MODE ▷ #factorio-learning-env (9 messages🔥):
> FLE 0.3.0 on Mac M1/M2, FLE on M4, Factorio Modding, FLE Sync Meeting
- FLE 0.3.0이 M 시리즈 Mac에서 작동합니다: 한 멤버가 FLE 0.3.0이 Mac M1/M2 칩에서 실행되는지, 그리고 잘 작동하는지 문의했습니다. 다른 멤버는 M4에서 성공적으로 실행됨을 확인하고 설치를 시도해 볼 것을 제안했습니다.
- Factorio 모딩 전문가가 합류합니다: Factorio 모딩 환경에 익숙한 한 멤버가 기여하겠다고 제안했습니다. 다른 멤버는 그들을 FLE Sync 미팅에 초대했습니다.
- FLE Sync 미팅 초대: 한 멤버가 FLE Sync 미팅을 언급하며 Google Meet 링크를 공유했습니다. 관련 Twitch 클립 링크도 공유되었습니다.
- 릴리스를 축하합니다!: 한 멤버가 FLE 0.3.0 릴리스를 축하했습니다. 그들은 로켓 이모티콘과 함께 앞으로 더 많은 릴리스가 있기를 희망했습니다.

---

### GPU MODE ▷ #amd-competition (32 messages🔥):
> rocshmem issues, all2all kernel errors, amd_gemm_rs timeouts, libtorch usage, gemm-rs optimization
- Custom Kernel 문제로 인한 혼란: 한 사용자가 baseline all2all kernel을 실행할 때 ImportError: cannot import name 'custom_kernel' from 'submission' 오류를 겪었으며, 구현해야 할 진입점인 custom_kernel 함수가 코드에 없다는 조언을 받았습니다. 다른 멤버는 타임아웃 문제의 원인이 사용자 구현의 버그일 수 있다고 제안했습니다.
- GEMM-RS가 점진적으로 발전합니다: amd_gemm_rs를 벤치마킹한 한 사용자가 타임아웃을 경험했지만, 설정은 정상적이며 한 사용자는 성공적으로 실행됨을 확인했습니다. 또한 gemm-rs의 최적화 가능 공간이 ag-gemm에 비해 더 클 수 있습니다.

좋은 computation 및 communication overlap이 있을 때 개선 사항이 더 두드러질 것이며, 온라인 주장에서는 gemm-rs overlap 개선이 약 1.25배에 달한다고 제안되었습니다.
- LibTorch의 한계가 크게 다가옵니다: 한 사용자가 cpp_extension과 함께 libtorch를 사용하는 방법과 해당 경로가 필요하다고 문의했습니다.

해당 사용자는 /dev/shm/ 공간이 충분하지 않다고 후속 보고했습니다.
- rocSHMEM의 험난한 길: 리소스 제한이 사용자들을 괴롭힙니다: 사용자들은 rocshmem과 관련하여 rdc와 링크해야 하는 필요성을 포함한 문제들을 보고했으며, 버퍼 할당 시 공간 제한에 부딪히고 일부 할당 후 오류가 발생했다고 보고했습니다.

해당 제한이 크기보다는 할당 시간과 관련이 있을 수 있다고 제안되었으며, rocshmemcc에 대한 공간 할당 증가 요청이 있었습니다.

---

### GPU MODE ▷ #cutlass (46 messages🔥):
> Vectorization Issues with cute.copy, Layout Visualizers, cute.copy vectorization, TV Layouts, SVG TV visualizer
- cute.copy 벡터화에는 소스와 대상 정렬이 필요합니다: 한 사용자는 tAsA.store(tAgA.load())는 벡터화되었지만, cute.copy(tiled_copy_g2s_a, tAgA, tAsA)는 그렇지 않다는 것을 발견했습니다. 더 큰 num_bits_per_copy를 강제하면 ICE가 발생했으며, 이는 smem layout을 뒤집어 양쪽을 벡터화함으로써 해결되었습니다. 한 멤버는 cute.copy가 벡터화를 위해 소스와 대상 모두 atom_v structure를 가져야 한다는 것을 확인했습니다.

합의된 의견은 atom creation 시 num_bits_per_copy를 지정하여 강제하면 IR verification에서 실패한다는 것입니다. 이는 제한적인 cp.async를 사용하기 때문에 전혀 벡터화할 수 없습니다.
- Layout Visualizer 대결: 멤버들은 Layout Visualizer에 대해 논의했으며, 한 멤버는 자신의 SVG TV visualizer를 선보였고, 다른 멤버는 Hugging Face Spaces에 호스팅된 자신의 TV layout visualizer 링크를 공유했습니다.

전자는 C++ 구현을 기반으로 하며, cute.jit/kernel 내에서 호출할 수 있고, 이제 swizzle, copy, MMA 레이아웃을 지원합니다. 반면 후자는 온라인으로 호스팅되며 일부 사용자에게는 더 편리하다고 여겨집니다.
- Cute Visualizer 경쟁이 뜨거워집니다: Layout Visualizer에 대한 논의에 이어, 한 멤버가 Python으로 작성된 자신의 cute-layout-display를 공유했습니다.

이는 Layout Visualizer를 만들기 위한 경쟁에 대한 유머러스한 댓글과 함께, 모든 CuteDSL 초보자가 이 기능이 필요한 것 같다는 의견을 불러일으켰습니다. 이 Visualizer는 swizzle, copy, MMA 레이아웃을 지원합니다.

### GPU MODE ▷ #general (1 messages):
- Mojo를 Python 스크립트에서 직접 설치: 이제 Python 스크립트 내에서 pip를 사용하여 Mojo를 직접 설치할 수 있습니다.

코드 스니펫이 공유되었습니다:

```
import pip
pip.main(['install', 'mojo'])📋
```
- pip를 사용하여 Mojo 관리: 이 메시지들은 Python 패키지 인스톨러인 pip를 사용하여 Mojo 설치를 관리할 수 있는 가능성을 강조합니다.

이는 Python 환경에 익숙한 사용자들에게 설치 과정을 간소화할 수 있습니다.

---

### GPU MODE ▷ #multi-gpu (53 messages🔥):
- 텐서 전송 프로파일링: 동기화 또는 과부하?: 텐서 전송을 프로파일링할 때 작은 텐서의 경우 동기화 오버헤드가 과도하게 측정될 수 있으며, (65536, 5120)와 같은 큰 텐서는 MoE 학습 실행에서 지연 효과(straggler effects)와 동기화 장벽을 드러낼 수 있습니다.

정확한 타이밍을 위해서는 Nsight-Systems (nsys)를 사용하거나, GPU 클록 잠금(clock locking) 및 워밍업 실행(warmup runs)을 통해 일관된 결과를 보장하면서 전송 전후에 CUDA 이벤트를 기록하는 것이 제안되었습니다.
- NVSHMEM 대 NCCL: 포인터 논쟁: NVSHMEM 및 NCCL에서 nvshmem_malloc()은 NVSHMEM 대칭 힙(symmetric heap)에서 메모리를 예약하고 각 PE(processing element)에서 동일하게 보이는 대칭 포인터를 반환하는 집합적 연산(collective operation)을 수행합니다.

포인터의 숫자 값은 PE마다 동일하지 않지만, nvshmem_ptr은 특정 접근 패턴 없이 원격 메모리에 효율적으로 접근할 수 있습니다. 그러나 단일 프로세스를 지원하는 NCCL과 달리 GPU당 하나의 프로세스가 필요합니다.
- Mercury: Multi-GPU의 오퍼레이터 옵티마이저: Mercury 논문은 루프 기반의 중간 표현(intermediate representation)인 CommIR을 소개하며, 원격 GPU 메모리를 multi-GPU 오퍼레이터 컴파일레이션을 위한 메모리 계층의 명시적으로 관리되는 확장으로 취급합니다.

이 접근 방식은 데이터 배치 및 장치 간 통신에 대한 전체적인 추론을 가능하게 하며, USP 및 Ulysses와 같은 최첨단 수동 최적화 설계보다 평균 1.56배의 속도 향상을 달성하고, 모델 수준의 3D-병렬(parallel)과 비교하여 실제 LLM 워크로드에서 최대 1.62배의 성능 향상을 이룹니다.
- GPU 클록 잠금: 속도 안정화: 일관된 프로파일링 결과를 보장하기 위해, 멤버들은 nvidia-smi --lock-gpu-clocks를 사용하여 GPU 클록을 안정적인 주파수로 잠글 것을 권장합니다. 이는 nvidia-smi –lock-gpu-clocks=tdp,tdp로 설정된 기본 TDP 주파수일 수 있습니다.

최대 주파수에는 주의해야 합니다. GPU가 과열되면 클럭 속도가 무조건 느려지기 때문입니다. 따라서 TDP를 기준으로 사용하면 전체 로드 시 안정적으로 유지됩니다.

---

### GPU MODE ▷ #irl-accel-hackathon (1 messages):
> Multimodal Data Processing, Hackathon Team Formation
- 해커톤 참가자가 멀티모달 처리 팀을 찾습니다: 한 참가자가 해커톤 기간 동안 멀티모달 데이터 처리 프로젝트에서 협업할 팀원을 찾고 있으며, 관심 있는 사람들에게 연락하여 함께 계획하자고 초대합니다.
- 해커톤 참가자가 팀 및 주제 목록에 대해 문의합니다: 한 참가자가 기여하고 새로운 아이디어를 제안하기 위해 기존 팀 목록과 주제를 검토하는 데 관심을 표명했습니다.

그들은 "한번 살펴보고 무언가를 제안하는 데 매우 적극적"입니다.

---

### GPU MODE ▷ #opencl-vulkan (2 messages):
> Khronos Group, OpenCL, Vulkan
- Khronos Group에 맞춰진 채널에 대한 열정: 한 멤버가 Khronos Group에 맞춰진 채널에 대한 열정을 표명하며, OpenCL 및 Vulkan 논의에 대한 관심을 나타냈습니다.

이는 병렬 컴퓨팅 및 그래픽 API에 대한 산업 표준에 중점을 두고 있음을 시사합니다.
- OpenCL 및 Vulkan에 대한 잠재적 논의: 사용자의 댓글은 OpenCL 및 Vulkan에 대한 심층적인 대화에 대한 열망을 암시합니다.

이들은 GPU 프로그래밍 및 크로스 플랫폼 개발을 위한 핵심 기술입니다.

---

### GPU MODE ▷ #cluster-management (10 messages🔥):
> Node Failures Mitigation, MPI Fault Tolerance, CCL Reconfiguration, AI-Driven Observability
- 노드 장애가 증가하는 GPU 클러스터를 괴롭힙니다: 멤버들은 더 큰 GPU 클러스터에서 노드 장애를 완화하기 위한 전략을 논의하고 있으며, 증가하는 장애 지점의 수와 안정성 및 복구의 중요성을 언급하고, 약 5%의 오버 프로비저닝을 언급한 연구를 인용했습니다.

이 논의는 1,056개의 H100 또는 A100 GPU 클러스터의 경우 오버 프로비저닝에 월 약 100만 달러가 소요될 수 있음을 강조합니다.
- MPI는 하드웨어 장애에서 살아남는 것을 목표로 합니다: 이 github에 따르면, MPI 포럼은 MPI 프로그램이 하드웨어 장애에서 살아남고 런타임에 노드를 다시 추가할 수 있도록 노력하고 있습니다.

전반적으로 많은 수고 없이는 이 문제에 대한 훌륭한 해결책이 없다는 점과 NCCL 및 RCCL이 MPI FTWG에서 참고할 수 있다는 점이 지적되었습니다.
- 체크포인트 및 재시작은 여전히 기본 복구 방법입니다: 멤버들은 오늘날 대부분의 CCL이 여전히 고정된 노드 세트를 가정하므로 체크포인트 및 재시작이 여전히 기본 복구 경로라는 데 동의했습니다.

단기적으로는 FT-MPI 또는 PCCL과 같은 내결함성 스택이 프로덕션 환경에서 동적 노드 변경을 처리할 수 있을 때까지 자동화 및 더 스마트한 체크포인팅에 투자하여 더 빠른 복구를 달성하는 것이 실용적인 접근 방식이라고 언급되었습니다.
- AI 기반의 사전 예방적 관측 가능성 중심 복원력: 해당 그룹은 최근 로그, 메트릭, 텔레메트리를 실시간으로 관찰할 수 있는 클러스터 내에 내장된 에이전트 시스템을 탐색하기 시작했습니다.

이 아이디어는 반응적 복구에서 사전 예방적, AI 기반의 관측 가능성 및 복원력으로 전환하고, 클러스터 상태를 유지하기 위해 자율적으로 로드를 재조정하는 것입니다.

---

### GPU MODE ▷ #penny (3 messages):
> Cloud Providers, Vast AI limitations, nvshmem, ncu/nsys access, nvm
- 프로젝트를 위한 클라우드 옵션 브레인스토밍: 한 멤버가 프로젝트를 지원할 적합한 클라우드 제공업체에 대한 추천을 찾고 있으며, 초기 테스트는 AWS와 nvm이라는 제공업체에서 계획되어 있습니다.

그들은 프로젝트 요구 사항에 필수적인 nvshmem 및 ncu/nsys 액세스 부재로 인해 Vast AI에 대한 불만을 표명했습니다.
- VastAI에서 Nvshmem 및 Nsys/NCU 액세스 누락: 한 사용자가 Vast AI에 nvshmem, ncu, nsys 액세스와 같은 필수 기능이 부족하여 프로젝트 진행을 방해하고 있다고 보고했습니다.

사용자는 이러한 제한 사항을 극복하기 위한 잠재적 대안으로 AWS와 NVM이라는 다른 제공업체를 탐색할 것이라고 언급했습니다.

---

### HuggingFace ▷ #general (312 messages🔥🔥):
> Image-to-text ONNX conversion, Large-scale conversation datasets, Model drift, Uncensored AI roleplay models, Hugging Face Pro subscription
- 이미지-텍스트 변환에 대한 ONNX 커뮤니티의 도움: VLM 및 멀티모달 모델을 ONNX로 변환하는 것은 까다로울 수 있으므로, ONNX 커뮤니티에 도움과 지침을 요청하는 것이 제안되었습니다.

HF Discord의 관련 채널 링크도 변환 팁과 함께 공유되었습니다.
- 고품질 대화 데이터셋 평가: 대규모의 고품질 3인 이상 참여 대화 데이터를 찾는 과정에서 Hugging Face의 기존 프로젝트를 탐색하는 것이 제안되었습니다.

또한 Hugging Science의 범위에 속할 수도 있습니다.
- 데이터 드리프트 해독 및 명확화: 한 멤버가 Model vs Data Drift에 대해 더 자세히 알아볼 수 있는 기사 링크를 공유했습니다.

해당 아티클은 drift가 어떻게 발생하는지, 그리고 LLM을 관련 정보로 지속적으로 업데이트하여 최신 상태를 유지하는 것과 같이 drift를 완화하거나 줄이기 위해 무엇을 할 수 있는지에 대한 교육적인 자료로 읽혔습니다.
- OVH 호스팅의 Vision-to-Text 모델 관련 문제: 한 멤버가 OVH 호스팅 관련 문제를 겪고 있으며, 이미지당 약 200단어의 설명을 위해 한 번에 약 100개의 다른 이미지를 처리하는 vision-to-text 모델을 사용하려고 할 때 조언을 구하고 있습니다.

해당 멤버에게 OVH를 사용하고 Ollama로 GGUF 파일을 배포하는 것을 제안했습니다.
- Deepseek 다운로드 문제 해결: 한 멤버가 Deepseek 모델에 사용할 적절한 파일을 찾는 데 어려움을 겪고 있었으며, deepseek-v3 모델을 위한 충분한 메모리가 없을 가능성이 100%라는 조언을 받았습니다.

로컬 호스팅의 경우, 대부분 GGUF를 실행하며 quantization에서 찾을 수 있다고 설명했습니다.

---

### HuggingFace ▷ #i-made-this (5개 메시지):
> Neovim RAG 챗봇, Gemma 1B IT 기반 PromptVault Android 앱, AI 텍스트 감지용 BERT 모델, 아트 비디오
- Neovim에 도입된 Vim-향상된 Semantic Search: 한 멤버가 Claude-4.5 Sonnet을 사용하여 Neovim 도움말 문서를 위한 RAG 챗봇을 구축했으며, Neovim 문서에 더 나은 semantic search를 제공하는 것을 목표로 합니다.

이 챗봇은 기본적인 수준임에도 불구하고 작성자가 만족하는 사이드 프로젝트입니다.
- Gemma를 통해 오프라인 프롬프트를 지원하는 PromptVault 앱: 한 멤버가 자신의 무료 Android 앱인 PromptVault가 이제 MediaPipe Tasks를 통해 Gemma 1B IT 모델을 사용하여 오프라인 프롬프트 생성 및 개선을 위한 온디바이스 AI를 실행한다고 발표했습니다.

이 앱은 로컬 및 Google Drive (암호화된) 백업, 오프라인 모드, 그리고 제목, 설명, 프롬프트 텍스트 작성을 위한 실험적인 AI 액션 기능을 포함합니다.
- BERT, AI 생성 에세이 단속: 한 멤버가 Kaggle의 데이터셋을 사용하여 AI가 생성한 텍스트와 사람이 작성한 텍스트를 감지하기 위한 BERT 모델을 훈련시켰습니다.

해당 멤버는 현재 데이터셋이 최적은 아니기 때문에 더 큰 데이터셋에서 훈련 속도를 높이기 위한 아이디어를 찾고 있습니다.
- AI 아트, GigaChad 반응을 불러일으키다: 한 멤버가 비디오를 공유하고, 해당 비디오가 예술로 간주된다면 시청자들에게 <:GIGACHAD:1159748479535558718>로 반응해달라고 요청했습니다.

해당 비디오는 download.mov 파일로 첨부되었습니다.

---

### HuggingFace ▷ #computer-vision (1 messages):
> Wan2.2 튜닝, 고노이즈 모델을 위한 LoRA 튜닝, 고노이즈 및 저노이즈 모델 튜닝, 단일 컴포넌트 모델 튜닝
- 사용자들은 Wan2.2 튜닝 팁을 찾습니다: 한 사용자가 고노이즈 모델에 LoRA를 사용하여 Wan2.2를 튜닝하고 있는데, 10개의 데이터셋 반복과 비디오당 81프레임의 비디오 77개로 5 epoch를 진행한 후 이상한 결과를 얻었습니다.

사용자는 해당 비디오들이 프레임 전체를 가로질러 움직이는 단일 항목만 보이는 '쓰레기' 같다고 설명하며 커뮤니티에 도움을 요청합니다.
- 고노이즈 및 저노이즈 모델 모두 튜닝: 동시 또는 개별적으로?: 한 사용자가 고노이즈 및 저노이즈 모델을 모두 튜닝해야 하는지, 그리고 그렇다면 한 번에 (예: 전체 사이클) 튜닝해야 하는지 물었습니다.

그들은 또한 동일한 저노이즈 모델을 사용하여 모델의 단일 컴포넌트(예: 고노이즈 모델만)를 튜닝할 수 있는지 문의했습니다.

---

### HuggingFace ▷ #smol-course (20 messages🔥):
> 리더보드 업데이트, DPOTrainer의 TrackIO 문제, 리더보드의 비공개 데이터셋, TRL + SmolLM3를 사용한 LoRA SFT 문제
- 리더보드 수동 새로고침: 코스 리더보드가 수동으로 업데이트되었습니다. [링크는 여기](https://huggingface.co/spaces/smol-ai/smol-lm-3-leaderboard).

멤버들이 업데이트 빈도에 대해 질문했고, 수동 프로세스임을 명확히 했습니다.
- TrackIO 패키지 문제: 한 멤버가 예시를 따르던 중 DPOTrainer/DPOConfig에서 trackio 관련 문제를 겪었고, [관련 문제에 대한 링크](https://github.com/huggingface/trl/issues/1651)를 공유했습니다.

trackio 버전을 0.4.0으로 고정하니 문제가 해결되었으며, 이는 버전 0.5.1에 문제가 있음을 시사합니다.
- 리더보드에 필요한 공개 데이터셋: 일부 학생들이 스페이스에 제출했지만 평가 데이터셋을 비공개로 남겨두었다는 점이 언급되었습니다.

공개 데이터셋 없이는 리더보드가 작동하지 않을 것이라는 경고가 발행되었으며, PR을 확인하고 [리더보드 토론 링크](https://github.com/huggingface/smol-course/discussions/10)를 참조해달라는 요청이 있었습니다.
- LoRA SFT 문제 해결: 한 멤버가 colab에서 TRL + SmolLM3를 사용하여 LoRA SFT를 사용할 때 예상치 못한 키워드 인수 `dataset_kwargs`와 관련된 TypeError를 겪었다고 보고했습니다.

논의 중 해결책은 발견되지 않았습니다.

---

### HuggingFace ▷ #agents-course (17 messages🔥):
> GAIA 오류, DuckDuckGoSearchTool 오류, Smolagents 비용, 복제(Duplicating) vs 클로닝(Cloning)
- GAIA 스페이스 클로닝으로 인한 500 오류: GAIA 실습 중, 한 사용자가 스페이스를 클론하려 할 때 500 오류를 겪었으며, 대신 스페이스를 복제(duplicate)하라는 조언을 받았습니다.

다른 멤버가 이 접근 방식을 확인해 주며 "클로닝 대신 스페이스를 duplicate해야 한다고 생각합니다. 저도 그렇게 했던 것 같습니다."라고 말했습니다.
- 툴 오류로 DuckDuckGoing 중단: AI Agents 코스를 시작하는 신입 학생이 에이전트의 툴에 DuckDuckGoSearchTool을 추가하는 중 오류를 겪었으며, 이는 첨부된 이미지로 공유되었습니다.
- Smolagents 비용: 무료인가요?: 한 사용자가 smolagents 사용과 관련된 비용에 대해 문의하며, 유료 AI 서비스가 필요한지 궁금해했습니다.

그들은 과거 코스를 계속하기 위해 Hugging Face에 무언가 비용을 지불해야 했던 경험을 떠올렸지만, 구체적인 내용은 기억하지 못했습니다.
- 클로닝보다 스페이스 duplicate 선호: 한 사용자가 로컬에서 스페이스를 클로닝하는 것보다 duplicate하는 것이 왜 더 나은지 질문했습니다.

답변은 코스에서 사용자들에게 클로닝 대신 스페이스를 duplicate하도록 특별히 지시했지만, 그 이유는 명시하지 않았다고 밝혔습니다.

---

### Modular (Mojo 🔥) ▷ #general (206 messages🔥🔥):
> GPU Puzzles Feedback, Mojo vs CUDA, Mojo and pthreads, Tinker as a Threat to Mojo, MAX and Pytorch
- Mojo GPU Puzzles 피드백: 한 멤버가 GPU puzzles 책에 대한 피드백 및 수정 사항을 제출할 적절한 채널에 대해 문의했습니다.

다른 멤버가 Mojo GPU Puzzles 리포지토리의 GitHub 링크를 제공했습니다.
- Mojo의 연산 능력 대 CUDA: 한 멤버가 Mojo가 CUDA에서 사용 가능한 모든 기능을 재현할 수 있는지 문의했습니다.

다른 멤버는 Mojo가 현재 GPU의 그래픽 하드웨어와 상호 작용하는 최적의 방법이 부족하지만, 사실상 모든 연산 작업은 달성 가능해야 한다고 답변했습니다. 충족되지 않은 요구 사항에 대한 기능 요청은 환영한다고 덧붙였습니다.
- Mojo에서 C 라이브러리를 사용한 스레딩: 한 멤버가 Mojo 내에서 pthreads와 같은 C 라이브러리를 활용할 가능성에 대해 문의했습니다.

한 멤버는 C 라이브러리가 일반적으로 잘 통합되지만, Mojo의 런타임 환경과 표준 라이브러리 함수에 미치는 영향 때문에 pthreads는 이상적이지 않을 수 있다고 답변했습니다. Mojo의 현재 동시성 모델은 불완전하다고 덧붙였습니다.
- Mojo 트레이닝에 대한 Tinker의 잠재적 위협: 한 멤버는 Tinker가 Mojo에 경쟁적인 도전을 제기할 수 있으며, 특히 파인튜닝된 가중치(weights)에 대한 접근을 제한하여 모델 커스터마이징 옵션을 제약할 경우 더욱 그렇다고 제안했습니다.

한 멤버가 Modular에게 훈련은 우선순위가 낮다고 응답했습니다. AI에서 모든 수익이 추론(inference)에서 발생하기 때문입니다.
- MAX: 추론 영역의 경쟁자: 한 멤버는 MAX가 TensorFlow와 PyTorch를 잠재적으로 대체할 수 있다고 언급하며, 현재 동등성(parity)과 오픈소스 상태에 대해 문의했습니다.

다른 멤버는 MAX가 추론 성능 면에서 비슷하거나 앞서지만, 부분적으로만 오픈소스이며 내년 초에 추가로 오픈할 계획이라고 응답했습니다.

---

### Modular (Mojo 🔥) ▷ #mojo (80개 메시지🔥🔥):
> Pixi vs UV, Mojo Notebook, Python 타입 힌트, Mojo const generics, Mojo 디버깅
- Pixi가 구원하다: 더 이상 Access Denied 없음!: 한 사용자가 Pixi에서 Access denied 오류를 보고했지만, nightly 대신 max-nightly 채널로 전환하여 문제가 해결되었습니다.

Pixi는 uv와 느리지 않은 conda 구현의 조합과 같습니다.
- Jupyter notebook 내에서 Mojo 실행: Mojo는 이 포럼 게시물에서 시연된 바와 같이 `%%mojo` magic command를 사용하여 Jupyter notebooks에서 사용할 수 있지만, 이제 import 경로는 `mojo.notebook`입니다.

Jupyter 셀에서는 아직 Mojo 구문 강조(syntax highlighting)가 지원되지 않습니다.
- Mojo에 Rust 스타일 Traits 도입: Mojo는 Python보다 더 강력한 타입 시스템을 제공하는 것을 목표로 하며, Rust 스타일의 traits, where 절(clauses), 강력한 const generics, 그리고 variadic generics가 계획되거나 이미 구현되었습니다.

또한 Mojo는 linear types를 가지고 있으며, static reflection API를 위해 Zig에서 영감을 얻었고, 궁극적으로 타입 시스템 기능 면에서 Dependent Haskell 또는 Idris에 근접하는 것을 목표로 합니다.
- UV 설치 시 Mojo 디버깅 불가: Mojo SDK가 wheel로 설치된 경우, ‘mojoFile’ 옵션을 사용하여 Mojo 파일을 디버깅하는 것은 지원되지 않습니다.

사용자들은 문서에 따라 pixi를 사용하고 apple M3와 같은 표준 프로세서를 사용하더라도 여전히 디버깅이 되지 않는다고 보고합니다.
- Vectorize 함수 세부 정보: Mojo의 vectorize 함수는 데이터 레이아웃에 신경 쓰지 않습니다. 사용자에게 오프셋 i에서 다음 width 요소를 로드하도록 지시하고 drain loop를 처리합니다.

vectorize를 사용하여 효율적인 처리를 위해서는 List[ComplexSIMD]를 사용하는 대신 데이터를 SoA (Structure of Arrays) 형식으로 재구성하는 것이 권장됩니다.

---

### Modular (Mojo 🔥) ▷ #max (3개 메시지):
> Max 하드웨어 지원, vLLM SGLang, Max docker 외부 OAI endpoint
- Max 하드웨어 지원 계층화: Modular Docs에서 현재 GPU 호환성에 대한 계층화된 목록을 확인할 수 있습니다.

20XX GPU에서 GPU 기능을 테스트할 수 있지만, 전체 모델은 거기서 작동하지 않을 수 있으며, H100에서는 작동할 것입니다.
- Max가 처리량 면에서 vLLM/SGLang을 능가할 수 있습니다: 모델에 따라 Max는 H100에서 처리량 면에서 vLLM 또는 SGLang에 비해 이점을 보일 수 있습니다.

스스로 테스트하려면 이 벤치마킹 가이드를 따르십시오.
- OAI Endpoint Exposable Outside Docker: OpenAI endpoint는 Docker 컨테이너 외부로 노출되어야 합니다.

---

### Nous Research AI ▷ #general (255 messages🔥🔥):
> Sora 2 Video Generation, Prompt Rewriting by LLMs, HP G1a R-AI-Max+ Pro 395 Performance, LPDDR5X vs DDR5, Qwen VL Model quirks and regressions
- Sora 2 Generates Anime but Falls into IP Jail: 회원들은 Sora 2 video generation model의 애니메이션 결과물이 놀랍도록 좋다는 것을 발견했지만, 해당 모델이 프롬프트를 재작성하고 밤새도록 저작권이 있는 콘텐츠를 전면 금지하기 시작하여, 한 트윗에 따르면 사실상 창의적 잠재력을 저해했다고 언급했습니다.

Sora 경험은 “speedrunning enshittification”으로 묘사되었으며, 많은 이들이 실망감을 표했습니다: “거의 영구적으로 무력화되었고, 그 시점에서. 왜 존재해야 하는가?”
- Ryzen AI Mini PCs Rock LPDDR5X: AI 테스트를 위해 128GB LPDDR5X를 탑재한 HP G1a R-AI-Max+ Pro 395를 확보했습니다. 이 제품은 온보드에 납땜되어 교체 불가능한 LPDDR5X를 특징으로 하며, 이는 일반적으로 DDR5보다 훨씬 빠르고 전력 소모가 적습니다.

이 Samsung Semiconductor 링크에 따르면, LPDDR5X는 DGX Spark와 인기 있는 Ryzen AI Mini PC가 8000 Mhz RAM 속도와 250GB/s 이상의 버스 속도를 달성할 수 있도록 합니다.
- Qwen VL Shows Weird Regressions and Clock Inaccuracy: 회원들은 Qwen 2.5 VL의 특이한 점에 대해 논의했습니다. 이 모델은 vision task에서 더 작은 모델이 때때로 더 큰 모델보다 성능이 좋지만, 텍스트가 주어지면 lost-in-the-middle 현상으로 어려움을 겪습니다.

한 clock reading 테스트에서, 이 모델은 두 개의 시계를 완전히 놓치고 대부분 틀린 것으로 나타났습니다. 반면 로컬 vllm 실행은 5개 중 3개를 맞추고 뒤집힌 숫자를 제외하고는 대부분 정확했습니다.
- Multi-Step Transformer Loss Tuning: 한 회원이 multi-step transformer loss를 사용한 자신의 실험에 대해 설명했습니다. 이는 transformer의 중간 계층 중 일부에서 hidden vector를 선택하여 input vector에 추가한 다음, 또 다른 forward pass를 수행하는 방식입니다.

그들은 이것이 정말 단순한 gemma 3 270 mit model에서 수행되었지만, cli agent가 학습 및 알고리즘 조작 실험에 대한 접근을 가능하게 하는 것이 정말 놀랍다고 언급했습니다.

### Nous Research AI ▷ #research-papers (1 messages):
> Dragon Hatchling Paper, Transformer Brain Link
- Dragon Hatchling: Transformer’s Brainy Missing Link?: 한 멤버가 “The Dragon Hatchling: The Missing Link Between the Transformer and Models of the Brain” (ArXiv:2509.26507)이라는 제목의 논문 링크를 공유했습니다.
- 이 논문은 Transformer 아키텍처와 뇌 모델 간의 잠재적인 연결을 탐구합니다.
- 추가 뇌 모델 탐색 요청: 최소 항목 수를 충족하기 위해 뇌 모델 및 Transformer 아키텍처와 관련된 추가 연구 또는 논의 사항을 고려해 주십시오.
- 예를 들어, Dragon Hatchling 논문에 대한 커뮤니티 피드백이나 다른 해석이 있다면 자세히 설명해 주십시오.

---

### Nous Research AI ▷ #interesting-links (2 messages):
> Brazilian Miku Nous girl
- Brazilian Miku Nous Girl 공유: 한 멤버가 X에서 Brazilian Miku Nous girl 링크를 공유했습니다.
- 해당 게시물은 ee.dd.라는 제목이었습니다.
- 또 다른 Brazilian Miku Nous Girl 공유: 다른 멤버도 X에서 Brazilian Miku Nous girl 링크를 공유했습니다.
- 이 두 번째 게시물은 ee.dd.의 변형된 제목을 가지고 있었습니다.

---

### Nous Research AI ▷ #research-papers (1 messages):
> Dragon Hatchling Paper, Transformer Brain Link
- Dragon Hatchling 논문 공개: 한 멤버가 “THE DRAGON HATCHLING: THE MISSING LINK BETWEEN THE TRANSFORMER AND MODELS OF THE BRAIN”이라는 제목의 새 논문 링크를 공유했습니다 (Arxiv link).
- Dragon Hatchling: Transformer와 뇌 모델 연결: “THE DRAGON HATCHLING”이라는 제목의 이 논문은 Transformer와 뇌 모델 간의 잠재적인 연결을 탐구합니다 (Arxiv link).

### Latent Space ▷ #ai-general-chat (184 메시지🔥🔥):
> DeepSeek의 CUDA 장악력, Sora 2 프리미어, Claude 대 인간 팀, Goodfire의 교훈, Kilo Code의 조회수
- DeepSeek, 중국의 CUDA 락인(lock-in)을 깨다: DeepSeek의 FP8 사양과 TileLang 언어는 공유 표준과 쉬운 프로그래밍 브리지를 통해 Nvidia의 CUDA 락인(lock-in)을 완화하는 것을 목표로 하며 중국 칩 주식을 끌어올리고 있습니다. 이는 전략적 제휴이지만, 성능 격차는 여전히 존재합니다. 더 자세한 내용은 X에서 확인하세요.
- Sora 2, 오리 단편 영화를 공개하다: OpenAI는 Sora 2를 사용하여 "The Quack: Part 1"이라는 제목의 30초 분량 AI 생성 단편 영상을 공개했으며, 포스터와 초대 코드(FRIYAY)와 함께 경외감, 흥분, 그리고 오리 카메오 밈을 불러일으켰습니다. X에 링크되어 있습니다.
- 에이전틱 브라우저, 브라우저 전쟁터가 되다: 에이전틱 브라우저는 AI 기업들이 경쟁하는 새로운 전장입니다. 지난 금요일 AIIA 통화에서 새로운 Claude 브라우저가 시연되었으며, 얼리 액세스 사용자는 일부 사용 사례에서 상당히 유능하다고 강조했습니다.
- Kilo Code의 조회수 주장에 의문이 제기되다: Discord에서는 Kilo Code 프로모션에 대한 Nick Baumann의 950만 유기적 조회수 주장에 대해 논쟁하고 있습니다. 해당 지표가 유료 광고/봇을 나타낸다고 주장하며 RooCode의 더 낮은 수치를 인용합니다. 원본 X 게시물을 확인하세요.
- GPT-5 전환이 반발을 불러일으키다: ChatGPT는 이제 고통 지원(distress support)이라는 명목으로 민감한 대화를 더 저렴한 GPT-5로 자동 라우팅합니다. 사용자들은 가스라이팅, 모델 전환, 잘못된 라벨링에 분노하며 #Keep4o 캠페인을 시작했습니다.

---

### Latent Space ▷ #genmedia-creative-ai (15 메시지🔥):
> Pika를 빠르게 추월하는 거대 기업들, AI 말 타는 우주비행사, OpenAI의 Medal 인수 실패, 일반적인 직관
- Pika의 AI 꿈, 거대 기업들에 의해 좌절되나?: Chongz Luong은 한때 크게 홍보되었던 스타트업 Pika가 Google, Meta, OpenAI에 의해 Veo 3, Vibes, Sora 2와 같은 고급 AI 비디오 모델을 통해 빠르게 추월당했다고 언급합니다 (xcancel.com).

이 논의는 재정적 힘이 AI 지배력을 어떻게 이끄는지 강조하며, 업계의 초점이 모델에서 도구로 전환됨에 따라 Pika가 관련성을 잃는 많은 홍보된 스타트업 중 첫 번째가 될 수 있다고 예측합니다.
- Sora의 우주비행사 승마 혁명: Pliny는 Sora 2 Pro가 작년의 불가능했던 정지 이미지 프롬프트에서 인상적인 저중력 비디오로 도약한 것을 찬양합니다. 이 비디오는 우주비행사 등에 업힌 사실적인 말과 즉흥적이고 자발적으로 생성된 미션 컨트롤 유머를 담고 있습니다 (xcancel.com).

답글들은 빠른 생성형 AI의 발전, 벤치마크, 그리고 내부 농담에 대해 논의합니다.
- OpenAI의 5억 달러 규모 Medal 인수 시도 실패!: 보도에 따르면 OpenAI는 작년에 게이머 비디오 플랫폼인 Medal을 5억 달러에 인수하려 했습니다. 이는 모델 훈련을 위한 영상 자료를 확보하기 위함이었습니다 (xcancel.com).

이 거래는 무산되었고, Medal은 대신 자체 AI 부서인 General Intuition을 출범시켰으며, 이 부서는 현재 1억 달러 규모의 투자 유치를 마무리하고 있습니다.

---

### Yannick Kilcher ▷ #general (77 messages🔥🔥):
> AI Sex Robots, Unitree R1 Robot, Discord Data Breach, Safety Benchmark, Automating Scientific Method
- 로보-로맨스: AI 섹스 로봇이 다가오는가?: 사용자들은 AI 섹스 로봇이 언제쯤 덜 어색해질지에 대해 논의했습니다. 이는 서투르지만 6천 달러의 저렴한 로봇인 Unitree R1의 출시와 맞물렸습니다 (X link).

이 대화는 영국의 연령 확인 요건과 동시에 진행되었으며, 일부 사용자들은 이미 로봇 아내와의 특이점(Singularity)을 예상하고 있습니다.
- 디스코드 재앙: 고객 서비스 데이터 유출 공개: 한 사용자가 Discord 고객 서비스 데이터 유출을 보도한 The Verge 기사 링크를 공유했습니다 (The Verge).

이는 일부 회원들이 Dan Chollet의 안전 벤치마크에 대해 논의하고 (Tweet 1, Tweet 2), NIST가 DeepSeek AI 모델에 대한 평가에서 단점과 위험을 발견한 시점에 나왔습니다 (NIST report).
- 행렬 수학 필수: 딥러닝은 선형 대수를 요구합니다: 한 사용자가 AI를 시작하기 위해 복잡한 행렬 수학을 배워야 하는지 문의했습니다.

회원들은 선형 대수, 다변수 미적분, 그리고 확률 개론을 배울 것을 조언하며, 커뮤니티 칼리지 강좌, YouTube 영상, 스탠퍼드 및 버클리 대학의 강의 노트와 같은 자료들을 추천했습니다.
- AGI 적대감: 일반 지능은 과대평가되었는가?: 한 사용자가 AGI에 대한 집착에 의문을 제기하며, 도메인별 ML 시스템이 세상을 더 효과적으로 변화시킬 것이라고 주장했습니다.

다른 사용자들은 AGI의 정의에 대해 논쟁했으며, 한 사용자는 AGI를 어떤 분야의 전문가에게 주어지는 모든 작업을 완료할 수 있는 능력으로 정의했습니다. 일부 사용자들은 이를 특이점(The Singularity)과 연결했습니다 (wikipedia).
- MoGs 매니아: AGI 지표로서의 Mixture of Gaussians: 한 회원이 대규모 파운데이션 모델이 Mixture of Gaussians (MoGs)를 암묵적으로 학습할 수 있다는 이론적 보장을 공유하며, 여러 논문(arxiv 1, arxiv 2, arxiv 3, aclanthology)을 인용했습니다.

그들은 AGI를 K개의 구성 요소를 가진 MoGs로 설명되는 데이터 분포를 대규모 파운데이션 모델이 완벽하게 모방할 수 있는 능력으로 정의할 것을 제안합니다.

### Yannick Kilcher ▷ #paper-discussion (15 messages🔥):
> Low Rank Gradients paper, Paper Exploration session, Peer pressure and beer
- 새로운 논문이 Low Rank Gradients에 대한 관심을 불러일으킵니다: 한 멤버가 새로운 논문인 Low Rank Gradients를 공유했고, 이는 관심과 잠재적인 탐색 세션을 촉발했습니다.
다른 멤버는 이 논문이 복잡한 LA (Linear Algebra)에 중점을 둔다고 언급했습니다.
- 유저가 논문 탐색에 대한 압박을 받습니다: 한 멤버가 다른 멤버에게 Low Rank Gradients 논문의 탐색 세션을 이끌고 싶은지 물었습니다.
다른 멤버는 복잡한 수학이 관련될 때 검은 고양이 사진을 사용하는 특정 멤버를 영입하는 것을 좋아한다고 농담했는데, 그가 훌륭한 공동 작업자이기 때문이라고 덧붙였습니다.
- 고양이 사진: 한 멤버는 고양이 비디오와 함께 "이상한 사람들은 어떤 이유에서인지 고양이 사진을 사용한다"고 농담했습니다.

---

### Yannick Kilcher ▷ #agents (2 messages):
> Diffusion Problem, Conditioning Signal, Guidance Weight
- Diffusion Model은 더 강력한 신호가 필요합니다: 한 멤버는 문제를 diffusion process로 구성할 것을 제안하며, conditioning signal이 충분히 가중되지 않은 것 같다고 언급했습니다.
그들은 출력이 스토리보드의 눈 배경에 더 가깝게 일치하도록 conditioning/guidance weight를 늘릴 것을 권장했습니다.
- 배경 언더피팅이 확인되었습니다: 다른 멤버는 모델이 배경에 언더피팅되고 있다는 데 동의하며, 더 강력한 conditioning signal의 필요성을 입증했습니다.
원본 게시자는 어떤 agent를 사용하든 그것을 어떻게 적용해야 할지 모른다고 인정했습니다.

---

### Yannick Kilcher ▷ #ml-news (9 messages🔥):
> LLM Reasoning, Tree-of-Thought, Gemini 2.5 Deep Think, GPT-5 Math Claims
- LLM 추론의 순진한 가정이 논의됩니다: 멤버들은 LLM이 순진하게 가정된 것처럼 실제로 reasoning Token을 사용하는지에 대해 논의하고 있으며, 성능 향상에도 불구하고 "reasoning"이 종종 사후 정당화처럼 보인다고 언급했습니다.
간단한 작업을 과도하게 생각하는 것과 같은 문제들이 현재의 reasoning이 실제적이라기보다는 수행적인(performative) 것일 수 있음을 나타낸다고 언급되었습니다.
- 다중 모델 추론이 탐색됩니다: 한 멤버는 서로 다른 출처의 여러 작은 모델이 함께 reasoning할 수 있는지 탐색할 것을 제안했으며, 각 모델이 솔루션을 향한 한 단계를 기여하는 방식입니다.

이 접근 방식은 inter-token weirdness에 의존하지 않고 의미론적으로 유용한 추론 단계를 장려하는 것을 목표로 합니다. 하지만 다른 멤버는 이 방식이 너무 비싸서 가치가 없을 수도 있다고 언급했습니다.
- Tree-of-Thought 변형 제안: 한 멤버는 다중 모델 추론 개념이 Tree-of-Thought (arxiv link)의 변형일 가능성이 높으며, 단계들이 전문가 모델로 라우팅되는 semi-agentic 파이프라인을 사용할 수 있다고 제안했습니다.

더 큰 모델보다 비용 효율성은 떨어지지만, 표준 프롬프팅을 넘어 성능을 확장할 수 있다는 점이 언급되었습니다. Google의 "Deep Think"와 Grok의 "Super Think"는 이 블로그 게시물과 같은 유사한 접근 방식을 사용할 수 있습니다.
- GPT-5의 수학 능력 주장: 수학자들이 GPT-5가 수학 문제 해결에 도움을 준다는 주장이 Tweet 1 및 Tweet 2 링크와 함께 제시되었습니다.

한 멤버는 첫 번째 트윗이 2025년 8월 1일에 게시되었다는 점을 언급하며, 이는 미래에 대한 주장임을 시사했습니다.

---

### Eleuther ▷ #general (68 messages🔥🔥):
> Diffusion Model 평가, Gemma 아키텍처 채택, AI 메모리를 위한 Synaptik Core, COLM Eleuther 밋업, AO3 스토리 서브셋
- 인간이 평가한 Diffusion Model: 멤버들은 FID/CLIPScore, 수동 인간 평가, 비디오용 FVD와 같은 자동화된 지표를 포함한 Diffusion Model 평가에 대해 논의했습니다.

한 멤버는 Sora 2를 가지고 놀면서 비디오 평가가 훨씬 더 원시적이라고 느껴져 비디오 측면에 대한 호기심이 생겼다고 말했습니다.
- Gemma의 아키텍처는 Qwen만큼 인기가 없습니다: 멤버들은 LM Arena에서 강력한 성능을 보였음에도 불구하고 Gemma의 아키텍처가 Qwen만큼 많이 사용되지 않는 이유에 대해 논의했습니다.

한 멤버는 아키텍처가 LLM 성능의 주요 동인이 아니며, 훈련 데이터와 파인튜닝 분포가 더 중요하다고 제안했습니다.
- Synaptik Core, AI 신뢰도 향상: Synaptik Core의 Janay는 AI 시스템에서 검증 가능한 장기 메모리 및 감사 가능성을 위한 툴체인인 Synaptik Core를 소개했습니다.

그녀는 AI 에이전트를 시연하는 LinkedIn 게시물과 OpenAI Open Model Hackathon 직전의 그녀의 스프린트를 설명하는 또 다른 게시물(link)을 공유했습니다.
- COLM에서의 Eleuther 타코: 멤버들은 COLM (Conference on Language and Model)에서의 밋업과 Featherless AI가 주최하는 타코 소셜(Luma link)을 언급했습니다.

그들은 커피나 맥주를 마시며 만나기를 희망했습니다.
- 더 쉬운 학습을 위한 AO3 서브셋 제작: 멤버들은 TinyStories와 유사하게 더 쉬운 학습을 위해 더 간단한 문법 구조를 가진 AO3 (Archive of Our Own) 스토리 서브셋을 만드는 것에 대해 논의했습니다.

그들은 데이터를 필터링하기 위해 가독성 점수를 사용하는 것을 고려했지만, 노이즈 제거에 대한 우려가 제기되었습니다.

---

### Eleuther ▷ #research (7개 메시지):
> Manning과 Csordas, top-k attention, MoE, BabyLM, hopfield-like optimizer
- Top-K Attention이 MoE 아키텍처를 확장합니다: 한 멤버는 top-k attention이 MoE와 top-k 라우터를 사용하는 MLP에서 보았던 것의 자연스러운 확장처럼 느껴진다고 언급했습니다.

그는 또한 재귀 없이 UT를 수행하는 것이 동기라고 밝혔으며, 만약 누군가 훈련에 사용하려 한다면 클러스터링이 gradient 흐름을 허용하고 모델의 computational graph에 불연속성을 유발하지 않도록 보장하는 방법에 대해 질문했습니다.
- 효율적인 인지적으로 타당한 모델을 위한 BabyLM: 한 멤버는 미래에 LLM으로 간주되는 것들이 다양한 기술을 통해 다시 "더 작아질" 가능성이 높으므로, 애초에 (추정상) 더 효율적인 인지적으로 타당한 모델을 살펴보는 것이 어떻겠냐고 말했습니다.

이러한 점에서 BabyLM과 같은 이니셔티브는 높이 평가됩니다.
- Hopfield-Like Optimizer가 Momentum을 버퍼링합니다: 한 멤버는 최종 업데이트 단계를 위해 N개의 momentum 버퍼가 결합된 optimizer를 제안했으며, 들어오는 벡터로부터 각 버퍼에 대한 hopfield (또는 hopfield-like) 업데이트가 있었습니다.

그들은 이것이 업데이트 행렬의 approximate top eigenvectors를 사용하여 많은 멋진 일들을 할 수 있는 길을 열어준다고 제안했습니다.

---

### Eleuther ▷ #scaling-laws (8개 메시지🔥):
> Block Scaling Factors vs Per-Float, Attention as Inductive Bias, nanoGPT Speedrun
- Block Scaling이 Per-Float Scaling을 능가합니까?: Rain AI의 block scaling factors가 per-float scaling factors보다 우수하다는 주장은 mxfp8에 의해 입증되는 것처럼 보이지만, 이는 Nvidia의 이전 제품 출시를 고려할 때 그들이 가졌을 수 있는 모든 경쟁 우위를 무효화합니다.

한 멤버는 inductive bias로서 Attention의 효과가 단순히 우연의 일치인지 아니면 Noam Shazeer의 선견지명 때문인지 질문했습니다.
- Attention: 단순히 GPU 편의성 이상인가요?: 한 멤버는 Attention 메커니즘의 또 다른 핵심 인물로 Dzmitry Bahdanau를 제안하며, Attention이 이미 "대세"였다는 것을 인정한 Andrej Karpathy의 게시물에 연결했습니다.

다른 이들은 GPU로 훈련하는 데 편리하다고 제안합니다.
- nanoGPT Speedrun 기록 시간 단축: 한 멤버는 LessWrong 게시물을 공유하며 nanoGPT speedrun 세계 기록이 3개월 만에 20% 단축되었고, 이는 더 작은 규모에서도 진전이 있음을 시사한다고 언급했습니다.

### MCP Contributors (Official) ▷ #general (48 messages🔥):
> GitHub 팀 관리 마이그레이션, MCP Tools 버전 관리, Cloudflare의 Code Mode, OpenAI의 AppsSDK

*   **GitHub 팀 관리 Infrastructure-As-Code로 마이그레이션:** 팀은 GitHub 팀 관리를 Infrastructure-As-Code로 마이그레이션했으며, modelcontextprotocol/access에서 코드를 통해 멤버십과 리포지토리 권한을 관리합니다.

    이번 마이그레이션은 커뮤니티 소유권, 투명성, 감사 추적, 그리고 AI 친화적인 접근 관리(access management)를 목표로 하며, 배포 중에는 일시적인 접근 중단이 예상됩니다.
*   **Intuit 엔지니어, MCP Tool 버전 관리 문제 조사:** 한 Intuit 엔지니어는 MCP Servers에서 MCP Tools의 버전 관리(versioning)에 어려움을 겪고 있으며, 특히 대규모 종속성 관리(dependency management) 및 호환성 문제에 직면해 있습니다. 현재 협력자를 찾고 있습니다.

    그들은 잠재적인 해결책이 담긴 SEP를 작성했으며, 이는 modelcontextprotocol/modelcontextprotocol#1575에서 확인할 수 있습니다. 현재 피드백을 기다리고 있습니다.
*   **Cloudflare의 Code Mode는 MCP를 과도하게 설계하는 것일까요?:** Cloudflare의 Code Mode에 대한 논의가 있었으며, 일부는 이 [블로그 게시물](https://blog.cloudflare.com/cloudflare-code-mode)에 따라 Code Mode가 MCP를 오해하거나 툴 호출(tool call)을 Cloudflare worker에 대한 요청으로 과도하게 설계(over-engineer)한다고 주장했습니다.

    Code Mode가 에이전트가 결과를 전달하는 데 필요한 턴(turn) 수를 줄일 수 있다는 점은 주목되었지만, 성능과 단순히 웹 API 또는 클라이언트 SDK를 사용하는 것보다 나은지에 대한 우려가 있었습니다. 이 [프로토타입](https://github.com/cloudflare/code-mode-prototype)을 기반으로 흥미로운 실험이 진행 중입니다.
*   **OpenAI AppsSDK, MCP-UI 중복 논쟁 촉발:** OpenAI는 AppsSDK를 출시하여 ChatGPT에 MCP와 함께 UI를 도입했으며, [발표](https://openai.com/blog/introducing-the-appssdk)에 따르면 TheFork가 런치 파트너로 참여했습니다.

    회원들은 MCP-UI와 협력하는 것이 더 나은 움직임이었을지 궁금해하고 있습니다. OpenAI는 이들이 자연스럽게 어우러지도록 협력할 예정이며, ACP를 사용하는 앱의 트랜잭션을 전적으로 지원할 것입니다.

---

### MCP Contributors (Official) ▷ #general-wg (13 messages🔥):
> Feature Support Matrix 명확화, 초기화 요청 시 서버 기능(Server capabilities), 서버 내 아이콘 메타데이터

*   **MCP Feature Support Matrix 발견:** 한 회원이 Model Context Protocol의 Feature Support Matrix에서 "Discovery"의 의미에 대해 문의했습니다.

    Discovery는 서버 기능(server capabilities)과 툴 변경 사항을 전달하는 능력(ability to communicate tool changes)을 의미하는 것으로 보입니다.
*   **서버 기능(Server Capabilities)이 예상치 못하게 전송됨:** 한 회원은 강연에서 Cursor를 언급하며, Cursor가 초기화 요청(initialization request) 시 서버 기능(server capabilities)을 전송한다고 지적했습니다.

다른 멤버는 이것이 예상치 못했지만 무해하다고 인정했으며, 첨부된 이미지에서 서버 기능이 노란색으로 강조되어 있었습니다.
- 아이콘 메타데이터 사용 사례 설명: 한 멤버가 서버에 아이콘 메타데이터를 추가하는 사용 사례에 대해 설명을 요청했으며, 다른 서버 프리미티브에도 아이콘이 존재한다는 점을 언급했습니다.

다른 멤버는 아이콘이 애플리케이션에 대한 시각적 표현을 제공한다고 설명했습니다. 예를 들어, 도구 목록에서나 도구를 실행할 때 채팅 내에서 인라인으로 표시되는 것과 같이 말입니다.

---

### Moonshot AI (Kimi K-2) ▷ #general-chat (47 messages🔥):
> Kimi-latest vs Kimi-k2, K1.5 is proprietary, em dashes usage, translation, Sam Altman introducing OK computer
- Kimi-latest는 Kimi-K2가 아닙니다: "Kimi-latest"는 Kimi 어시스턴트를 구동하는 비공개 프로덕션 Kimi LLM을 항상 가리키는 별칭이며, "Kimi-K2"는 별도의 오픈 웨이트 MoE 패밀리(예: k2-0905)입니다.

moonshot.ai의 "proprietary LLM" 문구는 UI에 K2가 눈에 띄게 표시되어 있더라도 K2가 아닌 해당 비공개 스택을 지칭합니다.
- 일부는 em dash가 있는 메시지를 무시합니다: 한 사용자는 AI와 대화하는 경우를 제외하고 em dash가 포함된 메시지를 다른 사람들이 즉시 무시하는지 궁금해했습니다.

다른 사용자는 평생 em dash를 사용해 왔으며, 사람들이 자신을 봇으로 생각하지 않도록 사용 본능을 억제해야 한다고 답하며 Kimi에 대한 애정을 보여주는 이미지 매크로를 첨부했습니다.
- Kimi의 번역 검열 문제: 한 멤버는 Kimi의 검열로 인해 때때로 작성한 모든 내용을 삭제하고 '죄송합니다. 이 문제에 대해서는 도움을 드릴 수 없습니다'로 대체되는 경우가 있다고 말했습니다.

그들은 Qwen이 백만 개의 context를 가지고 있고 번역에 더 뛰어나므로 번역에 Qwen을 사용할 것을 제안했습니다.
- 주주는 재미와 이익을 극대화합니다: 한 사용자는 Alibaba 주식의 8%를 비중으로 포함하고 있으며, Alibaba가 Moonshot에 약 35%의 지분을 가지고 있는 펀드에 100 USD 이상을 투자했다고 밝혔습니다.

그 멤버는 주주 가치를 극대화하지 않는다면 인간 삶의 의미가 무엇이냐고 주장했으며, 다른 사람들은 이를 재미있어했습니다.

---

### aider (Paul Gauthier) ▷ #general (34 messages🔥):
> Deepseek Browser testing with Claude, Aider vs. Opencode, Agentic tools and ripgrep, Aider and GPT-5 Codex, Aider with Emacs
- Deepseek, Claude로 Browser 테스트: 한 멤버가 Claude CLI 및 Chrome DevTools MCP를 사용한 Deepseek Browser 테스트에 대한 블로그 게시물을 공유했습니다.

그는 Deepseek이 툴 작업에서 더 나은 성능을 보여주기 때문에 Claude Code와 Opencode에서 Anthropic API를 통해 Deepseek을 사용하기도 합니다.
- Aider에 수동 컨트롤이 필요합니다: 한 멤버는 Aider에서 가장 아쉬운 기능으로 /tokens, /add, /remove, /clear와 같은 컨텍스트에 대한 수동 컨트롤을 꼽았습니다.

그들은 이 기능이 다른 모든 툴에서 절실히 필요하지만 아직 구현된 곳이 없다고 느끼며, 대규모 codebase의 경우 Aider가 이들(다른 툴)에 비해 경쟁력이 없다고 주장했습니다.
- Aider에 Agentic Grep이 필요합니다: 멤버들은 agentic 툴이 필요한 부분을 찾기 위해 regex grep을 사용하고 주변 라인에 대해 “views”를 수행하는 방식과, Aider에 ripgrep agentic handler가 없다는 점에 대해 논의했습니다.

다른 멤버도 동의하며, agentic grep이 Aider를 현 세대 툴과 경쟁력 있게 만드는 데 큰 도움이 될 것이라고 말했습니다.
- Aider가 GPT-5 Codex를 지원합니다: 한 멤버가 Aider가 GPT-5-Codex와 작동하도록 하는 patch가 있었는지 문의했습니다.

다른 멤버는 main에 지원이 추가되었다고 답변했습니다.
- Aider가 Emacs와 통합됩니다: 한 멤버는 개인적으로 자신이 잘 이해하는 codebase/task의 경우, Aider가 원하는 바를 정확히 수행할 수 있게 해주며 필요한 context를 제공하기 때문에 빠르다는 점에서 훌륭하다고 항상 느껴왔다고 말했습니다.

덤으로 Emacs와도 잘 통합됩니다.

---

### aider (Paul Gauthier) ▷ #questions-and-tips (4개 메시지):
> aider prompt cache, image analysis
- Aider Prompt Cache가 기본적으로 활성화됩니다: Aider의 models.py는 이제 provider Z.aialso에 대해 cache_control: bool = True 및 caches_by_default: bool = True를 설정하며, greeting message에 “prompt cache”를 추가합니다.

새로운 greeting message는 다음과 유사할 것입니다: “Main model: openrouter/deepseek/deepseek-v3.2-exp with diff edit format, 8k think tokens, prompt cache”.
- 사용자가 Image Analysis에 대한 도움이 필요합니다: 한 사용자가 hello, sorry i need some help…라는 메시지를 공유하고 Am i doing something wrong?이라고 묻기 위해 이미지를 첨부했습니다.

첨부된 이미지는 사용자가 도움을 필요로 하는 문제와 관련이 있을 것으로 보입니다.

### aider (Paul Gauthier) ▷ #links (1 messages):
- Open vs Closed Weights: 가격 우려 제기: 오픈 소스 대 클로즈드 소스 AI 모델 Weights 비교 논의가 진행되며, 클로즈드 Weights 소유자가 임의로 가격을 인상할 수 있다는 우려가 제기됩니다.
- Aider의 Tooling: 최신 상태 유지: Aider tool의 최신 업데이트 및 기능에 대한 지속적인 논의가 이루어지고 있으며, 사용자들이 tool의 잠재력을 최대한 활용할 수 있도록 합니다.

---

### DSPy ▷ #show-and-tell (1 messages):
- Neosantara AI, LLM Gateway 출시: Neosantara AI가 AI 앱 구축을 위한 새로운 LLM Gateway Platform을 무료로 출시했습니다.
- Neosantara AI에 DSPy 통합: 사용자들은 Neosantara AI를 DSPy와 통합할 수 있으며, 관련 문서는 여기에서 확인할 수 있습니다.
- 무료 월간 Token 신청: 신규 사용자들은 가입 시 매월 1만 개의 consume Token을 받습니다.

사용자들은 halo@neosantara.xyz로 이메일을 보내 피드백을 제공할 수 있습니다.

---

### DSPy ▷ #general (26 messages🔥):
- DSPy 로드맵: Issues 및 Changelog 그 이상?: 한 멤버가 승인된 개선 사항 및 새 릴리스에 대한 GitHub Issues 및 changelog 외에 DSPy 로드맵에 대해 문의했으며, X/Twitter의 최근 DSPy 언급 및 DSPy 공식 계정 링크가 공유되었습니다.

다른 멤버는 Drew Houston의 DSPy 사용 게시물과 Huyen Chip의 React+Reflection에 대한 블로그 게시물 링크를 공유했습니다.
- ReAct Trajectories: Disk vs. Memory?: 한 멤버가 더 긴 agent 단계에서 ReAct trajectories를 disk에 유지할지 memory 객체에 유지할지에 대해 질문했으며, 다른 멤버는 Decision Tree 개념을 가진 DSPy 프로젝트인 Weaviate의 Elysia를 추천했습니다.

다른 멤버는 React 부분의 전체 trajectory에 대해 성찰하는 방법으로 React+Reflection을 구현하는 것을 고려하고 있습니다.
- Hardcoded Fallback 문제 해결?: 한 멤버가 DSPy의 fallback 동작 변경에 대해 문의했으며, 현재는 hardcoded fallback임이 확인되었습니다.
- DSPy-ReAct-Machina 출시: 한 멤버가 DSPy-ReAct-Machina의 출시를 발표했습니다. 이는 단일하고 지속적으로 확장되는 context history를 유지하여 multi-turn 대화를 가능하게 하는 DSPy용 대체 ReAct 구현이며, PyPI에 게시되었습니다.

그들은 동기와 아키텍처를 설명하는 블로그 게시물도 공유하고 피드백을 요청했습니다.

---

### tinygrad (George Hotz) ▷ #general (메시지 15개🔥):
> tinygrad hiring process, tinygrad bounties, tinygrad meeting #91, tinygrad nir backend, tinygrad pattern matcher
- tinygrad 개발자들은 bounty 관문을 통과해야 합니다: 한 사용자가 직접 채용 기회에 대해 문의했지만, tinygrad의 채용 과정은 먼저 bounty를 해결하는 것을 중심으로 이루어진다는 답변을 받았습니다.

개인 인터뷰나 직접 채용은 없습니다.
- 다음 주 월요일 tinygrad 이사회 회의: tinygrad meeting #91의 안건에는 회사 업데이트, symbolic 관련 사항, rangeify 버그, 속도 개선, 정리 작업, 그리고 bounty 논의가 포함됩니다.

회의는 월요일 샌디에이고 시간 오전 6시 또는 홍콩 시간 오후 9시에 개최됩니다.
- NIR backend가 준비되었습니다: NIR backend가 이제 검토 준비를 마쳤습니다 (PR #12089).

다른 정보는 공유되지 않았습니다.
- pattern matcher에 match statements 사용 고려: 논의에서는 렌더링된 코드에서 반복되는 if statements 대신 pattern matcher를 컴파일할 때 match statements를 사용하는 방안을 모색했습니다.

팀은 이 접근 방식을 시도하기로 동의했습니다.

---

### tinygrad (George Hotz) ▷ #learn-tinygrad (메시지 8개🔥):
> tinygrad C++ port, tinygrad ONNX frontend, 3dgs repo
- 효율성을 위해 tinygrad 포트를 고려하는 3dgs Repo: 3dgs repo (LichtFeld-Studio)의 관리자는 libtorch를 제거할 계획이며, 추론 및 CUDA 지원을 위해 tinygrad를 C++로 포팅하는 것을 고려하고 있습니다.

한 멤버는 유지 관리되는 python tinygrad를 사용하여 모델을 CUDA 또는 C kernels로 컴파일한 다음 해당 kernels가 포함된 C 코드를 내보내고, EfficientNet C 예제에 연결하는 것을 제안했습니다.
- tinygrad의 ONNX Frontend가 해결책이 될 수 있습니다: 한 멤버는 빠른 해결책으로 tinygrad의 ONNX frontend를 사용하고 모델의 사용 가능한 ONNX 버전을 활용할 것을 제안했습니다.

다른 멤버는 추론 실행에는 문제가 없을 것이라고 확인했습니다.
- tinygrad를 C++로 포팅: 관리자는 포팅 작업에 개의치 않는다고 말했습니다. 그 이유는 tinygrad가 융합된 kernels에 대한 최적화를 제공하여 모든 경우에 직접 작성할 필요가 없기 때문입니다.

관리자는 대안적인 접근 방식으로 python interpreter를 포함하고 Blender와 유사하게 python에서 tensor 연산이 많은 작업을 수행하는 것을 언급했습니다.

### Manus.im Discord ▷ #general (6 messages):
> 리드 생성용 MCP 서버, Manus iOS 클라이언트 충돌, 개인 컨텍스트가 부족한 AI 도구
- 맬웨어 MCP 서버 배포!: wrangler/Cloudflare를 사용하여 도매 리드 생성을 위한 MCP 서버가 배포되었습니다. 이 서버는 특정 정부 사이트에서 저평가된 부동산과 적극적인 판매자를 찾아내고, 최종 구매 제안을 위한 분석 자료를 제공하며, 주소는 [wholesale-lead-generator-frontend.pages.dev](wholesale-lead-generator-frontend.pages.dev)입니다.

한 사용자는 농담으로 "삭제되기 전에 맬웨어를 잡으세요"라고 말했습니다.
- Manus iOS 클라이언트 충돌 문제 해결!: Manus iOS 클라이언트에서 예약된 작업 인터페이스에 텍스트를 입력하기 위해 텍스트를 선택할 때 100% 멈추거나 충돌하는 버그가 보고되었으며, 한 엔지니어가 두 가지 해결책을 제시했습니다.

첫 번째 해결책은 "Apple Notes와 같은 별도의 앱에 명령이나 텍스트를 작성합니다. 텍스트를 복사합니다. Manus 앱에서 입력 필드를 한 번만 탭하여 커서를 놓고 텍스트를 붙여넣습니다. 해당 필드 내에서 텍스트를 선택하거나 편집하는 것을 피하세요."였습니다. 두 번째 해결책은 Apple의 내장 Shortcuts 앱과 키보드의 내장 클립보드 관리자를 사용하는 것이었습니다.
- 개인 컨텍스트가 부족한 AI 도구에 대한 해결책 발견: 한 멤버가 개인 컨텍스트가 부족한 AI 도구를 위한 경량 솔루션을 모색 중이며, 구체적인 목표를 가지고 있습니다.

목표는 "선택한 데이터에 연결하고, 질문을 통해 지식 격차를 해소하며, 현재 목표와 상황을 이해하고, 사용 중인 모든 도구에 해당 컨텍스트를 삽입하는 것"이었으며, Bay Area 협력자와 초기 사용자를 찾고 있습니다.

---

### MLOps @Chipro ▷ #events (1 messages):
> Feature Store Summit, 실시간 Feature Engineering, Vector Databases & Generative AI, Batch & Real-Time 워크플로우
- Feature Store Summit 5회 개최 발표: 연례 온라인 행사인 Feature Store Summit은 Uber, Pinterest, Zalando, Lyft, Coinbase와 같은 기업의 기술 연사들을 초청할 예정입니다.

이 서밋은 AI, ML 및 대규모 확장성과 실시간 기능을 요구하는 애플리케이션을 위한 인프라에 중점을 둡니다.
- 서밋 강연에서 대규모 실시간 Engineering 다룰 예정: 서밋 강연에서는 대규모 실시간 Feature Engineering, 프로덕션 환경에서의 Vector Databases 및 Generative AI, 그리고 Batch 및 Real-Time 워크플로우의 균형에 대해 다룰 예정입니다.

참석자들은 2025년 Feature Store의 발전을 이끄는 새로운 트렌드에 대한 토론도 기대할 수 있습니다.
- Feature Store Summit 날짜 발표: 서밋은 10월 14일 오전 8시 30분 PT (오후 5시 30분 CET)에 시작할 예정입니다.

관심 있는 분들은 제공된 링크를 통해 등록할 수 있습니다.

---

## 🔗 링크 (464개)

1. [https://news.smol.ai/](https://news.smol.ai/)
2. [https://www.youtube.com/watch?v=hS1YqcewH0c&t=1382s](https://www.youtube.com/watch?v=hS1YqcewH0c&t=1382s)
3. [https://www.youtube.com/watch?v=QIdUllqmuls](https://www.youtube.com/watch?v=QIdUllqmuls)
4. [https://openai.com/devday](https://openai.com/devday)
5. [https://openai.com/index/introducing-apps-in-chatgpt](https://openai.com/index/introducing-apps-in-chatgpt)
6. [https://developers.openai.com/apps-sdk](https://developers.openai.com/apps-sdk)
7. [https://www.youtube.com/watch?v=2C4Cs6503gw](https://www.youtube.com/watch?v=2C4Cs6503gw)
8. [https://openai.com/index/introducing-agentkit](https://openai.com/index/introducing-agentkit)
9. [https://www.youtube.com/watch?v=44eFf-tRiSg](https://www.youtube.com/watch?v=44eFf-tRiSg)
10. [https://platform.openai.com/docs/guides/agents/agent-builder](https://platform.openai.com/docs/guides/agents/agent-builder)
11. [https://chatkit.studio/](https://chatkit.studio/)
12. [https://platform.openai.com/docs/guides/chatkit](https://platform.openai.com/docs/guides/chatkit)
13. [https://openai.github.io/chatkit-python/](https://openai.github.io/chatkit-python/)
14. [https://openai.github.io/chatkit-js/](https://openai.github.io/chatkit-js/)
15. [https://guardrails.openai.com/docs](https://guardrails.openai.com/docs)
16. [http://platform.openai.com/docs/guides/evaluation-getting-started](http://platform.openai.com/docs/guides/evaluation-getting-started)
17. [https://openai.com/index/codex-now-generally-available](https://openai.com/index/codex-now-generally-available)
18. [https://developers.openai.com/codex/sdk](https://developers.openai.com/codex/sdk)
19. [https://platform.openai.com/settings/organization/service-health](https://platform.openai.com/settings/organization/service-health)
20. [https://github.com/orgs/openai/repositories?q=apps-sdk+OR+chatkit+OR+guardrails](https://github.com/orgs/openai/repositories?q=apps-sdk+OR+chatkit+OR+guardrails)

*... 그 외 444개 링크*

---

*이 문서는 Gemini 2.5 Flash를 사용하여 자동 번역되었습니다.*
