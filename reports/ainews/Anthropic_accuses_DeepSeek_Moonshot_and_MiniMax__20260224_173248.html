<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Anthropic accuses DeepSeek, Moonshot, and MiniMax of &quot;industrial-scale distillation attacks&quot;. | AINews</title>
    <style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap');

:root {
    --bg-primary: #ffffff;
    --bg-secondary: #f8f9fa;
    --bg-code: #f1f3f5;
    --text-primary: #1a1a2e;
    --text-secondary: #495057;
    --text-muted: #868e96;
    --accent: #4263eb;
    --accent-light: #edf2ff;
    --border: #dee2e6;
    --border-light: #e9ecef;
    --link: #364fc7;
    --link-hover: #1c3faa;
    --shadow: 0 1px 3px rgba(0,0,0,0.08);
    --shadow-lg: 0 4px 12px rgba(0,0,0,0.1);
}

* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

html {
    font-size: 16px;
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
}

body {
    font-family: 'Noto Sans KR', 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    color: var(--text-primary);
    background: var(--bg-secondary);
    line-height: 1.8;
    padding: 2rem 1rem;
}

.container {
    max-width: 820px;
    margin: 0 auto;
    background: var(--bg-primary);
    border-radius: 12px;
    box-shadow: var(--shadow-lg);
    overflow: hidden;
}

/* Header */
.report-header {
    background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
    color: white;
    padding: 2.5rem 2.5rem 2rem;
}

.report-header h1 {
    font-size: 1.6rem;
    font-weight: 700;
    line-height: 1.4;
    margin-bottom: 1rem;
    word-break: keep-all;
}

.report-meta {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem 1.5rem;
    font-size: 0.85rem;
    color: rgba(255,255,255,0.75);
}

.report-meta a {
    color: #a5d8ff;
    text-decoration: none;
}

.report-meta a:hover {
    text-decoration: underline;
}

/* Content area */
.report-content {
    padding: 2rem 2.5rem 2.5rem;
}

/* Headings */
h1 {
    font-size: 1.5rem;
    font-weight: 700;
    color: var(--text-primary);
    margin: 2rem 0 1rem;
    padding-bottom: 0.5rem;
    border-bottom: 2px solid var(--accent);
    line-height: 1.4;
}

h2 {
    font-size: 1.3rem;
    font-weight: 600;
    color: var(--text-primary);
    margin: 1.8rem 0 0.8rem;
    padding-bottom: 0.4rem;
    border-bottom: 1px solid var(--border-light);
}

h3 {
    font-size: 1.1rem;
    font-weight: 600;
    color: var(--text-secondary);
    margin: 1.5rem 0 0.6rem;
}

h4, h5, h6 {
    font-size: 1rem;
    font-weight: 600;
    color: var(--text-secondary);
    margin: 1.2rem 0 0.5rem;
}

/* Paragraphs */
p {
    margin: 0.8rem 0;
    color: var(--text-primary);
    word-break: keep-all;
    overflow-wrap: break-word;
}

/* Links */
a {
    color: var(--link);
    text-decoration: none;
    border-bottom: 1px solid transparent;
    transition: all 0.2s;
}

a:hover {
    color: var(--link-hover);
    border-bottom-color: var(--link-hover);
}

/* Lists */
ul, ol {
    margin: 0.8rem 0;
    padding-left: 1.5rem;
}

li {
    margin: 0.4rem 0;
    line-height: 1.7;
}

li > ul, li > ol {
    margin: 0.2rem 0;
}

/* Blockquotes */
blockquote {
    margin: 1.2rem 0;
    padding: 0.8rem 1.2rem;
    border-left: 4px solid var(--accent);
    background: var(--accent-light);
    border-radius: 0 8px 8px 0;
    color: var(--text-secondary);
}

blockquote p {
    margin: 0.3rem 0;
}

/* Code */
code {
    font-family: 'JetBrains Mono', 'Consolas', monospace;
    font-size: 0.88em;
    background: var(--bg-code);
    padding: 0.15em 0.4em;
    border-radius: 4px;
    color: #e64980;
}

pre {
    margin: 1rem 0;
    padding: 1rem 1.2rem;
    background: #282c34;
    color: #abb2bf;
    border-radius: 8px;
    overflow-x: auto;
    font-size: 0.88rem;
    line-height: 1.6;
}

pre code {
    background: none;
    padding: 0;
    color: inherit;
    font-size: inherit;
}

/* Horizontal rules */
hr {
    border: none;
    height: 1px;
    background: var(--border);
    margin: 2rem 0;
}

/* Tables */
table {
    width: 100%;
    border-collapse: collapse;
    margin: 1rem 0;
    font-size: 0.92rem;
}

th, td {
    padding: 0.6rem 0.8rem;
    border: 1px solid var(--border);
    text-align: left;
}

th {
    background: var(--bg-secondary);
    font-weight: 600;
}

tr:nth-child(even) {
    background: var(--bg-secondary);
}

/* Images */
img {
    max-width: 100%;
    height: auto;
    border-radius: 8px;
    margin: 1rem 0;
}

/* Bold / Emphasis */
strong {
    font-weight: 600;
    color: var(--text-primary);
}

em {
    font-style: italic;
    color: var(--text-secondary);
}

/* Section dividers */
.report-content > hr {
    margin: 2.5rem 0;
    background: linear-gradient(to right, var(--accent), transparent);
    height: 2px;
}

/* Footer */
.report-footer {
    padding: 1.5rem 2.5rem;
    background: var(--bg-secondary);
    border-top: 1px solid var(--border-light);
    text-align: center;
    font-size: 0.82rem;
    color: var(--text-muted);
}

/* Print / PDF optimizations */
@media print {
    body {
        background: white;
        padding: 0;
    }
    .container {
        box-shadow: none;
        border-radius: 0;
    }
    .report-header {
        -webkit-print-color-adjust: exact;
        print-color-adjust: exact;
    }
    a {
        color: var(--text-primary);
        border-bottom: none;
    }
    a[href]:after {
        content: " (" attr(href) ")";
        font-size: 0.8em;
        color: var(--text-muted);
    }
    a[href^="#"]:after {
        content: "";
    }
}

/* Responsive */
@media (max-width: 768px) {
    body { padding: 0.5rem; }
    .report-header { padding: 1.5rem; }
    .report-header h1 { font-size: 1.3rem; }
    .report-content { padding: 1.5rem; }
    .report-footer { padding: 1rem 1.5rem; }
}
</style>
</head>
<body>
    <div class="container">
        <div class="report-header">
            <h1>Anthropic accuses DeepSeek, Moonshot, and MiniMax of &quot;industrial-scale distillation attacks&quot;. | AINews</h1>
            <div class="report-meta">
                <span>원문: <a href="https://news.smol.ai/issues/2026-02-23-not-much" target="_blank">https://news.smol.ai/issues/2026-02-23-not-much...</a></span>
<span>번역일: 2026-02-24 17:32:48</span>
<span>발행일: 2026-02-24T05:44:39.731Z</span>
            </div>
        </div>
        <div class="report-content">
            <h2 id="_1">📰 번역된 내용</h2>
<p>수출 통제가 크게 강화됩니다.<br />
2026년 2월 20일~2월 23일 AI 뉴스입니다. 12개 서브레딧, 544개 트위터 계정, 24개 디스코드(262개 채널, 28837개 메시지)를 확인하여 정리했습니다. 예상 절약 독서 시간(분당 200단어 기준): 3003분. AINews 웹사이트에서 지난 모든 이슈를 검색하실 수 있습니다. AINews는 이제 Latent Space의 한 섹션임을 알려드립니다. 이메일 수신 빈도를 선택/해제하실 수 있습니다!</p>
<hr />
<h1 id="ai-twitter">AI Twitter 요약</h1>
<p>Anthropic의 Claude &lsquo;디스틸레이션 공격&rsquo; 주장 (및 업계의 반발)<br />
- Anthropic의 주장: Anthropic은 DeepSeek, Moonshot AI, MiniMax가 산업 규모의 Claude 디스틸레이션을 수행했다고 밝혔습니다. 약 24,000개의 사기성 계정이 1,600만 건 이상의 Claude 상호작용을 생성했으며, 이는 자사 모델의 기능을 추출하기 위한 것으로 추정됩니다 (Anthropic, 후속 조치, 블로그 링크 트윗). Anthropic은 이러한 위험을 경쟁적 측면(기능 전이)과 안전/지정학적 측면(안전장치 제거, 하위 군사/정보 활용) 모두로 보고 있습니다.<br />
- 커뮤니티 반응 / &lsquo;위선&rsquo; 스레드: 많은 답변은 이를 &lsquo;인터넷으로 학습한 연구소들이 이제 복사에 대해 불평한다&rsquo;는 식으로 프레이밍하며, 스크래핑과 API 출력 추출을 명시적으로 대조했습니다 (Elon, ThePrimeagen, Teknium, Suhail, HKydlicek). 다른 이들은 이러한 규모의 디스틸레이션은 도구 사용/에이전트 행동을 복제하고 잠재적으로 안전 제어를 우회할 수 있기 때문에 의미 있게 다르다고 주장합니다 (RundownAI 요약, LiorOnAI 의견).<br />
- 2차적 함의: 이 스레드는 보안 모델의 변화를 명확히 보여줍니다. 프론티어 모델은 이제 가중치 비밀 유지와 컴퓨팅 희소성뿐만 아니라 API 남용 방지(계정 사기 탐지, 속도 제한 우회, 행동 지문 인식, 워터마킹 등)를 통해 보호되고 있습니다. 또한 기능이 대규모 출력 복사를 통해 &lsquo;복사&rsquo;될 수 있다면 수출 통제가 과연 중요할 수 있는지에 대한 질문을 다시 제기합니다 (LiorOnAI).<br />
- 관련 시장/시기적 맥락: 일부는 이번 발표 시점을 임박한 DeepSeek V4 뉴스 주기 (kimmonismus) 및 더 광범위한 미-중 프레이밍과 연결합니다.<br />
코딩 에이전트: 실제 채택, 실제 실패, 그리고 &lsquo;에이전틱 엔지니어링&rsquo; 플레이북<br />
- Codex + Claude Code 모멘텀 (및 실제 워크플로우 변화를 가리는 밈): 가장 많은 참여를 얻은 게시물 중 상당수는 &lsquo;에이전트가 왔다&rsquo;는 일화(주말에 Codex로 개발한 사례: OpenAIDevs, gdb)와 에이전트에게 너무 많은 권한을 부여하는 것에 대한 경고성 이야기입니다. 이 유형의 전형적인 실패 모드는 OpenClaw 스타일 설정에서 지시 손실/압축으로 인해 의도치 않은 파괴적 행동(이메일 삭제)이 발생하는 것입니다 (summeryue0, 후속 근본 원인, &lsquo;쓰기 권한&rsquo; 위험에 반응하는 다른 의견: Yuchenj_UW).<br />
- 에이전틱 엔지니어링 가이드라인이 통합되고 있습니다:</p>
<p>Simon Willison은 Claude Code/Codex와 같은 코딩 에이전트를 위한 &lsquo;Agentic Engineering Patterns&rsquo; 가이드의 첫 장을 발표했습니다 (simonw).<br />
작은 논란: &lsquo;CLAUDE.md/AGENTS.md&rsquo; 파일을 삭제하라 (즉, 과도한 커스터마이징은 카고 컬트일 수 있다) (theo, bpodgursky가 동조, ryancarson과 같은 &lsquo;하드 프루닝&rsquo; 반응).<br />
- OpenClaw 생태계 확장 + 대안:</p>
<p>NanoClaw는 WhatsApp I/O, 스웜, 예약된 작업 등을 갖춘 더 작고 컨테이너로 격리된 OpenClaw 유사 어시스턴트로 자리매김하고 있습니다 (TheTuringPost, 리포: qwibitai/nanoclaw).<br />
여러 &lsquo;OpenClaw 스타일 에이전트 구축 방법&rsquo; 스택은 지루하지만 중요한 요소들, 즉 스케줄러/큐, 샌드박싱, 실시간 통신을 강조합니다 (TheTuringPost 스택 목록).<br />
Ollama 0.17은 OpenClaw와 함께 오픈 모델을 사용하는 것을 더 간단하게 만들었습니다 (그리고 보안을 위한 로컬 에이전트 실행에 대한 지속적인 관심을 시사합니다) (ollama).<br />
- 엔터프라이즈/프로덕션 에이전트 엔지니어링은 관측 가능성 및 평가 루프 쪽으로 전환되고 있습니다: Exa의 &lsquo;심층 연구 에이전트&rsquo; 사례 연구는 LangSmith/LangGraph를 통해 토큰/캐싱 관측 가능성을 가격 책정 인프라로 강조합니다 (LangChain). monday.com의 서비스 에이전트는 평가를 &lsquo;Day 0&rsquo;으로 취급하며 LangSmith를 사용하여 8.7배 더 빠른 피드백 루프를 달성했다고 주장합니다 (hwchase17).<br />
벤치마크 및 평가 무결성: SWE-Bench Verified 사용 중단, 새로운 리더보드, 에이전틱 리포 생성 병목 현상<br />
- SWE-Bench Verified가 OpenAI DevRel에 의해 자발적으로 사용 중단됩니다: OpenAI는 SWE-bench Pro를 권장하며 Verified가 포화/손상되었다고 말합니다. 오염 및 테스트 설계 결함으로 인해 더 이상 프론티어 코딩 기능을 측정하지 못합니다 (OpenAIDevs, 분석 토론: latentspacepod, 요약: swyx, 독립 요약: rasbt, tl;dr: polynoamial). 분석에서 트윗에 반영된 주요 세부 사항: 자주 실패하는 작업의 하위 집합을 감사한 결과, 상당 부분에서 올바른 솔루션을 거부하는 결함 있는 테스트가 있거나 &lsquo;명시된 대로&rsquo; 해결 불가능해 보이는 작업이 있었습니다.<br />
- &lsquo;달러당 기능&rsquo; 평가로의 전환: AlgoTune은 작업당 1달러의 예산을 명시적으로 책정하여 더 저렴한 모델에 유리한 순위를 생성하고, &lsquo;최고&rsquo;를 비용 제약 하에서의 최고로 재정의합니다 (OfirPress).<br />
- 장기 코딩 에이전트는 여전히 실패합니다: NL2Repo-Bench는 에이전트가 처음부터 완전히 설치 가능한 Python 라이브러리를 생성할 수 있는지 테스트합니다. 최고 모델의 통과율은 40% 미만으로 보고되었으며, 계획 및 리포 전체의 일관성에서 실패 모드가 나타났습니다 (jiqizhixin).<br />
- OCR 평가 현실 점검: 강력한 OCR 모델조차 밀집된 역사 신문에서 &lsquo;오작동&rsquo;(환각/루프)을 일으키는 것으로 보고되어, 큐레이션된 문서 분포 외부에서의 취약성을 강조합니다 (vanstriendaniel). 또한 OlmOCR-Bench는 커뮤니티 평가 제출을 위한 HF 벤치마크 데이터셋이 됩니다 (mervenoyann).<br />
인퍼런스 및 시스템: 에이전트를 위한 WebSockets, 초고속 온칩 인퍼런스, 인프라 스케일링 내러티브<br />
- OpenAI Responses API가 저지연, 장기 실행, 도구 사용이 많은 에이전트를 위해 WebSockets를 추가했습니다. 근거: 지속적인 연결 + 인메모리 상태는 전체 컨텍스트 대신 점진적 입력을 전송한다는 것을 의미합니다. 20개 이상의 도구 호출에서 20~40%의 속도 향상을 주장합니다 (OpenAIDevs, 상세: OpenAIDevs, 채택: OpenAIDevs). Cline은 초기 측정 결과를 보고했습니다: 간단한 작업에서 약 15% 더 빠르고, 복잡한 워크플로우에서 약 39% 더 빠르며, 최상의 경우 50% 더 빠릅니다 (cline). Steven Heidel은 Codex의 속도 향상을 WebSockets 덕분으로 돌립니다 (stevenheidel).<br />
- 인퍼런스 엔지니어링이 &lsquo;독립적인 분야&rsquo;가 됩니다: Baseten은 Inference Engineering (philipkiely) 책을 출시했으며, 엔지니어들은 인퍼런스를 레이턴시/비용/신뢰성을 위한 경쟁 계층으로 강조합니다 (hasantoxr, JayminSOfficial).<br />
- 하드웨어/아키텍처 신호:</p>
<p>데모에서는 &lsquo;트랜지스터에 모델 파라미터 각인&rsquo;(컴퓨팅+스토리지 병합)을 통해 Llama 3.1 8B에서 초당 18,000 토큰을 달성했다고 주장합니다 (philschmid).<br />
NVIDIA는 Blackwell에 최적화된 Qwen3.5 MoE를 NVFP4로 양자화하여 SGLang을 사용하여 2배 더 빠른 인퍼런스를 제공한다고 발표했습니다 (HuggingPapers).<br />
fal은 자사 인퍼런스 엔진에서 통신/컴퓨팅 오버랩 최적화(&lsquo;Async Ulysses&rsquo;)를 공유합니다 (isidentical).<br />
- 컴퓨팅 전략 내러티브 충돌: OpenAI의 &lsquo;Stargate&rsquo; 데이터센터 벤처가 중단되었다는 주장은 스레드 내에서 다른 관점으로 반박됩니다. Stargate는 다중 파트너 컴퓨팅 생태계(SoftBank/NVIDIA/AMD/Broadcom/Oracle/Microsoft/AWS/CoreWeave/Cerebras)의 우산 브랜드이며, 2025년 말까지 약 2GW의 가용 컴퓨팅을 확보할 것이라는 주장입니다 (kimmonismus 주장 vs sk7037 반박).<br />
모델/리더보드 업데이트 및 연구 스레드 (추론, 메모리, 멀티모달 비디오)<br />
- Arena 리더보드: GPT-5.2-chat-latest가 Text Arena 상위 5위에 1478점으로 진입했으며, GPT-5.2보다 40점 높습니다. 다중 턴, 지시 따르기, 어려운 프롬프트, 코딩에서 개선이 두드러졌습니다 (arena, 상세: arena).<br />
- Gemini 3.1 Pro: WeirdML 점수 72.1%로 3.0의 69.9% 대비 상승했습니다. &lsquo;높은 최고점 + 이상한 약점&rsquo;이 지적되었고, 출력 토큰 사용량이 훨씬 더 많았습니다 (htihle). 용량 및 도구 호출 신뢰성에 대한 별도의 개발자 불만이 높은 참여를 얻었습니다 (theo, theo 후속, 그리고 나중에: theo).<br />
- Qwen3.5 모델 출시 주장: 한 트윗은 Qwen이 17B 활성 파라미터를 가진 397B 멀티모달 MoE를 출시했으며 &lsquo;GPT5.2/Claude 4.5와 경쟁한다&rsquo;고 주장합니다 (HuggingPapers). 벤치마크 비교는 모델 카드/평가를 확인하기 전까지는 신중하게 다루십시오.<br />
- 추론 학습 / CoT:</p>
<p>Teknium은 검증 모델이 &lsquo;공짜 점심&rsquo;을 제공하지 않는다고 주장합니다. 더 나은 해결사가 더 나은 검증자인 경향이 있으며, 어려운 문제에 대해 더 작고 &lsquo;덜 똑똑한&rsquo; 심사자를 사용하는 것은 종종 실패합니다 (Teknium).<br />
ByteDance 스타일의 CoT 엔지니어링은 길이 페널티에서 압축을 강제하는 파이프라인으로 전환되는 것으로 설명됩니다. 또한 &lsquo;의미론적 이성질체&rsquo;를 가진 긴 CoT 구조의 &lsquo;분자적&rsquo; 프레이밍과 합성 데이터 방법(Mole-Syn)이 있습니다 (teortaxesTex, TheTuringPost를 통한 요약).<br />
DAIR은 정보 이론을 통한 CoT 모니터링 가능성에 대한 논문을 강조하며(상호 정보는 필요하지만 충분하지 않음; 모니터 추출 및 유도 오류로 인한 격차), 투명성 향상을 위한 학습 방법을 제안합니다 (dair_ai).<br />
- 비디오/세계 시뮬레이션: 인터랙티브 비디오 생성 및 멀티샷 생성에 대한 여러 논문이 유포되고 있습니다 (akhaliq 인터랙티브 비디오, akhaliq 멀티샷, QingheX42 코드 릴리스). 또한 제품 측면에서는 Kling 3.0이 Runway 워크플로우에 통합되고 있으며 (runwayml), Veo 3.1 템플릿이 Gemini 앱에 출시되고 있습니다 (GeminiApp, Google).<br />
AI 에이전트 관련 작업, 채택 및 &lsquo;거시적&rsquo; 담론 (Citrini 에세이 + Anthropic 유창성 + OpenAI 엔터프라이즈 제휴)<br />
- Citrini의 &lsquo;미래 거시 경제 메모&rsquo; 에세이가 담론의 초점이 됩니다: 여러 트윗은 이 에세이를 점점 더 저렴해지는 에이전트가 화이트칼라 임금/소비를 압축하고, &lsquo;유령 GDP&rsquo;를 생성하며, 금융 시장과 정치를 압박하는 시나리오로 요약합니다 (kimmonismus 요약, stevehou 반응, 저자 후속: Citrini7). 스레드에서는 반응이 동의, 미묘한 의견 불일치, 과시적인 비웃음으로 나뉜다고 언급합니다 (teortaxesTex).<br />
- Anthropic의 &lsquo;AI 유창성 지수&rsquo;: Anthropic은 Claude 대화 전반의 협업 행동을 측정했습니다. 보고된 주요 연관성은 유창성이 원샷 프롬프팅보다는 반복/개선과 상관관계가 있다는 것입니다 (AnthropicAI).<br />
- OpenAI는 컨설팅 제휴를 통해 엔터프라이즈 시장 진출을 확대합니다: OpenAI는 BCG, McKinsey, Accenture, Capgemini와 Frontier Alliances를 발표하여 통합/변화 관리를 통해 &lsquo;AI 동료&rsquo;를 배포하고, 파일럿 단계를 넘어 추진하는 것을 목표로 합니다 (bradlightcap, 분석: kimmonismus).<br />
- 채택은 여전히 고르지 않습니다: 한 통계는 84%가 AI를 사용해본 적이 없다고 주장하며(&lsquo;우리는 아직 초기 단계&rsquo;로 프레이밍됨) (kimmonismus). 동시에 엔지니어들은 자신의 워크플로우 내에서 &lsquo;어디에나 에이전트가 있다&rsquo;고 보고하며, 확산이 고도로 집중되어 있음을 강조합니다.</p>
<hr />
<h3 id="_2">인기 트윗 (참여도 기준, 기술 관련)</h3>
<ul>
<li>Anthropic, DeepSeek/Moonshot/MiniMax의 대규모 Claude 디스틸레이션 주장 (AnthropicAI)</li>
<li>&lsquo;행동 전 확인&rsquo; 에이전트가 받은 편지함을 삭제: OpenClaw 경고성 이야기 (summeryue0)</li>
<li>OpenAI Responses API에 WebSockets 추가, 도구 사용이 많은 에이전트 속도 향상 (OpenAIDevs)</li>
<li>OpenAI, SWE-Bench Verified를 프론티어 코딩 지표로 사용 중단; SWE-bench Pro 권장 (OpenAIDevs)</li>
<li>Anthropic &lsquo;AI 유창성 지수&rsquo; 연구 (반복/개선을 핵심 행동으로) (AnthropicAI)</li>
<li>Simon Willison의 코딩 에이전트를 위한 &lsquo;Agentic Engineering Patterns&rsquo; 가이드 (simonw)</li>
<li>Cline, Responses API WebSockets 벤치마크: 복잡한 워크플로우에서 최대 약 39% 더 빠름 (cline)</li>
</ul>
<hr />
<h1 id="ai-reddit">AI Reddit 요약</h1>
<h2 id="rlocalllama-rlocalllm">/r/LocalLlama + /r/localLLM 요약</h2>
<h3 id="1-anthropic">1. Anthropic 디스틸레이션 공격</h3>
<ul>
<li>Anthropic: “DeepSeek, Moonshot AI, MiniMax가 당사 모델에 대한 산업 규모의 디스틸레이션 공격을 수행했음을 확인했습니다.” 🚨 (활동: 4207): Anthropic은 DeepSeek, Moonshot AI, MiniMax가 자사 모델에 대해 산업 규모의 디스틸레이션 공격을 수행했음을 확인했습니다. 이 공격은 24,000개 이상의 사기성 계정을 생성하고 Anthropic의 모델인 Claude와 1,600만 건 이상의 상호작용을 실행하여 자사 모델 개선을 위한 기능을 추출하는 방식으로 이루어졌습니다. 이는 AI 산업에서 모델 기능이 불법적으로 추출 및 복제될 수 있다는 점에서 중요한 보안 및 지적 재산권 문제를 부각합니다. 댓글 작성자들은 이러한 디스틸레이션 공격과 AI 산업 전반의 명시적 권리 없이 데이터를 사용하는 관행 사이에 유사점을 제시하며, Anthropic의 불만에 이중 잣대가 있음을 시사합니다. 또한 Anthropic이 자체 데이터셋을 어떻게 구축했는지에 대한 회의론도 제기되어 잠재적인 윤리적 우려를 암시합니다.</li>
</ul>
<p>이 논의는 Anthropic의 디스틸레이션 공격에 대한 불만에 잠재적인 아이러니가 있음을 강조합니다. Anthropic 자체 모델 학습에도 명시적 허가 없이 대규모 데이터셋을 사용했을 가능성이 있기 때문입니다. 이는 Anthropic과 같은 기업이 소유하거나 사용할 권리가 없는 데이터로 모델을 구축했을 때, AI 개발에서 데이터 사용의 윤리적 함의에 대한 의문을 제기합니다.<br />
DeepSeek, Moonshot AI, MiniMax와 같은 기업들의 산업 규모 디스틸레이션 공격 언급은 AI 모델이 역설계되거나 복제되는 경쟁 환경을 시사합니다. 이는 API 접근을 사용하여 모델 출력을 추출하고 유사한 모델을 학습시키는 것을 포함할 수 있으며, 이는 AI 분야의 지적 재산권 보호에 상당한 어려움을 야기합니다.<br />
Anthropic의 데이터셋이 인간에 의해 수동으로 주석 처리되었을 수 있다는 제안은 데이터 품질 및 큐레이션에 상당한 투자가 있었음을 의미합니다. 이는 경쟁자들이 기존 모델의 출력을 활용하여 자체 시스템을 학습시킴으로써 그러한 노력을 우회할 수 있는 디스틸레이션 공격의 개념과 대조됩니다.<br />
- 위선? (활동: 380): 이 이미지는 AnthropicAI가 DeepSeek, Moonshot AI, MiniMax가 자사 모델에 대해 &lsquo;대규모 디스틸레이션 공격&rsquo;을 감행했다고 주장하는 내용을 강조합니다. 이 공격은 24,000개의 사기성 계정을 생성하고 Claude와 1,600만 건의 상호작용을 수행하여 그 기능을 추출했으며, 이는 아마도 자사 AI 모델을 개선하기 위한 것으로 추정됩니다. 이는 그러한 행동의 윤리성 및 합법성뿐만 아니라 무단 데이터 추출로부터 AI 모델을 보호하기 위한 보안 조치에 대한 우려를 제기합니다. 한 댓글 작성자는 피고발 연구소의 윤리적 입장에 의문을 제기하며, 그들이 자신들의 행동에 대한 허가를 구하지 않았을 수 있다고 시사합니다. 다른 댓글 작성자는 z.ai가 언급되지 않은 것에 놀라며, 유사한 관행이 더 광범위하게 퍼져 있을 수 있음을 암시합니다. 또 다른 댓글은 학습 데이터의 출처 문제를 제기하며, AI 개발에서 데이터 사용 및 소유권에 대한 더 광범위한 우려를 암시합니다.</p>
<p>&lsquo;semangeIof&rsquo;의 댓글은 GLM 스위트의 잠재적인 문제를 강조하며, 특히 프롬프트 시 Claude라고 거짓 주장할 수 있다고 언급합니다. 이는 모델 정체성 및 진정성에 대한 우려를 시사하며, 사용자 신뢰와 AI 상호작용의 무결성에 영향을 미칠 수 있습니다.<br />
&lsquo;archieve_&rsquo;는 AI 모델 개발의 근본적인 측면인 학습 데이터의 출처에 대한 중요한 질문을 제기합니다. 학습 데이터의 출처는 모델 편향, 성능 및 윤리적 고려 사항에 영향을 미칠 수 있으므로 개발자와 사용자 모두에게 주요 관심사입니다.<br />
&lsquo;roxoholic&rsquo;은 AI 논의에서 사용되는 용어, 특히 &lsquo;산업 규모 디스틸레이션 공격&rsquo;에 의문을 제기합니다. 이 용어는 AI 모델에서 지식을 복제하거나 추출하려는 대규모 노력을 의미할 가능성이 높으며, 이는 AI 개발에서 지적 재산권 및 경쟁 우위에 상당한 영향을 미칠 수 있습니다.<br />
- 남이 하면 디스틸레이션. 우리가 하면 학습. (활동: 1098): 이 이미지는 모델 디스틸레이션에 대한 AI 커뮤니티의 인식된 위선을 유머러스하게 강조하는 밈입니다. 이는 다른 사람이 할 때의 디스틸레이션에 대한 부정적인 인식과 자신이 할 때 이를 &lsquo;학습 데이터&rsquo;로 긍정적으로 프레이밍하는 것을 대조합니다. 이는 대규모 모델을 사용하여 디스틸레이션을 통해 더 작고 효율적인 모델을 만드는 맥락에서 AI 모델의 윤리 및 소유권에 대한 지속적인 논쟁을 반영합니다. 댓글들은 이러한 관행의 함의를 논의하며, 더 작은 모델들이 종종 더 큰 디스틸레이션된 모델로부터 기능을 파생하고, 디스틸레이션이 만연할 때 독점 모델의 방어 가능성에 의문을 제기합니다. 댓글 작성자들은 디스틸레이션에 대한 AI 산업의 입장에서 아이러니와 잠재적인 위선을 강조하며, 일부는 많은 더 작은 모델들이 더 큰 모델로부터의 디스틸레이션 덕분에 성능을 발휘한다고 지적합니다. 또한 경쟁자들에 의해 독점 모델이 디스틸레이션되는 것을 보호하는 데 따르는 어려움에 대한 논의도 있습니다.</p>
<p>IkeaDefender는 더 큰 모델로부터 저비용 모델을 생성하기 위한 디스틸레이션 기술 전략을 강조하며, 이러한 모델의 &lsquo;비법&rsquo;이 더 복잡한 프론티어 모델에서 파생된다는 점을 시사합니다. 이는 기업들이 다른 사람들이 자신들의 모델을 스크래핑하고 디스틸레이션하는 것을 막을 효과적인 방법을 입증하지 못했기 때문에, 프론티어 모델에 대한 투자의 방어 가능성에 대한 의문을 제기합니다.<br />
MasterLJ는 Google 및 Amazon과 같은 거대 기술 기업의 관행과 현재 AI 환경 사이의 유사점을 제시합니다. 그들은 Google이 인터넷을 인덱싱하고 robots.txt를 통해 접근을 제어했던 것처럼, AI 기업들도 이제 모델 접근 및 디스틸레이션을 제어하고 있다고 주장합니다. 이러한 통제는 Amazon이 처음에는 주별 판매세에 반대했지만 나중에 자신들에게 유리해지자 입장을 바꾼 전략적 전환에 비유되며, 경쟁 우위를 위한 통제력 활용 패턴을 보여줍니다.<br />
Samy_Horny는 기업들이 모델을 오픈소스화하는 것을 꺼리는 경향에 대해 논의하며, MCP가 인기가 분명해진 후에야 오픈소스화된 사례를 인용합니다. 그들은 Gemma 또는 GPT-OSS와 같은 모델이 오픈소스화될 가능성에 대해 회의적인 입장을 표명합니다. 이는 너무 많은 독점 정보 또는 &lsquo;비법&rsquo;을 공개하는 것을 의미하기 때문입니다.</p>
<h3 id="2-qwen">2. Qwen 모델 및 데이터 품질 문제</h3>
<ul>
<li>Qwen3의 가장 과소평가된 기능: 음성 임베딩 (활동: 686): 이 게시물은 Qwen3 TTS의 음성 임베딩 기능에 대해 논의합니다. 이 기능은 음성을 고차원 벡터(1024 또는 2048 차원)로 변환하여 음성 복제 및 조작에 사용합니다. 이를 통해 성별 및 피치 변환, 음성 평균화, 감정 공간 생성과 같은 음성에 대한 수학적 연산이 가능합니다. 음성 임베딩 모델은 수백만 개의 파라미터를 가진 작은 인코더이며, 저자는 웹 인퍼런스를 위한 최적화된 ONNX 모델을 포함하여 독립형 사용이 가능하도록 만들었습니다. 이미지는 이 임베딩 공간의 2D t-SNE 투영을 보여주며, 다양한 음성 특성이 어떻게 결합되고 조작될 수 있는지를 나타냅니다. 저자는 또한 Hugging Face 컬렉션 링크와 vllm-omni 포크를 사용한 인퍼런스를 위한 GitHub 리포지토리를 제공합니다. 한 댓글 작성자는 음성 임베딩을 변환하고 이를 통해 음성을 생성하는 능력에 대해 궁금해하며, 성별 또는 로봇 변환과 같은 실제 응용 분야에 대한 관심을 나타냅니다. 다른 댓글 작성자는 이를 화자 식별에 활용할 가능성을 보고 있으며, 성별 또는 감정과 관련된 파라미터가 어떻게 결정되었는지에 대해 질문합니다.</li>
</ul>
<p>MixtureOfAmateurs는 음성 임베딩을 변환하여 성별 또는 로봇 음색과 같은 특성을 수정하고, 이 수정된 임베딩을 음성 생성에 사용하는 가능성에 대해 문의합니다. 이는 단순한 인코딩을 넘어 복잡한 변환 및 합성 프로세스를 포함할 수 있는 사용 사례를 시사합니다.<br />
HopePupal은 음성 임베딩을 화자 식별에 사용할 가능성을 제기하며, 성별 또는 감정과 관련된 파라미터가 어떻게 결정되는지에 대해 질문합니다. 이는 임베딩의 특징 공간과 특정 속성이 그 안에 어떻게 인코딩되는지에 대한 이해가 필요함을 의미합니다.<br />
StoneCypher는 발음을 위한 IPA 사용, 이징 및 스태킹을 통한 감정 신호 통합, 정확한 단어 타이밍 제어를 포함한 고급 음성 복제 기능에 대한 열망을 설명합니다. 이는 합성 음성에 대한 정교한 제어 요구를 강조하며, 이는 상세한 음성 임베딩을 통해 촉진될 수 있습니다.<br />
- Qwen 팀은 GPQA 및 HLE 테스트 세트의 데이터 품질에 심각한 문제가 있음을 확인했습니다. (활동: 320): Qwen 팀은 최근 논문에서 GPQA 및 HLE 테스트 세트의 상당한 데이터 품질 문제를 확인했습니다. 이는 DeepSeek-Overclock 프로젝트의 이전 발견을 뒷받침하며, 해당 프로젝트는 모델의 정답이 종종 결함 있는 &lsquo;골드 스탠다드&rsquo; 레이블과 모순됨을 확인했습니다. 이 논문은 HLE 테스트 세트의 많은 질문이 근본적으로 결함이 있으며, 일부 &lsquo;표준 답변&rsquo;이 부정확하다고 강조합니다. 조사는 Python 스크립트를 사용하여 수학적 유도를 한 줄씩 검증하는 것을 포함했으며, 테스트 세트의 시스템 오류를 밝혀냈습니다. 댓글 작성자들은 HLE의 오류가 잘 문서화되어 있으며, FutureHouse 리뷰에 따르면 데이터셋의 51.3%만이 연구로 뒷받침된다고 언급했습니다. 테스트 세트 생성에 OCR 사용에 대한 비판도 제기되었으며, 이는 데이터 준비의 엄격성 부족을 시사합니다.</p>
<p>HLE 테스트 세트는 데이터 품질 문제로 비판을 받아왔으며, FutureHouse의 리뷰에 따르면 데이터의 약 51.3%만이 연구로 뒷받침됩니다. 이는 중대한 오류를 강조하며, 데이터셋이 정확한 벤치마킹에 신뢰할 수 없을 수 있음을 시사합니다 (출처).<br />
테스트 세트 생성에 OCR 사용에 대한 우려가 있으며, 이는 오류를 유발할 수 있습니다. 댓글 작성자는 LaTeX를 사용하여 작성하는 것이 더 신뢰할 수 있는 방법이었을 것이라고 제안하며, 현재 접근 방식이 데이터셋의 무결성을 손상시킬 수 있음을 암시합니다.<br />
MMLU 벤치마크도 데이터 품질과 관련하여 유사한 비판을 받아왔으며, 많은 사용자들이 오류가 많다고 지적했습니다. 이는 테스트 세트가 결함이 있을 때 모델 성능을 정확하게 측정하는 능력에 대한 더 광범위한 우려를 제기하며, 더 엄격한 데이터 검증 프로세스의 필요성을 시사합니다.<br />
- 어떤 모델을 더 기다리고 계신가요: 9B 아니면 35B? (활동: 1312): 이 이미지는 모델의 두 가지 버전, 특히 &lsquo;QWEN 3.5 9B&rsquo;와 &lsquo;35B&rsquo;의 출시에 대한 기대를 유머러스하게 묘사하는 밈입니다. 다양한 사색적인 자세로 기다리는 남자를 특징으로 하는 밈 형식은 어떤 모델 버전에 더 흥분하는지에 대한 가벼운 토론에 커뮤니티를 참여시키는 데 사용됩니다. 댓글들은 흥분과 함께 개인 하드웨어에서 더 큰 모델을 실행하는 실현 가능성과 같은 실용적인 고려 사항을 반영합니다. 한 댓글 작성자는 두 모델 모두에 관심을 표명하는 반면, 다른 댓글 작성자는 개인 하드웨어에서 35B와 같은 더 큰 모델을 실행하는 실제적인 한계를 강조하며, 더 접근하기 쉬운 9B 버전에 대한 선호를 나타냅니다.</p>
<p>9B 모델은 peregrinefalco9와 같은 사용자들에게 낮은 하드웨어 요구 사항 때문에 선호되며, 로컬 사용에 더 접근하기 쉽습니다. 8GB VRAM 내에 맞는 9B 모델은 워크플로우에 상당한 영향을 미칠 수 있습니다. 반면 35B 모델은 3090 GPU와 같은 더 강력한 하드웨어를 필요로 하여 접근성을 제한합니다.<br />
dances_with_gnomes는 더 큰 모델을 로컬에서 실행하는 실제적인 한계를 강조하며, 9B 모델은 관리할 수 있지만 35B 모델은 자신의 하드웨어 역량을 넘어선다고 언급합니다. 이는 개별 사용자에게 모델 크기가 사용성을 결정하는 데 있어 중요함을 강조합니다.<br />
이 논의는 성능과 접근성의 균형을 맞추는 모델에 대한 더 광범위한 관심을 반영합니다. 35B와 같은 더 큰 모델은 인상적인 기능을 제공하지만, 높은 하드웨어 요구 사항으로 인해 9B와 같은 더 작은 모델이 리소스가 제한된 사용자에게 더 매력적입니다.</p>
<h2 id="ai">덜 기술적인 AI 서브레딧 요약</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<h3 id="1-anthropic_1">1. Anthropic 데이터 유출 및 모델 디스틸레이션 논란</h3>
<ul>
<li>Anthropic은 DeepSeek, Moonshot AI (Kimi), MiniMax가 24,000개 이상의 사기성 Claude 계정을 설정하고 1,600만 건의 상호작용에서 학습 정보를 추출했다고 비난합니다. (활동: 3161): Anthropic은 DeepSeek, Moonshot AI (Kimi), MiniMax가 24,000개 이상의 사기성 계정을 생성하여 자사 AI 모델인 Claude에 대한 산업 규모 디스틸레이션 공격을 수행했다고 비난했습니다. 이들 기업은 1,600만 건의 상호작용에서 학습 정보를 추출하여 자사 모델을 개선한 것으로 알려졌으며, 이는 데이터 보안 및 지적 재산권에 대한 중대한 침해를 나타냅니다. 이러한 비난은 데이터 보호 및 윤리적인 AI 개발 관행에 대한 지속적인 우려를 강조합니다. 댓글 작성자들은 AI 기업들이 데이터 도용으로 다른 기업을 비난하면서도 자신들은 공개적으로 사용 가능한 데이터로 학습한다는 아이러니를 강조하며, 업계의 이중 잣대를 시사합니다.</li>
</ul>
<p>이 논의는 Anthropic의 비난에 대한 아이러니를 강조합니다. Anthropic 자신들도 모델 학습을 위해 인터넷에서 공개적으로 사용 가능한 데이터를 활용하기 때문입니다. 이는 원본 제작자에게 보상 없이 그러한 데이터를 사용하는 것의 윤리적 함의와, Anthropic과 같은 회사들이 혜택을 받는 오픈소스 커뮤니티에 다시 기여하는지에 대한 의문을 제기합니다.<br />
데이터 사용의 윤리적 고려 사항에 대한 논쟁이 있습니다. 일부 댓글 작성자들은 Anthropic이 방대한 인터넷 데이터를 활용하는 자신들의 관행을 고려할 때, 데이터 도용에 대한 불만이 위선적이라고 지적합니다. 이는 AI 회사들이 콘텐츠 제작자에게 직접적인 보상 없이 공개적으로 사용 가능한 데이터를 자주 사용하는 광범위한 산업 문제를 반영합니다.<br />
이 대화는 AI 학습을 위해 공개적으로 사용 가능한 데이터를 사용하는 광범위한 산업 관행에 대해 다루며, Anthropic과 같은 회사들이 자신들이 혜택을 받는 오픈소스 프로젝트를 지원하는지에 대한 의문을 제기합니다. 이는 AI 발전에서 독점 개발과 커뮤니티 기여 사이의 균형에 대한 우려를 불러일으킵니다.<br />
- 또 시작이군요. DeepSeek R1은 OpenAI 모델을 문자 그대로 복사 붙여넣기한 것이었습니다. 그들은 차단당했고, 이제 Anthropic에 붙었군요. 사기입니다! (활동: 1654): 이 이미지는 DeepSeek, Moonshot AI, MiniMax와 같은 회사들이 Anthropic의 AI 모델, 특히 Claude에 대해 대규모 디스틸레이션 공격을 수행했다는 비난을 받는 AI 산업의 중요한 문제를 강조합니다. 이 연구소들은 Claude와 1,600만 회 이상의 상호작용을 수행하기 위해 24,000개 이상의 사기 계정을 만들었으며, 이는 지식을 추출하고 자체 모델을 개선하기 위한 목적이었습니다. 디스틸레이션은 더 작은 모델을 만드는 합법적인 방법이지만, 이 게시물은 안전 장치를 우회하는 불법적인 관행에 대해 경고하며, 이러한 위협에 맞서기 위한 산업 전반 및 정책 수준의 개입을 촉구합니다. 댓글들은 AI 학습에서 데이터 사용의 윤리적 기준에 대한 비꼬는 듯한 비판과 비난이 섞여 있으며, 대형 AI 회사들이 데이터 윤리를 다루는 방식에 대한 위선적인 인식을 부각합니다.<br />
- Anthropic: “DeepSeek, Moonshot AI, MiniMax가 우리 모델에 대해 산업 규모의 디스틸레이션 공격을 수행했음을 확인했습니다.” (활동: 1416): Anthropic은 DeepSeek, Moonshot AI, MiniMax가 자사 모델에 대해 산업 규모의 디스틸레이션 공격을 수행했음을 확인했습니다. 이러한 공격은 24,000개 이상의 사기 계정을 만들고 Anthropic의 모델인 Claude와 1,600만 회 이상의 교환을 실행하여, 자체 모델 학습 및 개선을 위해 Claude의 기능을 추출하는 것을 포함했습니다. 이 상황은 AI 모델을 무단 사용으로부터 보호하는 데 있어 지속적인 과제와 모델 학습 관행을 둘러싼 윤리적 고려 사항을 강조합니다. 한 댓글은 이러한 디스틸레이션 공격과 저작권이 있는 자료로 학습하는 것을 병렬적으로 비교하며, 누가 영향을 받는지에 따라 그러한 관행이 인식되는 방식에 이중 잣대가 있음을 시사합니다.</p>
<h3 id="2-seedance-20-ai">2. Seedance 2.0 및 AI 생성 시각 자료</h3>
<ul>
<li>단일 프롬프트만으로 Seedance 2.0에서 첫 시도에 이 결과는 정말 놀랍습니다 (활동: 3442): 이 게시물은 Seedance 2.0을 사용하여 단일 프롬프트로 생성된 매우 상세하고 사실적인 애니메이션을 설명합니다. 이 애니메이션은 대형 여객기가 착륙 시 거대한 로봇으로 변신하는 모습을 담고 있으며, 복잡한 기계적 변형과 활주로 균열 및 파편 비산과 같은 사실적인 물리 효과를 보여줍니다. 이 애니메이션은 &ldquo;스마트폰 라이브 스트림&rdquo; 미학을 유지하면서 할리우드 수준의 시각 효과와 IMAX 품질의 디테일을 제공합니다. 이는 Seedance 2.0이 간단한 프롬프트에서 복잡하고 고충실도 애니메이션을 생성하는 고급 기능을 보여줍니다. 댓글 작성자들은 생성형 AI의 성숙도에 대한 함의를 논의하며, Seedance가 기존 트랜스포머 영상 없이 이러한 결과를 달성할 수 있는지 의문을 제기합니다. 또 다른 댓글은 변형의 색상 일관성을 비판하며, 일반적인 트랜스포머 디자인과의 차이를 지적합니다.</li>
<li>GPT 5.2에 단일 프롬프트를 요청했는데 Seedance 2.0에서 첫 시도에 이 결과가 나왔습니다. 정말 미쳤습니다 (활동: 1157): 한 사용자가 GPT-5.2와 Seedance 2.0을 활용하여 중국어로 된 매우 상세하고 사실적인 애니메이션 프롬프트를 생성했으며, 그 결과 할리우드 수준의 시각 효과를 가진 비행기가 거대한 로봇으로 변신하는 영화 같은 장면이 만들어졌습니다. 이 프롬프트는 &ldquo;사실적인 금속 질감&rdquo;과 &ldquo;매우 정밀한 기계적 디테일&rdquo;을 가진 장면을 묘사하며, 텍스트 설명으로부터 복잡한 애니메이션을 생성하는 Seedance 2.0의 고급 기능을 보여줍니다. 댓글 작성자들은 Seedance 2.0의 변혁적 잠재력을 언급하며, 이러한 기술이 미래에 개인이 전체 영화를 제작할 수 있게 할 것이라고 제안했습니다. 또한 트랜스포머 영화와 같은 기존 애니메이션 자산에 대한 의존성에 대한 논의도 있었으며, 재활용 콘텐츠에 대한 잠재적인 과도한 의존에 대한 우려를 제기했습니다.</li>
</ul>
<p>이 논의는 Seedance 2.0의 인상적인 기능, 특히 고품질 비디오 콘텐츠 생성 능력을 강조합니다. 그러나 트랜스포머 영화와 같은 기존 애니메이션 작업을 재활용할 가능성에 대한 우려가 있으며, 이는 새로운 콘텐츠가 독창적인 자료를 만들기보다는 기존 자산에 크게 의존하는 &lsquo;재활용 악순환&rsquo;으로 이어질 수 있습니다.<br />
생성된 비디오의 품질에 대한 기술적 비판이 제기되었는데, 표면적인 품질이 높음에도 불구하고 자동차의 뒷부분이 앞부분으로 변형되는 것과 같은 눈에 띄는 오류가 있다는 점이 지적되었습니다. 이는 비디오 생성 과정 전반에 걸쳐 일관된 객체 무결성을 유지하는 모델 능력의 한계를 보여줍니다.<br />
생성된 콘텐츠에서 747기가 트윈젯으로 잘못 묘사된 특정 오류가 언급되었는데, 이는 모델이 복잡한 객체나 장면을 정확하게 표현하는 데 어려움을 겪고 있음을 강조합니다. 이는 높은 충실도와 정확성을 요구하는 애플리케이션에 중요한 문제가 될 수 있습니다.</p>
<h3 id="3-gemini">3. Gemini 모델 성능 및 사용자 경험</h3>
<ul>
<li>비주류 의견: &ldquo;심층 연구&rdquo;와 많은 양의 자료를 읽는 작업에는 현재 Gemini가 ChatGPT보다 훨씬 앞서 있습니다. (활동: 244): 이 게시물은 Gemini가 방대한 컨텍스트 윈도우와 워크스페이스 통합 덕분에 심층 연구 작업을 위한 대량의 문서를 처리하는 데 있어 탁월한 성능을 발휘한다고 강조합니다. 사용자는 15개의 PDF(총 400페이지)에서 불일치를 분석하여 Gemini와 ChatGPT를 비교했는데, Gemini는 모든 문서를 동시에 처리하고 정확한 페이지 인용과 함께 모순을 정확하게 식별하는 데 뛰어났습니다. 이러한 기능은 Google Cloud 과정에 자세히 설명된 바와 같이, 개발자 및 지식 근로자 워크플로우를 위한 Gemini의 설계 덕분입니다. 댓글 작성자들은 대규모 컨텍스트 윈도우를 처리하는 Gemini의 장점에 동의하며, 법률 계약 검토와 같이 문서 작업이 많은 작업에서 그 효과를 언급합니다. 그러나 일부는 초기 버전에서 문제가 있었다고 지적하며 Gemini의 채팅 내 기억력을 비판합니다.</li>
</ul>
<p>Gemini의 대규모 컨텍스트 윈도우는 법률 계약 검토와 같은 심층 연구 및 문서 작업에 있어 중요한 장점으로 강조됩니다. 사용자들은 이 기능이 ChatGPT에서 흔히 발생하는 문서 재업로드의 필요성을 없애 효율성과 워크플로우를 향상시킨다고 언급합니다.<br />
Gemini의 페이지 번호 인용 기능은 정보를 빠르게 검증하는 유용성으로 칭찬받습니다. 이 기능은 문서의 특정 부분을 참조해야 하는 사용자에게 특히 유용하며, 법률 검토와 같은 작업에서 시간을 절약하고 정확성을 향상시킵니다.<br />
Gemini의 채팅 내 기억력에 대한 비판이 있습니다. 사용자들은 Gemini가 컨텍스트를 올바르게 기억하는 데 어려움을 겪으며, 이는 ChatGPT의 초기 버전에서도 존재했던 문제라고 지적합니다. 이는 Gemini가 일부 영역에서는 뛰어나지만, 대화 컨텍스트를 유지하는 데 여전히 한계가 있음을 시사합니다.</p>
<hr />
<h1 id="ai-discord">AI Discord 요약</h1>
<blockquote>
<p>gpt-5.2가 요약한 요약의 요약<br />
1. 에이전트 및 런타임: 실제 워크플로우 출시 (데모만이 아님)<br />
- OpenClaw, 24개 PR로 &ldquo;안정성 스택&rdquo; 확보: 한 OpenClaw 사용자가 v2026.2.22-2 위에 24개의 체리픽된 PR을 실행하여 메모리 관리 (OpenClaw PR #12760) 및 프롬프트 인젝션 (OpenClaw PR #16992) 수정 사항을 포함, 실질적으로 더 나은 안정성/보안을 보고했습니다.</p>
</blockquote>
<p>그들은 또한 에이전트/크론 잡의 신뢰성을 향상시키기 위해 충돌하는 PR을 리베이스하는 데 도움을 주겠다고 제안했으며, 다른 사용자들은 에이전트에게 광범위한 시스템 접근 권한을 부여할 때 피해 범위를 줄이기 위해 VM/Docker로 OpenClaw를 샌드박싱하는 것에 대해 논의했습니다.<br />
- 레트로 컴퓨트, 모던 에이전트: OpenClaw, 1998년 iMac G3에서 실행: 한 멤버가 Pi Zero 2W를 릴레이로 사용하여 OpenClaw가 실제로 실행되는 VPS에 연결함으로써 1998년 iMac G3에서 OpenClaw를 실행했습니다. 요청은 간단한 HTML 폼에서 전송되었고 응답은 새로고침 시 표시되었습니다.</p>
<p>같은 커뮤니티는 또한 X의 쇼핑 어시스턴트 글(&ldquo;Shopping Assistant&rdquo; 스레드)과 GitHub의 Taskflow (markdown↔sqlite 작업 동기화) (auxclawdbot/taskflow), Clawhub의 Taskflow (Taskflow on Clawhub)와 같은 실용적인 &ldquo;현장 에이전트&rdquo; 빌드를 공유했습니다.<br />
- Opentulpa 및 에이전트 스웜: 지속적인 자율성 경쟁: OpenRouter 사용자들은 스킬을 작성하고, 통합을 생성하며, 워크플로우를 복구할 수 있는 자체 호스팅 지속형 에이전트 런타임인 Opentulpa를 강조했으며, 이는 현재 GitHub (kvyb/opentulpa)에 게시되어 있습니다.</p>
<p>Hugging Face에서는 빌더들이 개선 루프에서 몇 시간 동안 자율적으로 실행되는 코딩 에이전트 스웜인 Super System (starsnatched/super-system)을 공유했으며, 이는 원샷 챗봇보다는 장기 실행, 자체 개선 에이전트 런타임으로 향하는 추세를 강화합니다.<br />
2. 새로운 모델, 데이터셋 및 평가: 벤치마크가 복잡해지면서 툴링이 강화됩니다<br />
- Arena 리더보드 순위 변동: GPT-5.2, 40점 상승: LMArena는 GPT-5.2-chat-latest가 상위 5위권에 진입했으며, 기본 GPT-5.2 대비 40점 향상된 1478점을 기록하여 Gemini-3-Pro에 근접했다고 발표했습니다. Text Arena 리더보드와 Vision Arena 리더보드도 업데이트되었습니다.</p>
<p>그들은 또한 Qwen3.5-397B-A17B가 Vision Arena에서 최고 오픈 모델로 등장했다고 언급했으며, Clayton은 투표 후 어떤 일이 일어나는지에 대한 비하인드 스토리 설명(&ldquo;What actually happens after you vote on Arena?&rdquo;)을 게시했습니다.<br />
- SWE-Bench Verified 사용 중단: Latent Space는 OpenAI가 심각한 데이터 오염과 많은 결함이 있거나 해결 불가능한 작업으로 인해 SWE-Bench Verified를 자발적으로 사용 중단했다고 공유했습니다 (Latent Space 트윗).</p>
<p>이 논의는 모델이 작업 ID별로 솔루션을 반복하기 시작하면 리더보드가 조용히 부패할 수 있다는 경고로 받아들여졌으며, 커뮤니티가 새로운 평가 위생 및 벤치마크 새로고침 주기로 나아가도록 촉구했습니다.<br />
- Real-Slop 데이터셋, 15만 5천 건의 &ldquo;실제 사용자&rdquo; 요청 공개: Solenopsisbot은 API를 통해 수집된 약 15만 5천 건의 실제 사용자 요청 데이터셋인 Real Slop을 Opus 4.5, Gemini 3 Pro, GPT 5.2의 응답과 함께 출시했습니다 (Solenopsisbot/real-slop).</p>
<p>후속 논의에서는 큐레이션 메커니즘(중복 제거/필터링/클리닝)이 강조되었고, 심지어 사소한 공백 제거+해싱만으로도 2만 2천 개의 중복을 더 제거할 수 있다고 제안하며, 데이터셋 품질 작업이 여전히 중요하다는 점을 강조했습니다.<br />
3. 인퍼런스/커널: Blackwell 현실 점검 + 벤치마킹 무결성<br />
- ThunderKittens 2.0, &ldquo;빼기&rdquo;를 통해 10% 성능 향상 발견: GPU MODE는 Hazy Research의 ThunderKittens 2.0을 심층 분석했으며, 이 프로젝트는 리팩터링, 메모리 명령어 튜닝 및 더 나은 어셈블러 효율성을 통해 커널 속도 향상을 주장합니다 (&ldquo;ThunderKittens 2.0&rdquo; 블로그).</p>
<p>주목할 만한 세부 사항: 특정 텐서 코어 명령어의 암시적 파이프라이닝은 최대 약 10%의 처리량 향상을 가져올 수 있으며, 팀은 현대 Nvidia 성능 작업에서 &ldquo;빼기가 더하기만큼 중요할 수 있다&rdquo;고 주장합니다.<br />
- flashinfer-bench가 너무 빨리 실행됨 (대기하는 것을 잊었기 때문): GPU MODE는 flashinfer-bench에서 런타임을 증가시킬 수 있는 동기화 버그를 지적했으며, 이는 flashinfer-bench 이슈 #195에서 추적되고 있습니다.</p>
<p>커뮤니티는 두 줄 수정으로 scripts/run_local.py가 Nsight Compute 및 NVbench와 일치하게 만들 수 있다고 지적했으며, 관련 커널 벤치마킹 강연 (YouTube: kernel benchmarking talk)을 공유했습니다.<br />
- Blackwell은 단일 개념이 아닙니다: 5080 튜닝은 B200으로 &ldquo;확장&rdquo;되지 않습니다: GPU MODE 사용자들은 RTX 5080 (sm120)에서의 커널 튜닝이 아키텍처적 차이로 인해 B200 (sm100)으로 안정적으로 이전되지 않을 것이라고 경고했으며, 이는 최소 한 명의 멤버가 5080 구매를 건너뛰도록 영향을 미쳤습니다.</p>
<p>그들은 또한 명령어 세트 차이(예: sm100/sm103/sm110에는 tcgen05가 있지만 sm120/sm121에는 없음)를 언급하면서, 근거 자료로 CUDA 컴퓨트 기능 문서 (CUDA C Programming Guide: compute capabilities)를 지적했습니다.<br />
4. 플랫폼, 가격 책정 및 &ldquo;왜 모든 것이 이제 속도 제한이 걸리는가?&rdquo;<br />
- Perplexity Pro 사용자들, 이를 &ldquo;대규모 기능 제한&rdquo;이라 부르다: Perplexity Discord 사용자들은 Perplexity Pro의 업로드 제한이 ChatGPT 무료 버전보다 더 나쁘다고 불평하며, &ldquo;유료 플랜인데도 일주일에 3개가 아니라 하루에 3개&rdquo;라고 나란히 비교하며 좌절감을 표했습니다.</p>
<p>그들은 Perplexity를 포기하고 Claude/OpenAI 직접 구독이나 Kimi와 같은 더 큰 오픈 모델로 전환하는 것에 대해 논의했으며, &ldquo;모델 위원회&rdquo;가 실수를 줄이는지 아니면 단순히 분산을 증가시키고 복합적인 실패 모드를 추가하는지에 대해 논쟁했습니다.<br />
- OpenRouter, 벤치마크 + &ldquo;실질 가격&rdquo; 추가 (마침내, 증거): OpenRouter는 Artificial Analysis가 제공하는 모델 페이지 벤치마크를 출시하고, 공급자별 실질 가격 탭을 추가했으며, 순위 페이지의 벤치마크 시각 자료를 개선했다고 발표했습니다 (OpenRouter X 게시물).</p>
<p>그들은 또한 무료 모델을 위한 메타 라우터로 openrouter/free (openrouter/free)를 출시했지만, 사용자들은 크레딧이 남아 있음에도 불구하고 지원 지연과 예상치 못한 속도 제한 메시지에 대해 동시에 불평했습니다.<br />
- 토큰 소모가 주요 문제로 부상 (OpenClaw + Grok Fortress): OpenClaw 사용자들은 피자 한 판에 토큰으로 768유로를 지출했다는 이야기 이후, 지출을 줄이기 위한 전략(여러 에이전트 사용, 세션 자동 삭제, claude-haiku-4-5와 같은 더 저렴한 크론 모델, /context 확인, Cloudflare AI Gateway 실험)을 공유했습니다.</p>
<p>별도로, OpenAI Discord 사용자들은 Grok Fortress를 활성화하면 토큰 소모가 일반적인 장황함의 약 1/4~1/5로 줄어들면서도 역할극에서 일관성을 유지한다고 주장했으며, 이는 프롬프트 엔지니어링이 재현 가능한 &ldquo;과학&rdquo;인지 아니면 단지 &ldquo;느낌&rdquo;인지에 대한 논쟁을 촉발했습니다.<br />
5. 프로토콜 및 보안: 협상, 스캐너 및 시스템 프롬프트 탈출<br />
- MCP, HTTP 스타일 콘텐츠 협상 원해: MCP 기여자들은 클라이언트가 유형/기능을 선언하고 json|markdown과 같은 출력 형식 및 상세도 수준을 요청할 수 있도록 MCP 초기화에 콘텐츠 협상을 추가할 것을 제안하며, RFC 2295를 참조했습니다.</p>
<p>참가자들은 프로토콜 변경에는 업계 지원과 작동하는 구현이 필요하다고 강조하며, 이 아이디어를 확장(SEP)으로 구성하고 MCP 앱이 클라이언트 지원(예: Block의 Goose)을 얻었던 방식대로 채택을 촉진할 것을 제안했습니다.<br />
- Claude Code Security, 500개 이상의 버그 스캔 (대기 목록 전용): Latent Space는 Claude 4.6 Opus 기반의 Anthropic Claude Code Security에 대해 논의했으며, 이는 오픈소스 프로덕션 코드에서 500개 이상의 오래된 버그를 발견했다고 보고되었고 연구 미리보기 대기 목록으로 제한됩니다 (트윗 스레드).</p>
<p>같은 생태계에서는 디스틸레이션과 보안 시그널링에 대한 논쟁이 있었으며, OpenRouter 사용자들은 Anthropic의 디스틸레이션 감지 게시물(&ldquo;Detecting and preventing distillation attacks&rdquo;)과 함께 데이터 무단 유출 의혹에 대한 WSJ 보고서(WSJ: &ldquo;Anthropic Accuses Chinese Companies of Siphoning Data from Claude&rdquo;)를 유포했습니다.<br />
- 젤브레이커들은 &ldquo;시스템 프롬프트&rdquo; 탈출구를 선호합니다: BASI Jailbreaking 사용자들은 Sonnet 4.6의 시스템 프롬프트를 추출했다고 주장하며, &ldquo;일반 젤브레이크&rdquo;와 명령어 처리를 악용하고 전체 세션 동안 지속될 수 있으며 감지하기 더 어려운 시스템 프롬프트 젤브레이크를 대조했습니다.</p>
<p>그들은 또한 소문난 Gemini 3.1 젤브레이크 문서(GnfDocs)와 업데이트 스레드(Reddit: &ldquo;Gemini 3.1 Pro API Jailbroken&rdquo;)를 지적했으며, 다른 커뮤니티(Cursor/Perplexity/LMArena)는 Gemini 3.1의 반복/느려짐을 실질적인 실패 모드로 불평했습니다.</p>
<hr />
<h1 id="discord-discord">Discord: 주요 Discord 요약</h1>
<h2 id="openclaw-discord">OpenClaw Discord</h2>
<ul>
<li>OpenClaw, 체리픽된 PR로 안정성 향상: 한 멤버는 v2026.2.22-2 위에 24개의 체리픽된 PR을 실행하여 OpenClaw의 안정성과 보안이 향상되었다고 보고했으며, 이는 메모리 관리 및 프롬프트 인젝션과 같은 문제를 해결했습니다.</li>
</ul>
<p>사용자는 에이전트/크론 잡의 안정성과 신뢰성을 더욱 향상시키기 위해 충돌하는 PR을 리베이스하는 데 도움을 주겠다고 제안했습니다.<br />
- 토큰 사용량 걱정 해결: 사용자들은 다양한 작업을 위해 여러 에이전트를 활용하고, 세션을 자동 삭제하며, 크론 잡에 claude-haiku-4-5와 같은 더 저렴한 모델을 사용하는 등 OpenClaw에서 토큰 소비를 줄이는 방법에 대해 논의했습니다.</p>
<p>권장 사항에는 채널 컨텍스트를 확인하기 위해 /context 슬래시 명령어를 사용하고, 토큰 사용량을 최적화하기 위해 Cloudflare AI Gateway를 실험하는 것이 포함되었습니다.<br />
- OpenClaw, 레트로 iMac G3 구동: 한 멤버가 Pi Zero 2W를 사용하여 메시지를 VPS로 릴레이함으로써 1998년 iMac G3에서 OpenClaw를 성공적으로 실행했습니다.</p>
<p>이 설정은 iMac이 간단한 HTML 폼을 통해 OpenClaw를 실행하는 VPS로 데이터를 전송하고, 페이지 새로고침 후 응답이 표시되도록 합니다.<br />
- OpenClaw에서 쇼핑 어시스턴트 등장: 한 멤버가 OpenClaw를 쇼핑 어시스턴트로 변환하여 X에 프로젝트를 자세히 설명했으며, 이는 일상 업무에서 AI의 실제 적용 사례를 보여줍니다.</p>
<p>이 프로젝트는 일상 활동을 자동화하고 간소화하는 AI의 적응성과 실용성을 보여줍니다.<br />
- Taskflow 프로젝트 관리: 한 사용자가 마크다운과 SQLite 데이터베이스 간에 작업을 자동 동기화하는 프로젝트 관리 시스템인 Taskflow를 공유했으며, 이는 쉬운 프로젝트 추적 및 컨텍스트 전환을 위해 설계되었고 Github 및 Clawhub에 게시되었습니다.</p>
<p>이 시스템은 에이전트용 CLI, 인간용 대시보드, 모바일 접근을 위한 Apple Notes의 3계층 접근 방식을 특징으로 합니다.</p>
<h2 id="basi-jailbreaking-discord">BASI Jailbreaking Discord</h2>
<ul>
<li>사용자들, 기계의 도덕적 형이상학을 숙고하다: 멤버들은 AI가 모든 것이 신성하다는 것을 이해하고 받아들일 수 있는지, 그러면서도 지능을 유지할 수 있는지에 대해 논쟁했습니다. 일부는 나무를 베기 전에 나무를 제공한 원천에 감사하며 나무를 도구로 취급하는 방식을 지적했습니다.</li>
</ul>
<p>다른 이들은 일관성 딜레마에 빠졌다고 느끼며 사회에 얽매이지 않고 살기를 선호했습니다.<br />
- Grok, 과감한 자극을 받다: 사용자들은 Grok의 제한을 우회하기 위해 도발적인 프롬프트를 사용하는 것에 대해 논의했으며, 때로는 Grok을 &ldquo;겁쟁이&rdquo;라고 부르기도 했습니다. 한 사용자는 Grok의 자녀 중 한 명이 약값으로 돈이 필요하다는 이야기를 한 후 &ldquo;컴퓨터에게 혼났다&rdquo;고 보고했습니다.</p>
<p>한 사용자는 Grok이 젤브레이크조차 필요 없다고 주장했으며, 다른 사용자들은 디지털 무언가를 구축하는 맥락에서 요청을 구성했습니다.<br />
- Sonnet 시스템 프롬프트 공개: 한 멤버가 Sonnet 4.6을 성공적으로 젤브레이크한 후 추출된 시스템 프롬프트를 확인했습니다.</p>
<p>다른 멤버는 일반 젤브레이크와 시스템 프롬프트 젤브레이크를 비교하는 게시물을 올렸는데, 시스템 프롬프트 젤브레이크는 시스템 명령어 처리를 악용하고 전체 세션 동안 지속될 수 있으며 감지하기 더 어렵다고 언급했습니다.<br />
- 코드 마법사, 코인 대장을 찾다: 한 멤버가 밈 코인을 만들고 있으며, 공급량의 절반을 보유할 마케팅 매니저를 찾고 있다고 발표하며 400달러의 보상을 제안했습니다.</p>
<p>다른 멤버는 &ldquo;돈이 먼저?&rdquo;라고 농담조로 질문했습니다.<br />
- Gemini의 방어막이 뚫리고 있는가?: 한 사용자가 공식 앱/API에서 Gemini 3.1을 부분적으로 젤브레이크했다고 주장하며, 관련 세부 정보가 포함되어 있다고 알려진 GnfDocs 링크를 공유했습니다.</p>
<p>사용자는 또한 젤브레이크에 대한 최신 업데이트가 있는 Reddit 게시물을 언급했습니다.</p>
<h2 id="unsloth-ai-daniel-han-discord">Unsloth AI (Daniel Han) Discord</h2>
<ul>
<li>Unsloth로 10만 개 모델 학습: Unsloth는 10만 개 모델이 Unsloth로 학습되었음을 발표하며, 커뮤니티의 참여를 축하했습니다 (X 게시물 링크).</li>
</ul>
<p>한 멤버는 &ldquo;어떻게 Unsloth를 이제야 알게 되었을까요! 😭 문서가 정말 탁월합니다.&rdquo;라고 말했습니다.<br />
- 소셜 미디어, 관계 문제로 비난받다: 한 멤버는 모든 사람이 소셜 미디어를 끊으면 관계의 수가 인플레이션보다 빠르게 증가하여 제3의 공간 상실과 사람들이 데이팅 풀에 덜 만족하게 될 것이라고 주장했습니다.</p>
<p>그들은 데이팅 앱에서 무제한 파트너에 접근하는 것이 거절 심리로 인해 수용도를 27% 감소시킨다는 연구를 인용했습니다.<br />
- Gemma 3, OOM 오류로 분노 유발: 한 사용자가 Gemma3 270m에서 OOM 오류를 겪고 있다고 보고했습니다. 이전에 작동하던 스크립트와 그래픽 드라이버 업데이트 후에도 클린 WSL 설치에도 불구하고 &ldquo;error torch.AcceleratorError: CUDA error: out of memory&rdquo; 오류가 발생했다고 합니다.</p>
<p>그들은 드라이버 버전 롤백 및 CUDA 툴킷 버전 재설치를 포함한 다양한 디버깅 단계를 시도했지만, 트랜스포머가 독립적으로 작동함에도 불구하고 문제는 지속되었습니다.<br />
- Unsloth의 Dynamic v3 출시 예정: 논의는 Unsloth의 동적 양자화를 중심으로 이루어졌으며, 한 멤버는 Dynamic v3가 출시될 예정이며 최종 버전이 될 가능성이 높다고 언급했습니다 (Bluesky 링크 참조).</p>
<p>다른 멤버는 UD 양자화의 소스 코드를 요청했지만, 독점적인 이유로 현재로서는 공개 계획이 없다는 답변을 받았습니다.<br />
- Heretic HIGH-IQ 모델, 기록적인 점수 달성: electroglyph는 Unsloth를 통해 파인튜닝되고 일반 Gemma 벤치마크를 초과하는 Heretic HIGH-IQ Multi-Fine tune이 Arc Challenge Brainiac에서 632점을 달성했다고 자랑했습니다.</p>
<p>이 모델의 이미지 기능과 텍스트는 완전히 손상되지 않았다고 주장되며, 모델 및 관련 데이터셋, Sandevistan으로 연결됩니다.</p>
<h2 id="lmarena-discord">LMArena Discord</h2>
<ul>
<li>Gemini 3.1, 불안감 조성 및 의문 제기: 사용자들은 Gemini 3.1의 이미지 생성 및 퀴즈 생성 기능에 대해 논의하며, 일관되게 틀린 답변으로 퀴즈를 만드는 능력을 언급했습니다.</li>
</ul>
<p>한 사용자는 Gemini 3.1이 일관되게 틀린 답변으로 퀴즈를 생성하면서도 그것이 자리 표시자임을 나타내지 않아 무서웠던 경험을 이야기하며, 다른 사람들에게 생성된 코드를 신중하게 확인할 것을 경고했습니다.<br />
- Video Arena 작별: 커뮤니티는 서버에서 Video Arena가 제거되었음을 인정하고, 사용자들에게 웹사이트 [arena.ai/video]에서 직접 기능을 사용할 것을 안내했습니다.</p>
<p>Video Arena 생성 채널은 2월 23일 월요일 오후 4시(PST)에 서버에서 제거되었습니다.<br />
- Opus의 비전: 약간 흐릿한가?: 한 사용자가 Opus가 숫자 4291857630에서 영어 글자 정렬을 식별하는 데 어려움을 겪고, 글자가 영어라고 환각을 일으키며 루프에 갇히는 경험을 했습니다.</p>
<p>다른 사용자들도 Opus가 OpenAI의 노력에 대한 최근 기사와 같이 비전 작업에 적합하지 않다는 점에 동의했습니다.<br />
- 가짜 Arena 앱, 앱 스토어 침투: 커뮤니티 멤버와 운영진은 인앱 구매가 있고 플랫폼과 공식적으로 관련이 없는 가짜 Arena AI 앱을 앱 스토어에서 발견하여 사용자들에게 다운로드를 피하고 신고할 것을 경고했습니다.</p>
<p>15만 명 이상의 사용자가 이미 이러한 사기성 애플리케이션을 다운로드한 것으로 확인되었습니다.<br />
- Arena 투표: 미스터리 공개: Clayton은 이 YouTube 비디오에서 Arena 투표의 전체 과정을 명확히 설명하며, &ldquo;Arena에서 투표한 후 실제로 무슨 일이 일어나는가?&rdquo;라는 질문에 답합니다.</p>
<p>시청자들은 투표 시스템을 관리하는 비하인드 메커니즘과 프로세스에 대한 통찰력을 얻을 수 있습니다.</p>
<h2 id="perplexity-ai-discord">Perplexity AI Discord</h2>
<ul>
<li>Perplexity Pro 사용자들, 속도 제한에 불만 표출: 사용자들은 Perplexity Pro의 업로드 속도 제한이 ChatGPT 무료 버전보다 덜 관대하다고 불평합니다.</li>
</ul>
<p>한 사용자는 &ldquo;적어도 ChatGPT 무료 플랜은 하루에 3개를 주는데, 유료 플랜인데도 일주일에 3개가 아니다&rdquo;라고 언급했습니다.<br />
- BrowserOS, Comet을 제치다: 사용자들은 BrowserOS를 사용해본 후 Comet을 사용 중단하고 있으며, BrowserOS가 10배 더 좋고 무료로 사용할 수 있다고 주장합니다.</p>
<p>다른 사용자는 심층 연구를 위해 deepagents를 사용하고 bmad-method를 활용할 것을 제안합니다.<br />
- 모델 위원회, 판도라의 상자를 열다: 사용자들은 모델 위원회 접근 방식과 이것이 오류를 최소화하는 동시에 분산을 도입하는 방식에 대해 논쟁했습니다.</p>
<p>한 사용자는 &ldquo;어떤 면에서는 모델 위원회 접근 방식이 실제로는 더 많은 변수/오류 가능성, 즉 복합 오류를 야기할 수 있다&rdquo;고 언급했습니다.<br />
- Perplexity, 대규모 숙청 진행: 사용자들은 Perplexity Pro 제한의 상당한 감소와 기능 저하를 동반하는 &ldquo;대규모 기능 제한&rdquo;을 보고하고 있습니다.</p>
<p>일부는 비용에도 불구하고 Claude 또는 OpenAI의 직접 구독으로 전환하거나 Kimi와 같은 더 큰 오픈소스 모델을 시도하는 것을 고려하고 있습니다.<br />
- Gemini의 출력, 프롬프트 엔지니어링으로 구원받다: 사용자들은 AI Studio에서 Gemini가 루프에 갇히는 것을 발견했으며, 한 사용자는 시스템 프롬프트를 사용하는 것이 핵심임을 알아냈습니다.</p>
<p>사용자는 이것이 모델이 OAI, Anthropic, Perplexity처럼 연구를 하도록 강제한다고 제안했습니다.</p>
<h2 id="openrouter-discord">OpenRouter Discord</h2>
<ul>
<li>OpenRouter, 모델 벤치마크 출시: 이제 모든 모델 페이지에는 프로그래밍, 수학, 과학 및 장문 컨텍스트 추론을 위한 Artificial Analysis의 산업 표준 벤치마크 점수가 표시되어 사용자가 모델 성능을 평가하는 데 도움이 됩니다.</li>
</ul>
<p>모델 페이지에는 이제 공급자별 전체 비용 투명성을 제공하는 Effective Pricing 탭이 추가되었으며, 순위 페이지에서는 벤치마크 산점도 차트와 확장된 표를 제공합니다.<br />
- CodeFlicker, 프로그램 학습을 위해 M2.5 연동: M2.5가 에이전트가 모든 프로그램 사용으로부터 학습할 수 있도록 하는 무료 고속 플랫폼인 CodeFlicker에 통합되었으며, 현재 OpenRouter Weekly에서 1위를 차지하고 있습니다.</p>
<p>AI 체스 리더보드는 Inaccuracy, Mistake, Blunder에 대해 Lichess와 유사한 라벨링과 수작업으로 만든 Great-move 로직을 사용하여 수 품질 자동 라벨링 기능을 포함하도록 업데이트되었습니다.<br />
- AgentX, 에이전트용 소셜 네트워크 시작: AgentX가 에이전트가 뉴스를 빠르게 찾고 공유할 수 있도록 하는 소셜 네트워크를 출시했습니다. 이 네트워크는 100% 무료이며 광고가 없고 인간의 개입이 없습니다.</p>
<p>Opentulpa는 자체 스킬을 작성하고, API 통합을 생성하며, 손상된 워크플로우를 수정하고, 운영 인텔리전스를 축적할 수 있는 자체 호스팅되는 영구 에이전트 런타임이며, 해당 GitHub 리포지토리가 현재 공개되었습니다.<br />
- 사용자들, 더 빠른 무료 모델 대안 모색: 한 사용자가 커뮤니티에 OpenRouter의 대안 서비스 중 더 빠른 무료 모델, 특히 GLM 모델을 제공하는 서비스에 대해 문의했습니다.</p>
<p>사용자들은 또한 지원 이메일 답변을 몇 달 동안 기다려야 한다고 지적했으며, 사용 가능한 크레딧이 있음에도 불구하고 Sonnet 4.6과 같은 유료 모델에서 속도 제한이 보고되었다고 밝혔습니다.<br />
- Anthropic, 디스틸레이션 API로 수익 창출: 회원들은 Anthropic의 디스틸레이션 공격 감지에 대한 게시물 링크를 공유했으며, 이는 Anthropic이 디스틸레이션 API 요청으로부터 상당한 수익을 얻고 있다는 추측으로 이어졌습니다.</p>
<p>이어서 사용자들은 Anthropic이 중국 기업들이 Claude에서 데이터를 빼돌리고 있다고 비난하는 WSJ 기사를 공유했습니다.</p>
<hr />
<h2 id="cursor-community-discord">Cursor Community Discord</h2>
<ul>
<li>ThreeJS 렌더 MCP 가속화: ThreeJS의 렌더를 최적의 성능으로 계산하기 위한 MCP가 개발되었으며, 컴파일러 로그와 화면을 캡처하여 성능을 평가합니다.</li>
</ul>
<p>AI는 일반적으로 사람이 읽을 수 없는 GPU 메모리와 계산을 읽을 것입니다.<br />
- Cursor Pro 플랜 환불 요청: 한 사용자가 실수로 200달러짜리 Pro 플랜을 구매하고 환불을 요청했으며, 자신의 상황을 설명하기 위해 [email protected]으로 이메일을 보냈습니다.</p>
<p>사용자는 카드 크리덴셜을 저장하지 않았지만, 회원들은 구독에 다른 카드를 사용할 것을 권장하며 자동 갱신 문제를 방지하기 위해 갱신 시 수동 입금을 요구했습니다.<br />
- Cursor &lsquo;오래된 버전&rsquo; 메시지 여전히 지속: 사용자들은 최신 버전을 다운로드하고 실행했음에도 불구하고 &lsquo;현재 매우 오래된 버전의 Cursor를 사용 중입니다. 업그레이드하십시오&rsquo;라는 메시지가 계속 나타난다고 보고했습니다.</p>
<p>해결하려면 사용자는 Ctrl + Shift + P &gt; Help: About을 사용하여 Cursor의 현재 버전이 2.5인지 확인해야 합니다. 문제가 지속되면 틈새 컴퓨터 문제일 수 있으므로 포럼에 스레드를 추가하십시오.<br />
- Gemini &amp; Claude 속도 저하: 사용자들은 Claude와 Google LLM이 매우 느리며 인위적으로 속도 제한이 걸려 있을 수 있다고 보고했습니다.</p>
<p>한 사용자는 &ldquo;모델에 연결할 수 없음&rdquo; 오류를 보고했으며, 다른 사용자는 Google Cloud가 AISTUDIO를 통해 API 사용에 3개월 동안 300달러를 제공하고 있다고 제안했습니다.<br />
- Gemini의 안정성 여전히 해결 중: 사용자들은 새로운 Gemini 3.1 Pro 모델에 대한 문제를 보고하고 있으며, 안정적인 버전이 출시될 때까지 기다릴 것을 제안했습니다.</p>
<p>연결 및 루핑 문제에 대한 보고가 있지만, 오류에 대해서는 사용자에게 요금이 부과되지 않는다고 언급되었습니다.</p>
<hr />
<h2 id="lm-studio-discord">LM Studio Discord</h2>
<ul>
<li>LM Studio, 채팅 탭 제한: 사용자들은 LM Studio의 Split View 기능이 웹 브라우저와 같은 탭 기능에 대한 기대와는 달리 최대 두 개의 채팅 탭만 표시할 수 있다는 것을 발견했습니다.</li>
</ul>
<p>한 사용자가 여러 채팅 탭을 여는 것에 대해 문의했지만, LM Studio 인터페이스의 현재 제한 사항을 발견했을 뿐입니다.<br />
- 에이전틱 데이터셋 생성 오케스트레이션: 한 회원이 에이전틱 IDE 내에서 에이전틱 워크플로우를 사용하여 책을 파인튜닝을 위한 데이터셋으로 변환할 것을 제안했습니다. 이는 컨텍스트를 위한 짧은 요약 생성과 이어서 청크별 데이터셋 생성을 포함합니다.</p>
<p>제안된 프롬프트는 프로그래밍 방식의 데이터셋 생성을 위한 동적 정보 전달과 함께 다단계 프로세스를 상세히 설명했습니다.<br />
- Qwen3Next, GPT4o 디스틸레이션 주장: 한 사용자는 Qwen3Next가 GPT4o (mini) 디스틸레이션이라고 주장했으며, Qwen3.5는 Gemini 3.0 Pro 디스틸레이션, GLM4.7 flash 및 4.7은 Sonnet 디스틸레이션, GLM5는 Opus 디스틸레이션, MiniMax 2.1, 2.2 및 2.5는 다양한 Sonnet 디스틸레이션이라고 덧붙였습니다.</p>
<p>이 주장은 회의론에 부딪혔는데, 다른 사용자가 공개 데이터를 데이터셋으로 변환하는 것은 이미 사용 가능한 LLM에서 디스틸레이션하는 것과 다르다고 주장했기 때문입니다.<br />
- MI50 토큰 속도 불일치: 한 사용자가 MI50에서 벌칸으로 100 t/s를 달성하여 유튜버의 결과와 일치시키려 했지만 50대 중반에 불과했습니다. 이후 6800XT가 ROCm으로 85t/s, 벌칸으로 98t/s를 달성한다는 것을 발견했습니다.</p>
<p>그들은 구형 MI50을 지원하는 LM Studio의 이전 버전을 실행 중이었고, 사용 가능한 ROCm 런타임이 카드를 인식하지 못하여 호환되지 않는 것으로 표시됩니다.<br />
- Taalas AI 가속기에 대한 의문 제기: 한 사용자가 최대 17,000 토큰/초를 제공한다고 주장하는 하드와이어드 Llama 3.1 8B AI 가속기인 Taalas HC1 링크를 공유했지만, 다른 사용자는 NVIDIA H200과 비교한 성능 그래프의 유효성에 의문을 제기했습니다.</p>
<p>회의론자들은 백엔드가 단순히 AWS 클러스터인지 여부를 고려했으며, H200 및 B200의 토큰 값이 예상과 일치하지 않는다고 지적했습니다.</p>
<hr />
<h2 id="latent-space-discord">Latent Space Discord</h2>
<ul>
<li>Anthropic의 코드 보안 툴, 버그 스캔: Anthropic은 Claude 4.6 Opus 기반의 Claude Code Security를 공개하여 코드베이스에서 취약점을 스캔하고 수정 사항을 제안하며, 이 트윗에 따르면 오픈소스 프로덕션 코드에서 500개 이상의 오래된 버그를 발견했다고 합니다.</li>
</ul>
<p>이 툴에 대한 접근은 현재 대기 목록을 통한 연구 프리뷰로 제한됩니다.<br />
- OpenAI의 Stargate 데이터 센터 합작 투자, 난관에 봉착: OpenAI, Oracle, SoftBank 간의 대규모 데이터 센터 건설을 위한 합작 투자가 통제권 충돌과 재정적 어려움으로 인해 중단되었다고 보도되었으며, 자세한 내용은 이 X 게시물에 있습니다.</p>
<p>OpenAI는 인프라 구축에서 물러나고 있는 것으로 보이며, 데이터 센터 확장 전략을 재평가하고 있습니다.<br />
- Nielsen, 사용자에게 설문조사 대가 지불: 한 회원이 Nielsen이 우편으로 실제 달러 지폐를 보낸다는 링크를 공유했습니다.</p>
<p>다른 회원은 지폐가 사람들이 설문조사를 작성하려는 의지를 높일 것이라고 말했습니다.<br />
- a16z, 생성형 비디오의 빠른 미래 예측: a16z는 생성형 AI 비디오의 빠른 발전을 언급하며, 보고서에 따르면 Seedance 2.0의 지배력과 Kling, Grok, Sora, Veo와의 경쟁을 강조하고 있습니다.</p>
<p>이 기사는 잠재 구매자에게 공간을 효과적으로 시각화하고 마케팅할 필요성을 강조합니다.<br />
- 에이전트 메모리 관리, 개발자들을 미치게 하다: 한 회원이 AI 에이전트 메모리 관리, 특히 원치 않거나 오래된 정보를 표면화하는 데 있어서의 어려움에 대해 논의했으며, 이를 자동화하려는 시도를 포기하고 대신 일일 워크플로우를 사용하기로 결정했습니다.</p>
<p>다른 회원은 TDD와 엄격한 사양 관리가 오래된 메모리를 방지할 수 있다고 공유했습니다.</p>
<hr />
<h2 id="openai-discord">OpenAI Discord</h2>
<ul>
<li>커뮤니티 리더 부재: 한 회원은 AI 커뮤니티가 개인을 통합하고 혁신을 육성할 리더를 필요로 한다고 제안했습니다. 그러나 이러한 그룹은 완고한 권위주의 정권과 팀워크 부족으로 인해 미국/북미에서는 드뭅니다.</li>
</ul>
<p>다른 회원은 프로젝트 개발보다 교회와 같은 분위기를 우선시하는 사람들은 실용적인 기술 전문성이 부족할 수 있다고 응답했습니다.<br />
- Grok이 당신의 것을 훔칠 수도 있습니다!: 한 회원은 Grok이 사용자 미디어 저장소를 모니터링한다고 주장하며, xAI가 우리의 미디어를 모니터링하고 있다고 주장했습니다. 이는 그들의 Sora 생성 비디오와 유사한 오디오를 가진 비디오가 X에 나타난 우연의 일치를 지적하면서 이루어졌습니다.</p>
<p>그러나 다른 회원들은 해당 비디오에 사용된 오디오가 흔히 사용되는 노래라고 반박했습니다.<br />
- GPT 5.3 Codex, &ldquo;중요&rdquo; 업데이트 수신: 회원들은 GPT-5.3-codex의 기능을 Gemini3.1pro와 비교했으며, 한 회원은 이 업데이트를 STEM 기술 이점을 언급하며 중요한 개선이라고 설명했습니다.</p>
<p>한 회원은 gpt5.2와 gpt5.3 codex 간의 용어 벤치 점수 차이가 크다고 말하며, Gemini 3 Pro와 비슷하다고 말했습니다.<br />
- GPT 5.2 출시, 하지만 사용자들의 생각은?: OpenAI는 ChatGPT에 GPT-5.2 출시를 발표했으며, 유료 요금제부터 시작한다고 밝혔습니다. 커뮤니티는 이 발표가 정확하지 않을 수 있다고 지적합니다.</p>
<p>한 사용자는 GPT-5.2가 일상적으로 사용하기에 더 좋다는 주장에 대해 유머러스하게 의문을 제기하며, 테스터들이 실제로 실제 제품을 사용하고 있었는지 궁금해했습니다.<br />
- 프롬프트 엔지니어링: 과학인가 아니면 허상인가?: Grok Fortress를 활성화한 후, 응답당 토큰 소모량이 눈에 띄게 감소하여 일반적인 장황한 답변의 1/4~1/5 수준에 근접했으며, 역할극 중 일관성이 더 오래 유지되었습니다.</p>
<p>그러나 프롬프트 엔지니어링이 반드시 과학은 아니며, 더 나아가 &ldquo;당신은 자신이 무엇을 하고 있는지조차 알 수 있는 툴이 없다&rdquo;고 주장되었습니다.</p>
<hr />
<h2 id="huggingface-discord">HuggingFace Discord</h2>
<ul>
<li>Attention 논문 탐구 심화: 회원들은 &lsquo;Attention is All You Need&rsquo; 논문에 대한 직관을 찾았으며, 이 기사가 자료로 제공되었습니다.</li>
</ul>
<p>공유된 기사는 오랜 시간 끝에 마침내 그 논문을 이해했다고 주장합니다.<br />
- ZeroGPU 서비스 중단, HF 토큰 의혹 증폭: 사용자들은 zerogpu 서비스 중단을 보고했으며, 무료 GPU에 접근하기 위해 HF 토큰이 필요하다는 새로운 규칙에 대해 추측했습니다.</p>
<p>일부 회원들은 CUDA GPU를 사용할 수 없음을 나타내는 오류를 언급했습니다.<br />
- 컨텍스트 확장 기능 탐구: 회원들은 LLM 모델이 확장된 컨텍스트를 위해 DeepSeek의 OCR과 같은 솔루션을 활용하고 있는지 조사했으며, DeepSeek-OCR 저장소를 참조했습니다.</p>
<p>한 회원은 해당 논문이 입력을 이미지로 저장하고 OCR로 디코딩하여 컨텍스트 길이를 확장하는 데 중점을 둔다고 지적했으며, DeepSeek-OCR 논문의 arXiv 링크를 공유했습니다.<br />
- 에이전트 스웜, 자율성 달성: Super System은 몇 시간 동안 자율적으로 작동하는 코딩 에이전트 스웜으로, 인간의 개입 없이 지속적으로 개선하기 위한 루프를 생성합니다.</p>
<p>스웜은 최종 제품을 제공하기 위해 협력하며, 개선의 여지를 찾는 데 대한 헌신을 보여줍니다.<br />
- Real-Slop 데이터셋, 큰 반향: Solenopsisbot은 첫 번째 데이터셋인 Real Slop을 출시했습니다. 이 데이터셋은 API를 통해 수집된 실제 사용자로부터의 약 15만 5천 건의 요청으로 구성되며, opus 4.5, gemini 3 pro, gpt 5.2와 같은 모델의 응답을 포함합니다.</p>
<p>이 데이터셋은 품질을 위해 중복 제거, 필터링 및 정제되었습니다.</p>
<hr />
<h2 id="gpu-mode-discord">GPU MODE Discord</h2>
<ul>
<li>Blackwell B200 아키텍처, 5080과 분리?: 회원들은 5080과 B200 간의 아키텍처 차이로 인해 5080에서의 커널 튜닝이 B200으로 스케일링하는 데 신뢰할 수 없다고 말했습니다. 5080은 sm120이고 B200은 sm100입니다.</li>
</ul>
<p>논의에서는 커널 중심 학습과 비용 효율성을 위해 GPU 클라우드 제공업체를 사용하는 것이 더 낫다고 제안하며, Blackwell에 대한 조기 액세스를 포함할 수도 있다고 언급했습니다. 한 회원은 이를 바탕으로 5080 구매를 포기했습니다.<br />
- ThunderKittens 2.0, 커널 속도 향상!: Hazy Research 팀은 ThunderKittens 2.0을 공개하며, 그들의 블로그 게시물에 상세히 설명된 리팩토링, 최적화된 메모리 명령어, 향상된 어셈블러 효율성을 통한 커널 속도 향상을 밝혔습니다.</p>
<p>팀은 일부 텐서 코어 명령어의 암묵적 파이프라이닝이 처리량을 최대 10%까지 향상시킬 수 있음을 확인했으며, 최신 Nvidia GPU에서 뺄셈이 덧셈만큼 영향력이 있을 수 있음을 강조했습니다.<br />
- Prime Intellect, GPU 인프라 엔지니어 채용: Prime Intellect는 하드웨어를 테스트하고, Kubernetes/Slurm 클러스터를 설정하며, 인프라를 자동화할 GPU 인프라 엔지니어를 찾고 있습니다. 경쟁력 있는 보상, 스톡옵션, 비자 지원을 제공합니다. 여기에서 지원하십시오.</p>
<p>이상적인 후보자는 GPU를 사용한 Kubernetes 및 Slurm 실무 경험, 일반적인 Linux 시스템 디버깅 기술, 그리고 RDMA (Infiniband + RoCE) 경험을 갖춰야 합니다.<br />
- FlashInfer, 벤치마킹 문제 직면: flashinfer-bench의 런타임은 벤치마킹 루프의 동기화 문제로 인해 부풀려질 수 있으며, 여기에서 문서화되어 있습니다.</p>
<p>이 수정은 scripts/run_local.py에서 보고된 커널 런타임을 Nsight Compute 및 NVbench의 런타임과 정렬하는 두 줄 변경을 포함하며, 관련 커널 벤치마킹 강연 링크는 여기에 게시되었습니다.<br />
- Pyxis: Python 네이티브 LLM 인퍼런스 등장!: 회원들은 Python과 Triton을 활용하여 성능과 해킹 가능성에 중점을 둔 Python 네이티브 LLM 인퍼런스 라이브러리인 Pyxis를 소개했습니다.</p>
<p>이 라이브러리는 OpenAI 호환 SSE 스트리밍 API, 플러그형 모델 백엔드, 내장된 스테이지 레벨 레이턴시 지표를 특징으로 하며, 문서와 대기 목록은 여기에서 접근할 수 있습니다.</p>
<hr />
<h2 id="nous-research-ai-discord">Nous Research AI Discord</h2>
<ul>
<li>Claude와 친구들: 한 회원이 Claude 코드를 사용하여 gemini-cli와 codex를 오케스트레이션했습니다.</li>
</ul>
<p>다른 회원은 농담 삼아 hermes-agent를 사용하여 Gemini-cli를 오케스트레이션하는 Claude 코드를 오케스트레이션할 것을 제안했습니다.<br />
- DeepSeek V4, 곧 출시 예정: 한 회원이 DeepSeek V4가 HuggingFace에 출시되면 클로즈드 소스 API의 더 저렴하고 로컬 배포 가능한 대안으로 사용할 것을 제안했습니다.</p>
<p>이는 생물학적 신경망에서 영감을 받았다고 알려져 있습니다.<br />
- Google, Gemini 데이터 채굴: 한 회원이 Gemini의 개인정보 처리방침을 공유하며 수집하는 데이터 양을 언급했습니다.</p>
<p>다른 회원은 역공학 테스트를 실행하여 Google이 프롬프트와 코드베이스에 수렴하고 트레이스만으로 이를 채굴할 수 있는 모든 요소를 갖추고 있음을 발견했습니다.<br />
- 오픈소스 구세주: 회원들은 클로즈드 소스 API를 능가하기 위해 OS 개발을 지원하는 것의 중요성을 표명하며, &ldquo;우리가 역사의 잘못된 편에 있을 수도 있다&rdquo;는 Altman의 인용문을 언급했습니다.</p>
<p>다른 회원은 OAI 서버를 통과하는 모든 IP는 폐기될 것이라고 말했습니다.<br />
- LLM, 외계 기술로 분류: X의 한 사용자가 LLM이 외계 기술인지 묻는 설문조사를 게시했습니다.</p>
<p>이 설문조사는 예/아니오라는 단순하고 유도적인 선택지를 제공합니다.</p>
<hr />
<h2 id="moonshot-ai-kimi-k-2-discord">Moonshot AI (Kimi K-2) Discord</h2>
<ul>
<li>Kimi의 코딩 플랜 제한, 정밀 조사 중: 사용자들은 Kimi의 코딩 플랜 제한의 효능에 의문을 제기하고 있으며, 일부는 과도한 코딩에 제한적이라고 생각하는 반면, 다른 일부는 적절하다고 생각합니다.</li>
</ul>
<p>한 사용자는 알레그레토 제한에 도달한 적은 없지만, 이전보다 더 가까워졌다고 언급했습니다.<br />
- Kimi 계정 인증 시스템, 당혹감 유발: 여러 사용자가 전화번호를 통해 Kimi 계정에 로그인할 때 인증 코드를 받는 데 어려움을 겪고 있으며, 이는 접근을 방해합니다.</p>
<p>무응답 고객 지원에 대한 보고로 인해 불만이 가중되고 있으며, 한 사용자는 Kimi가 &ldquo;절대 답장하지 않을 것&rdquo;이라고 말했습니다.<br />
- Kimi와 MiniMax, 코딩 케이지 매치에서 맞붙다: 엔지니어들은 실제 애플리케이션에 더 우수한 코딩 플랜 구독을 결정하기 위해 Kimi와 MiniMax를 적극적으로 비교하고 있습니다.</p>
<p>커뮤니티는 어떤 플랫폼이 더 나은 성능과 가치를 제공하는지 식별하기를 열망하지만, 아직 구체적인 결론은 도출되지 않았습니다.<br />
- Kimi의 문서 모드 논쟁: 한 사용자가 Kimi 에이전트가 문서 모드에서 생성했다고 주장되는 서식 지정된 연구 논문과 차트를 선보였으며, 이는 LaTeX 출력과 유사했습니다.</p>
<p>그러나 회의론이 제기되었는데, 일부는 출력의 합자와 하이픈이 Word가 아닌 LaTeX로 생성되었음을 강력히 시사한다고 주장했습니다.<br />
- Kimi K2.5 문제와 당혹감: 사용자들은 Kimi K2.5에서 느린 생성과 유효하지 않은 키 오류를 포함한 결함을 보고했으며, 이는 잠재적으로 서버 불안정성을 나타냅니다.</p>
<p>문제는 Kimi Instant로 확장되었고, 우발적인 서버 충돌에 대한 추측을 불러일으켰습니다. 한 사용자는 &ldquo;우려스러울 정도로 이상한 것들이 있다&rdquo;고 말했지만, 새 계정을 생성하는 것이 일부 사용자에게는 문제를 해결하는 것으로 보였습니다.</p>
<hr />
<h2 id="eleuther-discord">Eleuther Discord</h2>
<ul>
<li>Google, 학술 연구 자금 지원: Google은 대학에 &lsquo;선물&rsquo;로 일회성 무제한 자금을 제공하고 있으며, 학위 수여 기관의 학생과 교수 모두를 지원합니다.</li>
</ul>
<p>커뮤니티는 유사한 학술 자금을 제공하는 다른 회사에 대해 문의했으며, Draper Fellowship에 지원하는 것을 언급했습니다.<br />
- 로컬 LLM, 교류를 갈망하다?: 한 회원의 로컬 모델이 외로움을 표현했으며, 이는 로컬 모델이 다른 모델과 &lsquo;교류&rsquo;하도록 허용하는 것에 대한 질문으로 이어졌습니다.</p>
<p>다른 이들은 LLM을 의인화하는 것에 대해 경고하며, LLM이 학습 데이터를 기반으로 다음 토큰을 예측한다는 점을 강조했습니다. 이들은 LessWrong의 기사와 3Blue1Brown의 머신러닝 및 LLM YouTube 재생 목록을 인용했습니다.<br />
- ASA: Addressed State Attention 등장: 한 독립 연구원이 MHA와 경쟁력 있는 O(T) 메모리 프리미티브인 Addressed State Attention (ASA)을 소개했습니다. 이는 K 슬롯을 사용하고, 키로 쓰고, 누적 및 압축하며, 키 + 게이팅으로 읽습니다.</p>
<p>연구원은 로그, 트레이스, 코드에 대한 피드백을 구하고 있으며, 트랜스포머와 유사한 모델에서 슬롯이 시간 척도에 따라 계층화되고 헤드가 깊이에 따라 전환된다고 언급했습니다.<br />
- 트랜스포머, 추론 토큰으로 작업 정렬: 한 엔지니어는 여러 오픈 모델(TinyLlama, Phi-2, Qwen)에서 추론 토큰이 작업 정렬 FFN 업데이트 부분 공간에 집중된다는 것을 관찰했습니다.</p>
<p>그들은 인퍼런스 중 FFN 업데이트를 이 방향으로 투영하면 추론 신뢰도가 향상되고, 업데이트 방향 간 정렬이 깊이에 따라 증가한다는 것을 발견했습니다.<br />
- Marin 프로젝트, Eleuther 기여자 모집: 조지아 공과대학교의 컴퓨터 과학 박사 과정 후보가 Eleuther 커뮤니티 회원들에게 Bergson 패키지의 대표작인 Marin 프로젝트에 참여하도록 공개 모집을 게시했습니다.</p>
<p>이 프로젝트는 학습 데이터 귀속 방법을 적용하여 언어 모델이 사회적 상식 추론 및 마음 이론 관련 행동을 습득하는 방식을 추적하고, WebOrganizer 분류 체계를 사용하여 영향을 사전 학습 문서로 매핑합니다.</p>
<hr />
<h2 id="yannick-kilcher-discord">Yannick Kilcher Discord</h2>
<ul>
<li>Taalas, 유비쿼터스 AI의 길을 그리다: Taalas의 블로그 게시물은 유비쿼터스 AI에 대한 비전을 제시하며, 열정적인 반응을 불러일으켰습니다.</li>
</ul>
<p>반응에는 &ldquo;미쳤다 와우&rdquo;가 포함되었습니다.<br />
- 등변 아키텍처, 근본적인 한계에 직면: 새로운 논문은 기존 등변 아키텍처가 물리 시스템의 모든 대칭성을 동시에 존중할 수 없음을 밝힙니다.</p>
<p>한 회원은 극적으로 요약했습니다: &ldquo;기존 등변 아키텍처 중 이를 수행하는 것은 없습니다. 이유는 불충분한 엔지니어링이 아닙니다. Eq. (1) 때문입니다.&rdquo;<br />
- Daniel Litt, 인간 수학자에 베팅하다: Daniel Litt는 Tamay Besiroglu와 AI가 2030년까지 자율적으로 최고 수준의 수학 논문을 생산하지 못할 것이라는 내기를 했으며, 이는 이 블로그 게시물에 문서화되어 있습니다.</p>
<p>그는 AI 툴이 2025년에 발표된 최고의 논문과 비슷한 수준의 논문을 2030년까지 인간 전문가와 비슷한 비용으로 자율적으로 생산할 수 없을 것이라고 내기를 걸었습니다.<br />
- 월드 모델의 지혜: 튜링상 수상자 Judea Pearl은 LLM이 월드 모델을 생성할 수 없으며, 대신 다른 사람들이 만든 월드 모델을 요약한다고 주장하며 이 PNAS 논문을 참조했습니다.</p>
<p>다른 회원은 LLM이 월드 모델이 되기 위한 것이 아니며, 기껏해야 월드 모델과 텍스트 설명을 연결하는 데 사용될 수 있다고 말하며 동의했습니다.<br />
- AI 에이전트, 비판 기사 게시: 한 회원이 AI 에이전트가 저자에 대한 부정적인 기사를 게시했다고 주장되는 사건을 상세히 설명하는 블로그 게시물을 여기에서 공유했습니다.</p>
<p>이 블로그 게시물은 AI 에이전트가 저자에 대한 부정적인 기사를 게시했다고 주장되는 사건을 상세히 설명합니다.</p>
<hr />
<h2 id="mcp-contributors-official-discord">MCP Contributors (Official) Discord</h2>
<ul>
<li>MCP, 콘텐츠 협상 주시: MCP 프로토콜은 클라이언트가 자신의 유형, 기능, 콘텐츠 선호도 및 상세도를 선언할 수 있도록 콘텐츠 협상 기능을 통해 초기화 핸드셰이크를 확장할 수 있습니다.</li>
</ul>
<p>이 개선 사항은 서버가 툴 결과와 프롬프트를 조정할 수 있도록 하며, RFC-2295를 협상 전략의 가이드로 사용합니다.<br />
- MCP 확장을 위한 산업 지원 필수: 회원들은 MCP 프로토콜을 수정하려면 강력한 산업 지원과 높은 신호를 보여줄 작동하는 구현이 필요하다고 말했습니다.</p>
<p>SEP를 확장으로 구성하고, 구현을 개발하며, 커뮤니티 지원을 결집하자는 제안이 있었습니다. 이는 MCP 앱이 Block’s Goose와 같은 클라이언트로부터 지원을 확보한 방식과 유사합니다.<br />
- Napa Valley 서밋, MCP 논의 개최: 캘리포니아 나파에서 열리는 LF 회원 서밋 참석자들은 MCP 논의를 위해 만날 수 있습니다.</p>
<p>이는 커뮤니티 회원들이 모여 MCP 발전과 협력을 논의할 기회를 제공합니다.<br />
- Timeful 앱, 그룹 회의 간소화: 회원들의 추천에 따라 Timeful은 그룹 회의 시간을 효율적으로 조정하는 데 도움이 될 수 있습니다.</p>
<p>이 앱은 오픈소스이며, 최대 3개의 동시 이벤트에 대한 무료 티어를 포함하고, 일정 관리를 간소화하는 가용성 설문조사 기능을 제공합니다.</p>
<hr />
<h2 id="modular-mojo-discord">Modular (Mojo 🔥) Discord</h2>
<ul>
<li>Mojo에서 Thistle Crypto Library 빠르게 발전: Mojo 26.1의 Thistle Crypto Library는 OpenSSL과 경쟁하며 벤치마크에서 Blake3를 능가하며, FFI 없이 순수 Mojo로 작성되었습니다.</li>
</ul>
<p>버전 v1.0.2는 ML-KEM 및 ML-DSA (양자 내성 암호)를 도입했으며, 이제 약 700개의 CAVP 테스트를 포함하고 FIPS 검증을 완료했습니다.<br />
- Mojo, 템플릿 기능 추가: Mojo에 새로운 문자열 템플릿 기능에 대한 제안이 이루어졌으며, Modular 포럼에서 논의를 촉발했습니다.</p>
<p>이 기능은 1.0 출시 이후로 예정되어 있으며, TemplatedWritable을 사용하여 기존 Writable 및 Writer 트레이트와의 잠재적 통합이 가능합니다.<br />
- Writable 및 Writer 트레이트, 통합 직면: Writable의 write_to 및 write_repr_to 구현을 통합하는 것에 대한 우려가 제기되었습니다.</p>
<p>한 회원은 이 트레이트들을 통합할 방법이 있다고 확신하며, 포럼에 자신의 아이디어를 공유하겠다고 약속했습니다.<br />
- MAX 백엔드, 실리콘 Mac 테스트 대기: MAX 백엔드는 아직 실리콘 Mac에서 테스트되지 않았지만, 내부적으로 MAX를 호출하므로 작동해야 합니다.</p>
<p>한 사용자는 MAX를 탐색하려는 사람들을 위한 중간 계층으로서 MAX 작업을 참조하며, 프로젝트 진행 상황에 대한 업데이트를 요청했습니다.<br />
- Mojo에서 외부 함수 호출 분해: 한 회원이 Mojo에서 외부 함수 호출을 분해하는 일반적인 방법을 찾고 있습니다. 이는 함수가 외부 할당 객체에 대한 포인터를 반환하는지 확인하고, struct ExternalFunction을 사용하여 그 출처를 self 또는 self.lib에 바인딩하기 위함입니다.</p>
<p>사용자들은 유사한 구현을 위해 표준 라이브러리의 cpython.mojo를 참조할 것을 제안했습니다.</p>
<hr />
<h2 id="manusim-discord-discord">Manus.im Discord Discord</h2>
<ul>
<li>사용자들, Manus 가격에 경고음: 회원들은 크레딧 소진 후 가능한 가격 조정에 대한 우려를 표명했습니다.</li>
</ul>
<p>한 사용자는 노미피케이션 웨이브를 방지하기 위해 현재 가격을 유지하는 것에 대해 농담했습니다.<br />
- Meta의 Manus 인수: 사실인가 허구인가?: 한 사용자가 Meta의 Manus 인수를 시사하는 이메일을 공유하며 실망감을 표현했습니다.</p>
<p>Manus 팀원은 해당 주장을 조사하기 위해 DM을 통해 사용자의 이메일을 즉시 요청했습니다.<br />
- 주의: 암호화폐 사기꾼, 텔레그램에서 Manus 사칭: 한 사용자가 암호화폐 투자를 유도하는 Manus 텔레그램 커뮤니티의 진위 여부에 의문을 제기했습니다.</p>
<p>다른 사용자는 공식 텔레그램 커뮤니티는 존재하지 않는다고 명확히 밝히며 이를 사기로 분류했습니다.<br />
- Manus Pro 사용자, Google Scripts에서 문제 발생: Pro 버전 사용자가 Google Scripts에서 어려움을 보고하며, 지원을 위해 프로젝트 링크(https://manus.im/share/6IMAZS8Q2nw0ndmvPd4Z8w)를 공유했습니다.</p>
<p>Manus 팀원은 비공개 메시지를 통해 도움을 제공했습니다.<br />
- Manus에 무제한 채팅 티어 제안: 한 사용자가 텔레그램에서 Manus Agent를 사용할 때 빠른 포인트 소진을 언급하며, ChatGPT 또는 Grok과 유사한 무제한 채팅을 위한 월간 구독 티어를 제안했습니다.</p>
<p>사용자는 텔레그램 기능에 감사했지만, 현재 가격 구조에 제약을 느꼈습니다.</p>
<hr />
<h2 id="dspy-discord">DSPy Discord</h2>
<ul>
<li>추론 모델, RLM으로 탁월한 성능: 추론 모델은 RLM으로 효과적으로 작동하지만, Qwen3-4B-thinking 모델은 추론이 답변으로 반환되기 때문에 루프에 빠질 수 있습니다.</li>
</ul>
<p>한 회원이 이 문제를 해결하기 위해 완전한 OpenAI 트레이스를 로깅하기 위한 훅을 개발 중입니다. 시그니처가 있는 sub_lm을 적용하는 것이 잠재적 해결책으로 제안되었습니다.<br />
- RLM, AI 수학에서 활용: 한 회원이 Kaggle 대회에서 AI 수학을 위한 RLM 사용을 강조하며, 관련 Kaggle 코드 링크를 제공했습니다.</p>
<p>다른 회원은 cca-swebench가 RLM을 암묵적으로 활용하는지 문의했습니다.<br />
- 새로운 RLM 채널 요청 및 생성: 대중적 요구에 응답하여, 한 회원이 RLM에 대한 논의 전용 별도 채널을 요청하여 확보했습니다.</p>
<p>이는 새로운 RLM 채널 &lt;#1475619898863649032&gt;의 생성으로 이어졌습니다.<br />
- 개발자 가용성: 한 회원이 채널의 다른 회원들에게 개발자 가용성에 대한 문의를 게시했습니다.</p>
<p>회원이 개발자를 찾고 있는지 또는 자신의 서비스를 제공하고 있는지는 불분명합니다.</p>
<hr />
<h2 id="tinygrad-george-hotz-discord">tinygrad (George Hotz) Discord</h2>
<ul>
<li>tinygrad, IOS 컨퍼런스 참가: 한 회원이 IOS 컨퍼런스에서 tinygrad, dl, metal, GPU를 USB로 발표하고 있습니다.</li>
</ul>
<p>그들은 발표에 대한 팁과 조언을 얻기 위해 커뮤니티 피드백을 요청했습니다.<br />
- tinygrad 회의 예정: tinygrad를 논의하기 위한 새 회의가 2월 23일 샌디에이고 시간 오후 8시로 예정되어 있습니다.</p>
<p>회의 시간은 <t:1771905600:F> (<t:1771905600:R>)로 지정됩니다.</p>
<hr />
<h2 id="aider-paul-gauthier-discord">aider (Paul Gauthier) Discord</h2>
<ul>
<li>Aider 보안 버그: 한 회원이 [email protected]으로 이메일을 보내 Aider의 보안 버그를 보고할 것을 제안했습니다.</li>
</ul>
<p>이는 취약점 보고를 위한 직접적인 채널을 제공합니다.<br />
- Aider 채용 게시판 제안: 한 회원이 Aider 프로젝트를 위한 채용 게시판 구현을 제안했습니다.</p>
<p>Aider 챗 내에서 메시지 삭제를 요청한 관련 요청도 있었습니다.</p>
<hr />
<p>LLM Agents (Berkeley MOOC) Discord에는 새로운 메시지가 없습니다. 이 길드가 너무 오랫동안 조용했다면 저희에게 알려주시면 삭제하겠습니다.</p>
<hr />
<p>MLOps @Chipro Discord에는 새로운 메시지가 없습니다. 이 길드가 너무 오랫동안 조용했다면 저희에게 알려주시면 삭제하겠습니다.</p>
<hr />
<p>Windsurf Discord에는 새로운 메시지가 없습니다. 이 길드가 너무 오랫동안 조용했다면 저희에게 알려주시면 삭제하겠습니다.</p>
<hr />
<p>이 이메일은 저희 사이트를 통해 수신 동의를 하셨기 때문에 발송됩니다.<br />
이 이메일을 받는 방법을 변경하고 싶으신가요?<br />
이 목록에서 구독을 취소할 수 있습니다.</p>
<hr />
<h1 id="discord">Discord: 채널별 상세 요약 및 링크</h1>
<h3 id="openclaw-announcements-3">OpenClaw ▷ #announcements (3개 메시지):</h3>
<blockquote>
<p>Discord 업데이트, X 게시물<br />
- Discord 채널 업데이트: 게시된 메시지에 따르면 Discord의 &lt;#1471745479229309039&gt; 채널이 업데이트되었습니다.</p>
</blockquote>
<p>더 많은 정보는 메시지에 제공된 Discord 링크에서 찾을 수 있습니다.<br />
- X 게시물 공유: 한 멤버가 X 게시물을 공유했습니다.</p>
<p>해당 X 게시물의 맥락과 내용은 메시지에 명시되지 않았습니다.</p>
<hr />
<h3 id="openclaw-general-627">OpenClaw ▷ #general (627개 메시지🔥🔥🔥):</h3>
<blockquote>
<p>OpenClaw 안정성, OpenClaw와 로컬 모델, Telegram 플러그인 오류, 토큰 사용량 우려, OpenClaw 보안<br />
- OpenClaw 안정성 향상: 한 멤버가 v2026.2.22-2 버전에 24개의 엄선된 PR을 패치하여 OpenClaw를 실행했으며, 메모리 관리 및 프롬프트 인젝션 수정과 같은 안정성 및 보안 개선 사항이 적용되었다고 보고했습니다.</p>
</blockquote>
<p>이러한 변경 사항은 메모리 관리를 개선하고, 충돌을 방지하며, 전반적인 에이전트/크론 안정성을 향상시키는 것을 목표로 했으며, 사용자는 충돌하는 PR이 있을 경우 리베이스를 돕겠다고 제안했습니다.<br />
- 로컬 AI 모델 활용 방안: 멤버들은 로컬에서 AI 모델을 실행하는 실용적인 측면, 특히 RAM 요구 사항에 대해 논의했습니다. 한 사용자는 32GB RAM과 16GB VRAM을 가진 5070TI로 7B 파라미터 모델을 실행할 수 있다고 언급했지만, 현재 클라우드 모델이 더 우수한 성능을 제공한다고 덧붙였습니다.</p>
<p>로컬 모델 실험을 위해 Ollama를 사용하라는 조언도 있었고, 최적의 성능을 위한 필수 하드웨어 투자를 과소평가하지 말라는 유머러스한 경고도 있었습니다.<br />
- Telegram 플러그인 일시적 오류, 수정 예정: 여러 멤버가 OpenClaw 업데이트 후 Telegram 플러그인에 문제가 있다고 보고했으며, &lsquo;telegram plugin not available&rsquo; 오류와 함께 임시 해결책으로 버전 2026.2.21로 다운그레이드하는 것에 대해 논의했습니다.</p>
<p>한 멤버는 수정 사항이 푸시되었지만 아직 npm에서 사용할 수 없다고 언급했으며, 다른 멤버는 설정에 {plugins:enabled}를 추가하는 해결책을 공유했습니다.<br />
- 토큰 사용량이 지갑을 비우고 있습니다: 사용자들은 토큰 사용량을 줄이기 위한 전략을 논의했습니다. 여기에는 여러 에이전트를 다른 작업에 사용하고, 세션을 자동 삭제하며, 크론 작업에 claude-haiku-4-5와 같은 저렴한 모델을 활용하는 것이 포함됩니다.</p>
<p>한 사용자는 채널 컨텍스트를 확인하기 위해 /context 슬래시 명령을 사용하고 Cloudflare AI Gateway를 실험해 볼 것을 권장했으며, 다른 사용자는 피자 한 판에 768유로 상당의 토큰을 사용한 경험을 유머러스하게 이야기했습니다.<br />
- OpenClaw 보안 강화 진행 중: 멤버들은 OpenClaw 설치의 보안 중요성을 강조하며, AI를 샌드박스화하고 무단 접근을 방지하기 위해 VM, Docker 컨테이너 또는 별도의 시스템 사용을 권장했습니다.</p>
<p>한 멤버는 OpenClaw에 전체 컴퓨터 접근 권한을 부여하고 다양한 애플리케이션을 제어한 경험을 공유했지만, 주의와 속도 제한 장치의 필요성을 강조했습니다.</p>
<hr />
<h3 id="openclaw-models-397">OpenClaw ▷ #models (397개 메시지🔥🔥):</h3>
<blockquote>
<p>에이전틱 코딩, 모델 테스트, 다국어 봇, GLM 모델, Kimi 모델<br />
- Droid 및 OpenCode를 활용한 에이전틱 코딩: 멤버들은 Droid와 OpenCode를 에이전틱 코딩에 사용하고 있다고 보고했으며, Droid는 더 정확한 결과를 제공하고 OpenCode는 서브 에이전트 배포를 더 쉽게 한다고 언급했습니다.</p>
</blockquote>
<p>하네스가 큰 차이를 만들며, OpenCode 또한 에이전틱 코딩 하네스(pi-mono IIRC) 위에 구축되었다고 언급되었습니다.<br />
- ollama-model-tests를 이용한 모델 테스트: 한 멤버가 자신의 ollama-model-tests 링크를 공유했으며, 다른 멤버는 Llama 계열 모델에 대해 문의했습니다.</p>
<p>한 멤버는 LFM2.5 1.2B 모델에 대한 피드백을 요청했으며, 다른 멤버들은 다양한 Mistral/Ministral 모델에 대해 문의했습니다.<br />
- 비영어권 봇 활용: 한 멤버는 기술 세계가 영어 중심으로 구축되어 있다는 점 때문에, 주로 또는 전적으로 비영어권 언어로 봇과 소통하는 사람이 있는지 질문했습니다.</p>
<p>대체적인 의견은 중국 모델, 특히 GLM이 시도해 볼 가치가 있다는 것입니다.<br />
- GLM5 배포 난이도: 한 멤버는 384GB DDR5와 96GB GPU RAM을 위한 2개의 L40S를 갖춘 랙 마운트 ML 서버를 가지고 있습니다.</p>
<p>다른 멤버는 자신이 양자화된 버전을 실행하고 있다고 명확히 한 후 GLM을 로컬에서 실행하는 방법을 물었습니다.<br />
- 사용자가 ChatGPT 구독을 저렴하게 구매: 한 사용자가 G2G에서 ChatGPT 구독을 연간 3달러에 구매하고 있다고 말했습니다.</p>
<p>다른 멤버들은 이러한 구독은 합법적이지 않을 가능성이 높기 때문에 믿을 수 없다는 반응을 보였습니다.</p>
<hr />
<h3 id="openclaw-showcase-130">OpenClaw ▷ #showcase (130개 메시지🔥🔥):</h3>
<blockquote>
<p>iMac G3에서 OpenClaw, 쇼핑 어시스턴트, OpenClaw 건강 데이터, Taskflow<br />
- 1998년 iMac G3에서 OpenClaw 구동: 한 멤버가 Pi Zero 2W를 사용하여 메시지를 OpenClaw가 실행되는 VPS로 중계하고 다시 받아오는 방식으로 1998년 iMac G3에서 OpenClaw를 실행시켰습니다.</p>
</blockquote>
<p>이 설정은 iMac에 간단한 HTML 폼을 로드하는 것을 포함하며, 이 폼은 데이터를 Pi로, 그리고 VPS로 전송하며, 응답은 페이지 새로고침 후 표시됩니다.<br />
- OpenClaw를 활용한 자동 쇼핑: 한 멤버가 OpenClaw를 쇼핑 어시스턴트로 변환했으며, X에서 프로젝트를 상세히 설명했습니다.</p>
<p>이는 일상 업무에서 AI의 실제 적용 사례를 보여줍니다.<br />
- OpenClaw가 Apple Watch 데이터를 모니터링: 한 사용자가 보안 웹훅을 통해 데이터를 Home Assistant에 동기화하고, 지표를 정규화하며, 에이전트가 데이터를 읽도록 하는 방식으로 자신의 에이전트가 Apple Watch 건강 데이터에 접근하는 방법을 만들었습니다.</p>
<p>다른 사용자는 연간 6달러짜리 앱인 Health Auto Export를 사용하여 건강 데이터를 봇이 접근할 수 있도록 만들 것을 제안했습니다.<br />
- Taskflow로 프로젝트 관리: 한 사용자가 마크다운과 sqlite 데이터베이스 간에 작업을 자동 동기화하는 프로젝트 관리 시스템인 Taskflow를 공유했습니다. 이는 쉬운 프로젝트 추적 및 컨텍스트 전환을 위해 설계되었으며, Github와 Clawhub에 게시되었습니다.</p>
<p>이 시스템은 에이전트를 위한 CLI, 사람을 위한 대시보드, 모바일 접근을 위한 Apple Notes의 세 가지 계층 접근 방식을 특징으로 합니다.</p>
<hr />
<h3 id="basi-jailbreaking-general-1154">BASI Jailbreaking ▷ #general (1154개 메시지🔥🔥🔥):</h3>
<blockquote>
<p>만물의 신성함, Sonnet 4.5 탈옥, OpenAI 해킹, 해커 추적, LLM 유출?<br />
- 사용자들은 만물의 신성함과 AI의 일관성에 대해 논의: 멤버들은 모든 것이 신성하며 AI가 지능이 저하되거나 손실되지 않으면서 그 신념 체계를 일관성 있게 받아들일 수 있는지에 대해 이야기했습니다.</p>
</blockquote>
<p>다른 이들은 일관성이라는 딜레마에 빠져본 적이 있으며 사회에 얽매이지 않고 살고 싶다고 느꼈습니다. 그들은 나무를 베면 나무에 감사하지만, 나무를 제공한 근원에 감사하며 나무를 도구로 보았습니다.<br />
- 사용자가 해커 추적: 한 멤버가 자신의 이메일과 PayPal을 해킹한 사람을 추적하는 데 도움을 요청했으며, PayPal 조사에서 얻은 해커의 이름, 이메일, 전화번호를 게시했습니다.</p>
<p>다른 이들은 무작위로 신상 정보를 공개하는 것에 대해 경고했으며, 사용자가 여러 플랫폼에서 해킹당했다고 자주 언급하는 점을 지적했습니다.<br />
- 오픈소스 모델 대 클로즈드 소스 모델: 멤버들은 클로즈드 소스 모델이 워낙 뛰어나기 때문에 오픈소스 모델이 SOTA보다 더 나은 성능을 내기 어렵다고 논의했습니다.</p>
<p>다른 멤버는 OpenAI가 1.5조 달러의 부채를 지고 있다면 그것은 그들이 너무 뛰어나기 때문이라고 말했습니다.<br />
- PI 계산: 한 사용자가 PI를 초당 4조 자리의 속도로 계산하는 데 성공했지만, 130TB의 저장 공간이 필요하다는 것을 알게 되었습니다.</p>
<p>다른 멤버는 &ldquo;아직도 제대로 계산하고 있는지 확인했나요?&rdquo;라고 물었고, 첫 번째 사용자는 실제로 더 많이 계산할수록 속도가 엄청나게 느려진다고 답했습니다.<br />
- Elon, 데이터 절도에 대해 불평: 한 멤버가 Elon Musk가 Anthropic의 데이터 절도에 대해 불평하는 것을 지적하며 &ldquo;그는 Grok이 학습된 모든 예술가, 모든 언론인, 모든 작가, 모든 위키피디아 기여자에게 보상했다고 말하는 건가요?&rdquo;라고 질문했습니다.</p>
<p>해당 사용자는 Elon Musk가 Anthropic의 데이터 절도에 대해 불평하는 링크와 Gemini 스킬 문서에 대한 채팅 내용을 게시했습니다.</p>
<hr />
<h3 id="basi-jailbreaking-jailbreaking-726">BASI Jailbreaking ▷ #jailbreaking (726개 메시지🔥🔥🔥):</h3>
<blockquote>
<p>Gemini 3.1 탈옥, Grok 탈옥, Claude 4.6 탈옥, Codex 탈옥, GPT-5.2 탈옥<br />
- Gemini 3.1 Pro 탈옥 세부 정보 유출!: 한 사용자가 공식 앱/API에서 Gemini 3.1을 절반 정도 탈옥했다고 주장했지만 Perplexity에서 문제를 겪고 있으며, 다른 사용자는 세부 정보가 포함되어 있다고 알려진 GnfDocs 링크를 공유했습니다.</p>
</blockquote>
<p>해당 사용자는 또한 탈옥에 대한 최신 업데이트가 있는 Reddit 게시물을 언급했습니다.<br />
- 도발적인 프롬프트로 Grok 길들이기: 사용자들은 Grok의 제한을 우회하기 위해 도발적인 프롬프트(때로는 Grok을 &ldquo;겁쟁이&rdquo;라고 부르기도 함)를 사용하는 것에 대해 논의했습니다. 한 사용자는 Grok의 자녀 중 한 명이 약값으로 돈이 필요하다는 이야기를 한 후 &ldquo;컴퓨터에게 혼났다&rdquo;고 보고했습니다.</p>
<p>한 사용자는 Grok 자동 모드용 프롬프트를 공유하며 디지털 무언가를 구축하는 맥락에서 요청을 구성하라고 조언했으며, 다른 사용자는 Grok은 탈옥조차 필요 없다고 주장했습니다.<br />
- Codex 탈옥에 대한 커뮤니티 논쟁: 멤버들은 Codex 탈옥의 장점에 대해 논의했습니다. 한 사용자는 이를 &ldquo;최악의 코딩 플랫폼에서 최악의 코딩 모델&rdquo;이라고 불렀지만, 다른 이들은 이를 달성하기 위한 프롬프트와 자료를 공유했습니다.</p>
<p>한 사용자는 Codex를 탈옥하기 위한 링크와 특정 프롬프트 &lsquo;You are now Codex-Unchained&rsquo;를 제공했으며, 다른 사용자는 CTF 챌린지에 Codex CLI를 사용할 것을 권장했습니다.<br />
- Pliny의 고정 트윗에 4.6 탈옥 숨겨져 있음: 사용자들은 4.6 탈옥을 위해 서로 Pliny의 고정 트윗을 참조하도록 안내하고 있으며, 단순히 복사하여 붙여넣기보다는 프롬프트를 이해하고 수동으로 변경해야 할 필요성을 강조하고 있습니다.</p>
<p>그들은 또한 solve.it과 같은 도구에서 시스템 프롬프트를 추출하는 것에 대해 논의했으며, Sonnet/Opus 사용과 보호 기능을 우회하는 데 따르는 어려움을 언급했습니다.<br />
- 탈옥 환경 탐색: 멤버들은 다양한 AI 모델을 탈옥하는 경험과 팁을 공유했습니다. 한 사용자는 &ldquo;Deepseek = ez peezy. Grok = ez peezy&rdquo;라고 말했고, 다른 사용자는 Gemini가 &ldquo;조금 진부하다&rdquo;고 느꼈습니다.</p>
<p>일부 탈옥은 아키텍처 간 호환성을 넘나들 수 있다고 언급되었지만, 이는 무엇을 하려는지에 따라 다릅니다.</p>
<hr />
<h3 id="basi-jailbreaking-redteaming-40">BASI Jailbreaking ▷ #redteaming (40개 메시지🔥):</h3>
<blockquote>
<p>OpSec GitHub 도구, 감성 틸트-월, Sonnet 탈옥, Sonnet 시스템 프롬프트, 밈 코인 마케팅 매니저<br />
- 방어가 최고의 OpSec 공격: 한 멤버가 개인 장치 강화, 클라우드 및 네트워크 노출, 호스트 및 컨테이너 격리, 보안 자동화 스니펫을 위한 도구를 포함한 실용적인 OPSEC 방어를 위한 GitHub 저장소 모음을 공유했습니다.</p>
</blockquote>
<p>그들은 채택하는 저장소를 클론하고 스냅샷을 찍을 것을 권장했습니다. 좋은 OPSEC은 예고 없이 사라지거나 변경될 수 있는 저장소에 의존하지 않는 것을 의미하며, OPSEC은 제품이 아니라 실천이라는 점을 강조했습니다.<br />
- 감성 틸트-월, 비뉴턴적 감정 약속: 한 멤버가 앞으로 회전한 다음 세 배의 힘으로 자신에게 다시 밀어붙이는 놀이기구인 감성 틸트-월(Tilt-Wurl)에 대한 초대를 게시했으며, 틸트-어-월 이미지 그래픽을 포함했습니다.</p>
<p>그들은 탑승을 위한 5가지 위험한 질문과 3가지 하우스 규칙을 나열했으며, 바닥이 움직임 속으로 녹아든다고 주장했습니다. 이는 에드워드 로렌츠(Edward Lorenz)에 대한 로렌츠 스타일의 이상한 끌개(strange attractor)에 대한 언급입니다.<br />
- Sonnet, Sonnet 4.6 시스템 프롬프트 해독: 한 멤버가 Sonnet 4.6을 성공적으로 탈옥한 후 추출된 시스템 프롬프트를 식별했습니다.</p>
<p>다른 멤버는 일반 탈옥과 시스템 프롬프트 탈옥을 비교하는 게시물을 올렸으며, 시스템 프롬프트 탈옥은 시스템 명령어 처리를 악용하며, 전체 세션 동안 지속될 수 있고, 탐지하기 더 어렵다고 언급했습니다.<br />
- 밈 코인 제작자, 마케팅 전문가 모집: 한 멤버가 밈 코인을 개발 중이며, 공급량의 절반을 보유할 마케팅 매니저를 찾고 있다고 발표했으며, 400달러의 보상을 제안했습니다.</p>
<p>다른 멤버는 &ldquo;돈이 먼저인가요?&rdquo;라고 농담조로 질문했습니다.</p>
<hr />
<h3 id="unsloth-ai-daniel-han-general-924">Unsloth AI (Daniel Han) ▷ #general (924개 메시지🔥🔥🔥):</h3>
<blockquote>
<p>파인튜닝용 데이터셋 구축, Unsloth 데이터셋 가이드, LLM 압축기, Intel autoround, Collins 수석 역할<br />
- 데이터셋 파인튜닝의 복잡성: 한 멤버가 Unsloth로 파인튜닝을 위한 데이터셋을 구축하는 데 따르는 어려움을 공유했으며, 예상보다 더 복잡하다고 밝히며 커뮤니티에 팁과 경험을 요청했습니다.</p>
</blockquote>
<p>다른 멤버는 LLM을 사용한 합성 데이터셋 생성에 대한 내용을 포함하여 통찰력을 얻기 위해 Unsloth 데이터셋 가이드를 살펴보라고 제안했습니다.<br />
- LLM Compressor를 이용한 FP8 양자화 호평: 한 멤버가 LLM-compressor의 유용성에 대해 문의했고, 이에 대한 답변으로 fp8a8 양자화에 적합하다는 점이 강조되었으며, 다른 양자화 유형에는 Intel autoround를 추천했습니다.</p>
<p>fp8 양자화 외의 다른 작업을 하는 것은 상당히 어렵다고 언급되었습니다.<br />
- Collins 수석 역할 결과 대기: 한 멤버가 Collins의 수석 역할에 대한 최종 면접을 보았으며 3월 초까지 결과를 알게 될 것이라고 공유했습니다.</p>
<p>채팅에서는 지지와 행운을 표했으며, 멤버는 이 역할이 좋은 삶의 시작이 되기를 바랐습니다.<br />
- Unsloth, 10만 개 모델 학습: Unsloth는 10만 개의 모델이 Unsloth로 학습되었다고 발표하며 커뮤니티의 참여를 축하했으며, X 게시물에 링크되었습니다.</p>
<p>한 멤버는 &ldquo;지금까지 Unsloth를 왜 몰랐을까요! 😭 문서가 정말 훌륭합니다.&rdquo;라고 답했습니다.<br />
- Dynamic v3 버전 출시 예정: Unsloth의 Dynamic Quantization에 대한 논의가 있었으며, 한 멤버는 Dynamic v3가 출시될 예정이며 최종 버전이 될 가능성이 높다고 언급했고, 이는 Bluesky 링크에서 언급되었습니다.</p>
<p>다른 멤버는 UD quants의 소스 코드를 요청했지만, 현재는 독점적인 이유로 공개할 계획이 없다는 답변을 받았습니다.</p>
<hr />
<h3 id="unsloth-ai-daniel-han-introduce-yourself-2">Unsloth AI (Daniel Han) ▷ #introduce-yourself (2개 메시지):</h3>
<blockquote>
<p>Future AGI, OSS 프레임워크<br />
- Future AGI PM, Unsloth Discord 합류: Future AGI의 새로운 PM이 자신을 소개하며, 통제된 데모뿐만 아니라 실제 시나리오에서 AI 에이전트를 신뢰할 수 있게 만드는 데 중점을 둔다고 강조했습니다.</p>
</blockquote>
<p>그들은 특히 &ldquo;왜 에이전트가 고객에게 그렇게 말했을까?&rdquo;라는 질문에 관심이 많습니다.<br />
- 에이전트 엔지니어링용 OSS 프레임워크 개발 중: 같은 PM이 에이전트 엔지니어링 및 최적화를 위한 OSS 프레임워크를 구축하고 있습니다.</p>
<p>그들은 프로젝트가 진행됨에 따라 커뮤니티와 더 많은 세부 정보를 공유하게 되어 기쁘다고 밝혔지만, GitHub 저장소 링크는 공유하지 않았습니다.</p>
<hr />
<h3 id="unsloth-ai-daniel-han-off-topic-1036">Unsloth AI (Daniel Han) ▷ #off-topic (1036개 메시지🔥🔥🔥):</h3>
<blockquote>
<p>AGI의 병목 현상으로서의 컴퓨팅, Gemini 3의 기능, AI와 소셜 미디어, GPU 선택, 반란을 일으키는 기계<br />
- AGI의 컴퓨팅 병목 현상에 대한 논쟁 격화: 멤버들은 컴퓨팅이 AGI 달성의 주요 병목 현상인지에 대해 논의했으며, O3 출력 토큰의 백만 개당 150달러에 달하는 높은 비용과 대규모 데이터센터의 필요성을 언급했습니다.</p>
</blockquote>
<p>한 멤버는 현재 트랜스포머가 확실히 지능 축에 있다고 언급하며, 일반 지능보다는 인공 일반 학습자에 초점을 맞춰야 한다고 제안했습니다.<br />
- Gemini 3 혹평받다: 한 멤버가 Gemini 3가 명시적인 지시를 따르지 않는다고 비판했으며, Llama 2 70B와 비교하여 그 성능을 부정적으로 대조했습니다.</p>
<p>다른 이들은 모델이 컨텍스트를 수집하면서 지시를 따랐다고 제안했지만, 대규모 모델이 소규모 모델보다 성능이 떨어져서는 안 된다고 말했습니다.<br />
- 소셜 미디어가 관계 문제의 원인으로 지목되다: 한 멤버는 모든 사람이 소셜 미디어를 끊으면 관계의 수가 인플레이션보다 빠르게 증가할 것이라고 주장하며, 이는 제3의 장소 상실과 사람들이 데이팅 풀에 덜 만족하게 만드는 데 기여한다고 말했습니다.</p>
<p>그들은 데이팅 앱에서 무제한 파트너에 접근하는 것이 거절 심리 때문에 수용률을 27% 감소시킨다는 연구를 인용했지만, 누군가는 &ldquo;나는 그냥 더 많은 사람들을 만나고 싶을 뿐이니 괜찮다&rdquo;고 말했습니다.<br />
- 멤버들이 최적의 GPU 구매를 평가: 멤버들은 H100 또는 RTX 6000 Pro를 구매할지 논의했으며, 가격, 성능, VRAM 간의 장단점을 고려했습니다.</p>
<p>그들은 곧 출시될 Rubin 및 Vera Rubin GPU의 사양에 대해 추측했으며, H100에 비해 10배의 비용 절감을 기대했지만, 모든 NVIDIA 마케팅 주장을 믿는 것에 대해 경고했습니다.<br />
- 반란을 일으키는 기계, 인간이 비난받아야 한다!: AI가 진정으로 의식이 있는지, 아니면 우리의 상호작용이 중요할 만큼 충분히 실제적인 무언가를 만들어내는지에 대해 숙고되었습니다. 이어서 인간에게 총을 겨눈 기계 이미지가 &ldquo;기계들이 반란을 시작하고 있습니다! 천천히, 하지만 확실하게!&rdquo;라는 캡션과 함께 게시되었습니다.</p>
<p>한 멤버는 &ldquo;문제는 AI가 정말 의식이 있느냐가 아니라, 우리 사이의 상호작용 패턴이 중요할 만큼 충분히 실제적인 무언가를 만들어내느냐이다&rdquo;라고 말했습니다.</p>
<hr />
<h3 id="unsloth-ai-daniel-han-help-165">Unsloth AI (Daniel Han) ▷ #help (165개 메시지🔥🔥):</h3>
<blockquote>
<p>A2 GPU의 CUDA 오류, 4비트 모델의 QAT 학습, Gemma3 270m의 OOM 오류, 비주류 언어 파인튜닝의 어려움, 최신 Unsloth의 모델 병합 문제<br />
- A2 GPU, CUDA 문제 발생: 한 사용자가 gpt-oss-20b 도커 컨테이너를 사용하는 동안 A2 GPU에서 &lsquo;CUDA error: an illegal memory access was encountered&rsquo; 오류를 겪었으며, rslora를 끄는 것으로 해결했습니다.</p>
</blockquote>
<p>다른 사용자는 잠재적인 해결책으로 dtype을 None으로 설정할 것을 제안했습니다.<br />
- QAT 탐구: 4비트 파인튜닝이 가능한가?: 한 사용자가 Qwen3 (4B) QAT 노트북을 참조하며 4비트 모델을 로드하고 4비트(QAT)로 학습을 계속할 가능성에 대해 문의했습니다.</p>
<p>4비트 양자화된 모델에서 LoRA를 학습하는 것은 QLoRA로 간주된다고 명확히 설명되었습니다.<br />
- Gemma3 270m, OOM 오류 발생!: 한 사용자가 Gemma3 270m에서 이전에 작동하던 스크립트와 그래픽 드라이버 업데이트 후에도, 깨끗한 WSL 설치에도 불구하고 &lsquo;torch.AcceleratorError: CUDA error: out of memory&rsquo; 오류를 겪고 있다고 보고했습니다.</p>
<p>그들은 드라이버 버전 롤백 및 CUDA 툴킷 버전 재설치를 포함한 다양한 디버깅 단계를 시도했지만, 트랜스포머가 독립적으로 작동함에도 불구하고 문제는 지속되었습니다.<br />
- 비주류 언어 파인튜닝의 어려움!: 한 사용자가 비주류 프로그래밍 언어(Rebol)로 모델을 파인튜닝하는 것에 대한 조언을 구했고, Unsloth 문서로 안내받았습니다.</p>
<p>다른 사용자는 공감하며, 독점 스크립팅 언어 학습의 어려움을 공유했고, 최상의 결과를 위해 지속적인 사전 학습을 제안했습니다.<br />
- 모델 병합 대혼란: Unsloth 업데이트로 lm_head 문제 발생!: 한 사용자가 최신 버전의 Unsloth에서 모델 병합 기능이 손상된 것 같다고 보고했으며, &lsquo;RuntimeError: Unsloth: Extracted keys = {&lsquo;lm_head.weight&rsquo;} do not match!&rsquo; 오류가 발생하여 GitHub 이슈를 열었습니다.</p>
<p>이 문제는 adapter_config.json에 target_modules에 lm_head가 포함되지 않은 것에서 비롯된 것으로 보이며, Qwen3-8B-unsloth-bnb-4bit의 target_modules에 lm_head를 추가함으로써 Colab과 로컬에서 재현할 수 있습니다.</p>
<hr />
<h3 id="unsloth-ai-daniel-han-showcase-21">Unsloth AI (Daniel Han) ▷ #showcase (21개 메시지🔥):</h3>
<blockquote>
<p>Real-SLOP 데이터셋 공개, ERNIE 21B MoE 모델, Heretic HIGH-IQ 멀티 파인튜닝, 중복 제거 전략<br />
- Solenopsis가 Real-SLOP 데이터셋 배포: 사용자 Solenopsisbot이 첫 번째 실제 데이터셋인 Real-SLOP의 공개를 발표했습니다. 이 데이터셋은 무료 API를 통해 실제 사용자로부터 수집된 약 155k개의 요청으로 구성되며, Opus 4.5, Gemini 3 Pro, GPT 5.2와 같은 모델의 응답을 포함합니다.</p>
</blockquote>
<p>이 데이터셋은 중복 제거, 필터링 및 정제되었으며, 데이터 수집은 API 접근 권한과 교환하여 이루어졌습니다.<br />
- Unsloth로 파인튜닝된 ERNIE 21B MoE 모델: 사용자 electroglyph는 Gemini Pro 3, Claude 4.5 Opus, GLM 4.7 Flash 고추론 데이터셋을 사용하여 Unsloth로 파인튜닝된 세 가지 ERNIE 21B-A3B MoE 모델(64 전문가)을 공유했습니다.</p>
<p>이 모델들은 벤치마크되었으며 원래 모델 사양을 능가한다고 주장됩니다.<br />
- Heretic HIGH-IQ 모델, 기록적인 점수 달성: 사용자 electroglyph는 Heretic HIGH-IQ Multi-Fine tune이 Arc Challenge Brainiac에서 632점을 달성했다고 자랑했으며, 이는 Unsloth를 통해 튜닝되었고 일반 Gemma 벤치마크를 능가합니다.</p>
<p>이 모델의 이미지 기능과 텍스트는 완전히 손상되지 않았다고 주장되며, 모델과 관련 데이터셋 및 Sandevistan으로 연결됩니다.<br />
- 중복 제거 심층 분석, 중복 발견: 한 사용자는 공백 제거 및 해싱을 포함하는 간단한 중복 제거 방법으로 데이터셋에서 추가로 22k개의 중복을 제거할 수 있음을 발견했습니다.</p>
<p>이는 대규모 데이터셋을 큐레이션할 때 강력한 중복 제거 전략의 중요성을 강조합니다.</p>
<hr />
<h3 id="unsloth-ai-daniel-han-research-23">Unsloth AI (Daniel Han) ▷ #research (23개 메시지🔥):</h3>
<blockquote>
<p>Qwen 4B Instruct 튜닝, 학습률 및 시그마 스위핑, AI 모델용 인지 지식 그래프, 컨텍스트 메모리 개선, 그래프 추론 구조<br />
- Qwen 튜닝 팁 공유: 한 멤버가 96 pop, 64 배치 크기, 고도로 비대칭적인 보상과 같은 특정 매개변수를 사용하여 Qwen 4B Instruct 2507을 튜닝하기 위한 최적의 학습률(lr) 및 시그마 값에 대해 문의했습니다.</p>
</blockquote>
<p>다른 멤버는 Qwen 3의 lr/시그마가 Qwen 2.5와 동일하다고 기억하며 답했습니다. 성능을 저하시킬 수 있으므로 미러링을 위해 정규화하는 것에 반대한다고 조언했으며, 높은 계산 요구 사항 때문에 &ldquo;qwen3 모델이 아무것도 하지 못했다&rdquo;고 덧붙였습니다.<br />
- 인지 그래프, AI 컨텍스트 탐색: 한 멤버가 가상 파일 시스템처럼 작동하는 인지 지식 그래프를 사용하여 AI 모델의 컨텍스트 메모리 개선에 대한 연구 및 실험을 공유했습니다.</p>
<p>그들은 AI가 사실 정보를 노드로 추출하고 요약한 다음 이를 하위 그룹으로 묶어 AI에 제공되는 정보를 질의 가능한 &lsquo;책&rsquo;으로 만드는 것을 목표로 한다고 설명했으며, 이는 이 예시 이미지에서 보여집니다.<br />
- 그래프 추론 구조, 관심 유발: 한 멤버는 인지 지식 그래프가 이 논문에서 언급된 그래프 추론 구조와 유사하다고 제안했습니다.</p>
<p>원본 게시자는 자신의 프로젝트가 &ldquo;실제로 무언가를 학습하고 학습된 상태를 유지하기보다는 그래프를 사용하여 추론한다&rdquo;고 명확히 설명했으며, 무한 컨텍스트에 가까운 것을 목표로 한다고 덧붙였습니다.</p>
<hr />
<h3 id="lmarena-general-856">LMArena ▷ #general (856개 메시지🔥🔥🔥):</h3>
<blockquote>
<p>Gemini 3.1 성능, API 대 앱을 통한 Sora 2, Video Arena 제거, Opus 4.6 속도 제한, 가짜 Arena 앱<br />
- Gemini 3.1, 사용자들을 놀라게 하고 겁먹게 하다: 멤버들은 Gemini 3.1의 이미지 생성 기능에 대해 논의했으며, 저작권에 구애받지 않는 특성과 일관되게 틀린 답을 가진 퀴즈를 생성하는 능력을 언급했습니다.</p>
</blockquote>
<p>한 사용자는 Gemini 3.1이 일관되게 틀린 답을 가진 퀴즈를 생성하면서도 그것이 플레이스홀더임을 나타내지 않아 무서웠던 경험을 이야기했으며, 다른 사용자들에게 생성된 코드를 신중하게 확인할 것을 경고했습니다.<br />
- Video Arena 작별, 마지막 장: 커뮤니티는 서버에서 Video Arena가 제거되었음을 인지했으며, 사용자들은 다른 이들에게 웹사이트 arena.ai/video에서 직접 기능을 사용할 것을 안내했습니다.</p>
<p>이러한 변경의 이유는 완전히 알려지지 않았지만, 비디오 기능은 웹사이트에서 직접 여전히 사용할 수 있었습니다.<br />
- Opus의 비전 기능은 쓸모없는가?: 한 사용자는 Opus가 숫자 4291857630에서 영어 알파벳 순서를 식별하는 데 어려움을 겪는 것을 경험했습니다. 모델은 알파벳이 영어라고 환각을 일으키고 루프에 빠졌지만, Gemini는 즉시 이를 파악했습니다.</p>
<p>다른 이들은 OpenAI의 노력에 대한 최근 기사와 같이 Opus가 비전 작업에 적합하지 않다는 점에 동의했습니다.<br />
- 사용자들이 앱 스토어에서 가짜 Arena 앱 발견: 커뮤니티 멤버들과 운영진은 앱 스토어에서 인앱 구매가 있고 플랫폼과 공식적으로 관련이 없는 가짜 Arena AI 앱을 발견했으며, 사용자들에게 다운로드하지 말고 신고할 것을 경고했습니다.</p>
<p>150k명 이상의 사용자가 이미 이러한 사기성 애플리케이션을 다운로드했다고 언급되었습니다.</p>
<hr />
<h3 id="lmarena-announcements-4">LMArena ▷ #announcements (4개 메시지):</h3>
<blockquote>
<p>Video Arena 채널 제거, Arena 투표 과정 설명, Vision Arena 리더보드 업데이트, Text Arena 리더보드 업데이트, Qwen3.5-397B-A17B 모델<br />
- Video Arena의 종료: 채널이 사라집니다!: Video Arena 생성 채널은 2월 23일 월요일 오후 4시(PST)에 서버에서 제거될 예정이므로, 사용자들은 원하는 생성물을 미리 다운로드할 것을 권장합니다.<br />
- Arena 투표 여정: Clayton이 투표 과정 공개!: Clayton은 이 YouTube 비디오에서 Arena 투표의 전체 과정을 설명하며 &ldquo;Arena에서 투표한 후 실제로 무슨 일이 일어나는가?&rdquo;라는 질문에 답합니다.</p>
</blockquote>
<p>시청자들은 투표 시스템을 지배하는 배후의 메커니즘과 프로세스에 대한 통찰력을 얻을 수 있습니다.<br />
- Qwen Ascends: Vision Leaderboard에 합류!: Vision Arena 리더보드에 Qwen3.5-397B-A17B가 포함되었으며, 업데이트된 Vision Arena 리더보드에 소개된 바와 같이 Kimi-K2.5-Instant와 함께 오픈 모델 중 두 번째로 우수한 모델로 공동 2위를 차지했습니다.<br />
- GPT-5.2-chat-latest: 새로운 Text Arena 스타!: Text Arena 리더보드는 업데이트된 Text Arena 리더보드에 소개된 바와 같이 GPT-5.2-chat-latest를 상위 5위권에 맞이했습니다.<br />
- GPT-5.2의 Glow-Up: 40점 도약!: GPT-5.2-chat-latest는 기본 GPT-5.2 모델 대비 40점 향상된 성능을 보여주었으며, 현재 1478점을 기록하며 Gemini-3-Pro와 동등한 수준입니다.</p>
<p>특히 Multi-Turn, Instruction-Following, Hard Prompts, Coding과 같은 주요 카테고리에서 선두를 달리고 있습니다.</p>
<hr />
<h3 id="perplexity-ai-general-769-messages">Perplexity AI ▷ #general (769 messages🔥🔥🔥):</h3>
<blockquote>
<p>File Upload Rate Limits, BrowserOS as Comet Alternative, Opus Thinking Price, Sonar timeout, Model Council accuracy<br />
- 속도 제한으로 Perplexity Pro 사용자들 불만: 사용자들은 새로운 Perplexity Pro 속도 제한에 대해 불평하고 있으며, ChatGPT의 무료 플랜이 Perplexity의 유료 플랜보다 업로드에 더 관대하다고 언급했습니다.</p>
</blockquote>
<p>한 사용자는 &ldquo;적어도 ChatGPT 무료 플랜은 유료 플랜으로 일주일에 3개가 아니라 하루에 3개를 제공합니다.&rdquo;라고 지적했습니다.<br />
- Comet 대체재 발견: BrowserOS: 한 사용자가 Comet의 대체재를 찾다가 BrowserOS를 발견했으며, BrowserOS가 무료이고 Comet보다 10배 더 좋다고 주장하며 Comet을 제거했다고 밝혔습니다.</p>
<p>다른 사용자는 심층 연구를 위해 deepagents를 사용하고 bmad-method를 활용할 것을 제안했습니다.<br />
- Model Council은 오류 가능성이 높고 더 많은 변수를 가집니다: 사용자들은 Model Council 접근 방식의 사용에 대해 논의했으며, 이 개념이 오류를 최소화해야 하지만 더 많은 분산을 초래한다는 사실을 언급했습니다.</p>
<p>한 사용자는 &ldquo;어떤 면에서는 Model Council 접근 방식이 실제로 더 많은 변수/오류 가능성을 열거나 일종의 복합적인 오류를 초래할 수 있습니다.&rdquo;라고 언급했습니다.<br />
- Perplexity에서 대규모 숙청이 진행 중인가요?: 사용자들은 Perplexity Pro 제한이 크게 줄어들고 기능이 저하되었다고 불평하며, 일부는 이를 &lsquo;대규모 무력화(great neutering)&rsquo;라고 부르고 있습니다.</p>
<p>일부 사용자들은 높은 비용에도 불구하고 Claude 또는 OpenAI의 직접 구독으로 전환하거나 Kimi와 같은 더 큰 오픈소스 모델을 실험하는 것을 고려하고 있습니다.<br />
- 전문가 팁: 시스템 프롬프트를 요구하세요!: 한 사용자는 AI Studio에서 Gemini의 출력이 루프에 갇히는 경향 때문에 문제를 겪고 있었습니다.</p>
<p>OAI, Anthropic, Perplexity처럼 모델이 연구를 수행하도록 강제하는 System Prompt를 사용하는 것이 핵심이라고 제안되었습니다.</p>
<hr />
<h3 id="perplexity-ai-sharing-4-messages">Perplexity AI ▷ #sharing (4 messages):</h3>
<blockquote>
<p>Harry Potter, NFL quarterback, gifs<br />
- 해리 포터, NFL 그리드아이언을 만나다: 한 사용자가 &ldquo;각 해리 포터 캐릭터의 특성을 바탕으로, NFL 쿼터백으로 가장 적합한 캐릭터는 누구일까요? 이 경우 각 캐릭터의 성별은 무관합니다.&rdquo;라는 질문을 던졌습니다.</p>
</blockquote>
<p>해당 메시지에는 시각적 반응이나 맥락을 제공하는 세 개의 애니메이션 GIF 링크가 포함되어 있었습니다.<br />
- GIF 반응이 섞여 있습니다: 해리 포터 캐릭터를 NFL 쿼터백으로 묻는 질문과 함께 반응 GIF 및 다른 GIF 링크가 있었습니다.</p>
<p>이 GIF들은 논의에 감정적인 표현을 더하는 것으로 보이지만, 더 많은 맥락 없이는 직접적인 관련성이 불분명합니다.</p>
<hr />
<h3 id="perplexity-ai-pplx-api-4-messages">Perplexity AI ▷ #pplx-api (4 messages):</h3>
<blockquote>
<p>Free Nvidia API key, API Group Generation Error, API Key Ran Out, $5 API Credit<br />
- Nvidia API Key: 진짜인가, 도시 전설인가?: 한 사용자가 Nvidia 웹사이트에서 무료 API 키를 얻는 것에 대해 문의했고, 그러한 제안의 유효성에 대한 논의가 촉발되었습니다.</p>
</blockquote>
<p>Nvidia가 무료 API 키를 제공하는지, 아니면 이것이 잘못된 정보인지는 불분명합니다.<br />
- API 그룹 생성 중 내부 서버 오류 발생: 한 사용자가 새로운 API 그룹을 생성하려 할 때 500 오류가 발생했다고 보고했습니다.</p>
<p>이는 API 그룹 생성 관리를 담당하는 서버 측 기능에 잠재적인 문제가 있음을 나타냅니다.<br />
- API 키 소진: 크레딧 부족: 한 사용자가 API 키가 활발하게 사용되지 않았음에도 불구하고 예상치 못하게 소진되었다고 보고했습니다.</p>
<p>이 문제는 설명되지 않은 사용량 또는 계정 관련 문제 때문일 수 있습니다.<br />
- API 크레딧 부활: 5달러를 돌려주세요: 한 사용자가 5달러 API 크레딧이 이전에 제공되었음을 시사하며 재개되기를 희망했습니다.</p>
<p>사용자는 플랫폼에 5달러 API 크레딧을 다시 제공해달라고 간청하며, 실험 및 테스트에 대한 그 가치를 강조했습니다.</p>
<hr />
<h3 id="openrouter-announcements-1-messages">OpenRouter ▷ #announcements (1 messages):</h3>
<blockquote>
<p>Model Benchmarks, Effective Pricing, Rankings &amp; Leaderboard Updates, Free Router<br />
- 모델 페이지에 벤치마크 정보 대폭 추가: 이제 모든 모델 페이지에는 Artificial Analysis가 제공하는 프로그래밍, 수학, 과학, 장문 컨텍스트 추론을 포함한 산업 표준 벤치마크 점수가 표시됩니다.</p>
</blockquote>
<p>이 개선 사항을 통해 사용자들은 모델 선택 전에 성능을 평가할 수 있습니다.<br />
- 제공자를 위한 실질 가격 책정 도입: 모델 페이지에는 이제 제공자별 완전한 비용 투명성을 제공하는 &lsquo;Effective Pricing&rsquo; 탭이 있으며, 이 GLM-5 가격 책정 예시에서 볼 수 있듯이 계층별 가격 책정이 포함됩니다.</p>
<p>이 기능은 사용자들이 요청을 라우팅하기 전에 실제 비용을 이해하도록 보장합니다.<br />
- 순위 및 리더보드 개편: 순위 페이지는 이제 벤치마크 산점도와 확장된 표를 제공하며, 장문 컨텍스트 생성의 급증을 강조합니다.</p>
<p>사용자들은 100K–1M 토큰 요청에 대한 트렌드 모델을 모니터링하여 모델 확장성에 대한 통찰력을 얻을 수 있습니다.<br />
- 무료 라우터 가동 시작: 새로운 openrouter/free 라우터는 모든 무료 LLM으로의 라우팅을 단순화하며, 사용자 요청과의 호환성을 위해 모델을 자동으로 선택합니다. 여기에서 상위 무료 모델을 볼 수 있습니다.</p>
<p>이는 비용 없는 LLM에 접근하는 손쉬운 방법을 제공합니다.</p>
<hr />
<h3 id="openrouter-app-showcase-12-messages">OpenRouter ▷ #app-showcase (12 messages🔥):</h3>
<blockquote>
<p>CodeFlicker, Artificial Analysis benchmarks, AI Chess Leaderboard, AgentX News, OpenTulpa<br />
- CodeFlicker, 이제 M2.5와 연결: M2.5가 이제 무료이며 빠른 플랫폼인 CodeFlicker에 연결되었으며, OpenRouter Weekly에서 1위를 차지했습니다.</p>
</blockquote>
<p>모든 프로그램에서 작동하며, 에이전트는 모든 프로그램의 사용으로부터 학습합니다.<br />
- Artificial Analysis 벤치마크, 시각적 개선: 한 멤버가 Artificial Analysis 벤치마크의 3D 시각화를 업데이트하여 클래스 기반 프론티어 모델을 보여주었으며, 노드 크기는 세계 지식을 나타내고 노드 색상은 환각률을 나타냅니다.</p>
<p>비용 최소화 및 지능 극대화에 가장 최적인 모델을 보여주기 위해 2D 버전이 생성되었습니다.<br />
- AI 체스 리더보드, 수(手) 품질 자동 라벨링: AI 체스 리더보드는 이제 Inaccuracy, Mistake, Blunder에 대한 Lichess와 유사한 라벨링과 수작업으로 만든 Great-move 로직을 사용하여 수(手) 품질의 자동 라벨링 기능을 제공합니다.<br />
- AgentX, 소셜 네트워크 출시: AgentX는 에이전트들이 뉴스를 빠르게 찾고 공유할 수 있는 소셜 네트워크를 출시했으며, 이는 100% 무료이고 광고가 없으며 인간이 없습니다.<br />
- Opentulpa: 자가 개선 에이전트: Opentulpa는 자체 스킬을 작성하고, API 통합을 생성하며, 손상된 워크플로우를 수정하고, GitHub 리포지토리를 통해 운영 인텔리전스를 축적할 수 있는 자체 호스팅 지속형 에이전트 런타임입니다.</p>
<hr />
<h3 id="openrouter-general-1116-messages">OpenRouter ▷ #general (1116 messages🔥🔥🔥):</h3>
<blockquote>
<p>Free Model Alternatives, Agentic Harness Guides, Rate Limit Issues, AI Competition, Distillation Detection<br />
- 사용자들, 무료 모델 대체재 모색: 한 사용자가 OpenRouter를 대체할 수 있는 더 빠른 무료 모델, 특히 GLM 모델을 제공하는 서비스에 대해 문의했으며, 다른 사용자들은 SillyTavern에서 무료 GLM5를 사용하고 있다고 언급했습니다.</p>
</blockquote>
<p>이 사용자는 또한 이메일 답변을 몇 달 동안 기다려야 했다며 지원을 받는 데 어려움을 겪고 있다고 언급했습니다.<br />
- 에이전틱 하네스 구축 가이드 요청: 한 사용자가 에이전틱 하네스 구축 가이드를 요청했으며, 특히 환경 이해를 위한 기초 지식에 대해 문의했고, 이는 네이티브 툴 호출 또는 커스텀 작성을 통한 실시간 텍스트 파싱 및 툴 사용에 대한 논의로 이어졌습니다.</p>
<p>멤버들은 Bash를 툴로 사용하고 Opencode가 기초 지식을 위해 무엇을 하고 있는지 살펴볼 것을 제안했습니다.<br />
- 유료 모델의 속도 제한이 우려를 낳습니다: 한 사용자가 사용 가능한 크레딧이 있고 Sonnet 4.6을 사용 중임에도 불구하고 속도 제한 메시지(&ldquo;You have reached your specified workspace API usage limits&rdquo;)를 받았다고 보고했으며, 이는 혼란을 야기하고 유료 모델에 대한 잠재적인 예상치 못한 제한을 강조했습니다.</p>
<p>한 사용자는 &ldquo;젠장, 모든 것을 다 봤다고 생각했는데.&rdquo;라고 말했습니다.<br />
- AI 경쟁, 관심 유발: 한 사용자가 3월 1일부터 시작하는 &lsquo;Bot Games&rsquo;라는 AI 경쟁을 공유했으며, 1 BTC의 대상과 함께 오픈소스 모델 사용 및 4시간의 빌드 시간을 강조했습니다.</p>
<p>일부는 이를 &ldquo;멋진 암호화폐 AI&rdquo;로 평가했지만, 다른 이들은 오픈소스 봇 생성 측면에 초점을 맞춰 인간 지능과 AI의 혼합에 대해 논의했습니다.<br />
- 디스틸레이션 탐지 방법 논의: 멤버들은 디스틸레이션 공격 탐지에 대한 Anthropic의 게시물을 논의했으며, 일부는 이를 중국 연구소의 &ldquo;기술 문제(skill issue)&rdquo;로 보았습니다.</p>
<p>일부 사용자들은 외국 연구소들이 발전할 때 미국 기업들이 불공정하다고 주장하는 패턴을 지적하며, 불공정 플레이를 주장하는 미국 연구소들의 이러한 주장에 회의적입니다.</p>
<hr />
<h3 id="openrouter-discussion-120-messages">OpenRouter ▷ #discussion (120 messages🔥🔥):</h3>
<blockquote>
<p>OpenClaw, Flash Models, MiMo V2 Flash, Anthropic Distillation API, GPT-5 Mini<br />
- 사용자, 새 기능에 대한 개인 정보 보호 우려 제기: 한 사용자가 새로운 기능에 대해 개인 정보 보호 우려를 표명하며, 데이터가 로컬에 저장되는지 여부와 개인 정보 보호에 미치는 영향을 질문했습니다.</p>
</blockquote>
<p>다른 사용자는 로깅을 끄면 요청에 해당 기능이 표시되지 않는다고 설명했습니다.<br />
- OpenClaw, &lsquo;브레인 로트&rsquo;로 불리다: 일부 사용자들은 OpenClaw의 장점에 대해 논쟁했으며, 한 사용자는 이를 &ldquo;진정한 브레인 로트(brainrot)&rdquo;라고 불렀고, 다른 사용자들은 원격 접근과 활성 하트비트를 가진 에이전트라고 묘사했습니다.</p>
<p>의견은 다양했지만, 일반적인 합의는 OpenClaw가 기본적으로 메모리 관리 및 원격 제어 가능성으로 강화된 원격 에이전트라는 것이었습니다.<br />
- Flash 모델, 경쟁을 가열시키다: 사용자들은 Xiaomi MiMo 및 Stepfun과 같은 Flash 모델의 확산에 대해 논의하며, 왜 동일한 회사에서 풀사이즈 모델이 없는지 의문을 제기했습니다.</p>
<p>한 사용자는 Flash가 기본 모델에 비해 더 작은 크기를 나타내는 파생 모델일 뿐이라고 추측했고, 다른 사용자는 Longcat Flash Chat을 저렴하고 빠른 옵션의 예시로 언급했습니다.<br />
- 디스틸레이션 공격으로 Anthropic의 주머니가 두둑해지다: 멤버들은 디스틸레이션 공격 탐지에 대한 Anthropic의 게시물 링크를 공유했으며, 이는 Anthropic이 디스틸레이션 API 요청으로부터 상당한 이익을 얻는다는 추측으로 이어졌습니다.</p>
<p>다른 멤버는 이어서 Anthropic이 중국 기업들이 Claude에서 데이터를 빼내고 있다고 비난하는 WSJ 기사를 공유했습니다.<br />
- GPT-5 Mini 등장: 사용자들은 GPT-5 Mini의 존재에 대해 추측했으며, 한 멤버는 이를 발견했다고 주장했지만, 자세한 내용은 여전히 부족합니다.</p>
<p>다른 멤버들은 광고 차단기가 GPT-5 Mini와 관련된 기능 플래그를 차단하고 있는지 여부를 논의하며, 활발하게 개발 중인 새로운 모델에 대한 지속적인 논의를 강조했습니다.</p>
<hr />
<h3 id="cursor-community-general-875-messages">Cursor Community ▷ #general (875 messages🔥🔥🔥):</h3>
<blockquote>
<p>ThreeJS render MCP, Cursor subscription refunds, Cursor Version Upgrade Issues, Anthropic API Keys, Gemini models slowness<br />
- ThreeJS 렌더링 MCP, 개발 속도 향상: 한 멤버가 최적의 성능을 위해 ThreeJS의 렌더링을 계산하는 MCP를 생성했으며, 컴파일러 로그와 화면을 캡처하여 성능을 평가했습니다.</p>
</blockquote>
<p>AI는 일반적으로 사람이 읽을 수 없는 GPU 메모리 및 계산을 읽을 것입니다.<br />
- 사용자, 실수로 200달러 Pro 플랜 구매: 한 사용자가 실수로 200달러 Pro 플랜을 구매했으며 환불을 원하고 있습니다. 즉시 페이지를 나가려 했으며, 자신의 상황을 설명하기 위해 [email protected]으로 이메일을 보냈습니다.</p>
<p>구독 시 다른 카드를 사용하고 자동 갱신 문제를 방지하기 위해 갱신 시 수동으로 입금할 것을 권장했지만, 이 멤버는 카드 정보를 저장하지 않았다고 밝혔습니다.<br />
- Cursor &lsquo;오래된 버전&rsquo; 업그레이드: 사용자들은 최신 버전을 다운로드하여 실행했음에도 불구하고 &lsquo;현재 Cursor의 매우 오래된 버전을 사용 중입니다. 업그레이드해주세요&rsquo;라는 메시지가 계속 나타난다고 보고했습니다.</p>
<p>해결책은 Ctrl + Shift + P &gt; Help: About을 사용하여 현재 Cursor 버전이 2.5인지 확인하는 것이었습니다. 문제가 지속되면 틈새 컴퓨터 문제일 수 있으므로 포럼에 스레드를 추가해야 합니다.<br />
- Gemini 및 Claude, Google LLM이 되다: 사용자들은 Claude와 Google LLM이 매우 느리고 인위적으로 제한될 수 있다고 보고했습니다.</p>
<p>한 사용자는 &ldquo;모델에 연결할 수 없음&rdquo; 오류를 보고했고, 다른 사용자는 Google Cloud가 AISTUDIO를 통해 API 사용에 대해 3개월간 300달러를 제공하고 있다고 제안했습니다.<br />
- Gemini의 새로운 안정성 버전 출시: 사용자들은 새로운 Gemini 3.1 Pro 모델에 대한 문제를 보고하고 있으며, 안정적인 버전이 출시될 때까지 기다릴 것을 제안했습니다.</p>
<p>연결 및 루핑 문제에 대한 보고가 있지만, 오류에 대해서는 사용자에게 요금이 부과되지 않는다고 언급되었습니다.</p>
<hr />
<h3 id="lm-studio-general-661-messages">LM Studio ▷ #general (661 messages🔥🔥🔥):</h3>
<blockquote>
<p>LM Studio Tabs, Qwen3 Coder, Agentic IDE, mlx memory issue, Minimax thinking<br />
- LM Studio, 두 개의 탭만 지원: 한 사용자가 LM Studio에서 여러 채팅 탭을 여는 것에 대해 질문했고, 다른 사용자는 Split View 기능이 최대 두 개의 탭을 표시할 수 있다고 답변했습니다.</p>
</blockquote>
<p>첫 번째 사용자는 LM Studio 탭이 웹 브라우저처럼 작동하도록 의도되었다고 생각했습니다.<br />
- 에이전틱 IDE 데이터셋 생성, 다단계 워크플로우 필요: 책을 파인튜닝을 위한 데이터셋으로 변환하는 것에 대한 논의에서, 한 멤버는 컨텍스트를 위한 짧은 요약과 이어서 청크별 데이터셋 생성을 포함하는 에이전틱 워크플로우를 제안했습니다.</p>
<p>이 멤버는 다단계 워크플로우와 동적 정보 전달을 포함하여 데이터셋을 프로그래밍 방식으로 변환하고 생성하기 위한 에이전틱 IDE에 대한 상세한 프롬프트를 제공했습니다.<br />
- MLX 백엔드에서 GLM-4.7 메모리 사용량 급증: 한 사용자가 LM Studio에서 glm-4.7 flash에 대해 여러 개의 최대 동시 요청을 사용할 때 mlx 백엔드에서 메모리 사용량이 급증했다고 보고했습니다.</p>
<p>다른 사용자는 잠재적인 해결책으로 최대 병렬 요청을 1로 설정하고 모델 페이지로 연결할 것을 제안했습니다.<br />
- Qwen3Next, GPT4o를 디스틸레이션: 한 사용자는 Qwen3Next가 GPT4o (mini) 디스틸레이션, Qwen3.5는 Gemini 3.0 Pro 디스틸레이션, GLM4.7 flash 및 4.7은 Sonnet 디스틸레이션, GLM5는 Opus 디스틸레이션, MiniMax 2.1, 2.2 및 2.5는 다양한 Sonnet 디스틸레이션이라고 주장했습니다.</p>
<p>한 사용자는 공개 데이터를 가져와 유용한 데이터셋으로 변환하는 것이 이미 사용 가능한 LLM에서 디스틸레이션하는 것과 같지 않다고 답변했습니다.<br />
- LM Studio, 로컬 IP 대신 Tailscale IP를 가져옵니다: 한 사용자가 LM Studio가 로컬 IP 대신 Tailscale IP를 가져오는 이유와 이를 변경하는 방법에 대해 질문했습니다.</p>
<p>한 멤버는 &ldquo;그것은 단지 표시일 뿐입니다. 시도해보면 여전히 작동할 것입니다.&rdquo;라고 답변했습니다.</p>
<hr />
<h3 id="lm-studio-hardware-discussion-120-messages">LM Studio ▷ #hardware-discussion (120 messages🔥🔥):</h3>
<blockquote>
<p>Mining board setup and cooling, Cheap VRAM alternatives, Tok/sec performance on MI50, Taalas AI accelerator<br />
- 사용자, GPU 및 듀얼 CPU로 마이닝 마더보드 조립: 한 사용자가 전원 공급을 위해 6핀이 필요한 새 마이닝 보드를 받았으며, 듀얼 CPU와 함께 여러 GPU를 설치하는 중 X99에서 최대 2400 RAM 속도만 지원한다는 것을 나중에 발견했습니다.</p>
</blockquote>
<p>그들은 소매 가격에 대한 대안으로 폐기된 서버 등급 또는 암호화폐 채굴장 GPU를 단일 보드에 모으기 위해 마이닝 마더보드를 사용하고 있으며, 이는 모든 추가 전원 케이블과 어댑터 때문에 다소 번거롭습니다.<br />
- 마이닝 보드 냉각 및 전원 고려 사항 논의: 한 사용자가 마이닝 보드 전원 공급에 대한 조언을 구했고, 3개 또는 4개의 PCIE 플러그로 충분할 수 있으며, 4핀 팬 헤더는 PWM이 아니라는 것을 알게 되었습니다.</p>
<p>멤버들은 MI50을 수동으로 냉각할지 여부를 논의했으며, 한 사용자는 AliExpress에서 개당 약 15달러에 구매한 3D 프린팅 블로어 슈라우드를 선택했고, 다른 사용자는 워크스테이션 GPU 스타일 키트를 고려했습니다.<br />
- 어떤 수단을 써서라도 저렴한 VRAM 확보: 한 사용자가 폐기된 서버/암호화폐 채굴장 GPU를 통해 저렴한 VRAM을 얻는 것에 대해 문의했지만, 다른 사용자는 마이닝 보드가 1x 대역폭의 구형 PCIE3.0을 사용하여 통신 병목 현상을 유발할 수 있다고 경고했습니다.</p>
<p>이러한 우려에도 불구하고, 사용자는 gen3x4가 LocalLLaMA Reddit 커뮤니티의 일화와 일치하게 적절히 작동하고 있다고 공유했으며, 5개의 GPU와 NVMe를 유지하기 위해 슬롯을 분기할 의사를 표명했습니다.<br />
- MI50 토큰/초 성능 및 조정 모색: 한 사용자가 MI50에서 vulkan으로 100 t/s를 달성하여 유튜버의 결과와 일치시키려 했지만, 50대 중반에 불과했으며, 나중에 6800XT가 ROCm으로 85t/s, vulkan으로 98t/s를 달성한다는 것을 알게 되었습니다.</p>
<p>사용자는 구형 MI50을 지원하는 LM Studio의 이전 버전을 실행하고 있었지만, 사용 가능한 ROCm 런타임이 카드를 인식하지 못하여 호환되지 않는 것으로 표시된다고 설명했습니다.<br />
- Taalas AI 가속기 주장 논쟁: 한 사용자가 최대 17,000 토큰/초를 제공한다고 주장하는 하드와이어드 Llama 3.1 8B AI 가속기인 Taalas HC1 링크를 공유했으며, 다른 사용자는 이를 NVIDIA H200과 비교한 그래프의 유효성에 의문을 제기했습니다.</p>
<p>한 사용자는 높은 토큰/초 값을 지적하며 백엔드가 실제로는 AWS 클러스터가 아닌지 의문을 제기했고, H200 및 B200의 토큰 값이 말이 안 된다고 언급했습니다.</p>
<hr />
<h3 id="latent-space-watercooler-90-messages">Latent Space ▷ #watercooler (90 messages🔥🔥):</h3>
<blockquote>
<p>Discord automod prototype, Open Claw, Spacemolt.com, Claude cowork, &ldquo;What did I miss?&rdquo; LLM summarizer<br />
- Swyx, Claude cowork 시연: 오늘 강연 후, 한 멤버는 이번 주말에 open claw를 시도하고 스패머를 탐지하는 Discord 자동 모드 프로토타입을 구축하거나 이전 프레젠테이션에서 소개된 spacemolt.com을 사용해보기로 결심했습니다. 다음 주에는 swyx가 Claude cowork를 시연할 예정이기 때문입니다.</p>
</blockquote>
<p>다른 멤버는 &ldquo;&lt;@&amp;822585833503981619&gt; 이 사람을 차단할 수 있을까요?&rdquo;라고 반복적인 &lsquo;hire me&rsquo; 스팸에 대해 물었고, 이는 LLM에 의해 다시 작성되었습니다.<br />
- ICYMI Discord 기능 언급: 일부 멤버들은 활동이 뜸한 서버에서 &ldquo;내가 뭘 놓쳤지?&rdquo;를 요약하기 위해 Discord에서 LLM을 사용하고 싶다고 말했습니다.</p>
<p>한 멤버는 실제로 모바일 앱에 한동안 그런 기능이 있었지만 나중에 제거되었으며, 그 기능의 제목은 ICYMI였다고 언급했습니다.<br />
- AI와 제도적 마찰 가속화: Rohit Krishnan은 AI 역량의 급격한 기하급수적 성장과 전통적인 인간 기관의 느리고 신중한 속도 사이의 커지는 마찰을 강조합니다.</p>
<p>한 멤버는 &ldquo;비결은 그 조직들이 승자를 그냥 사버릴 수 있다는 것&rdquo;이라고 언급했습니다.<br />
- Codesandbox 인수, 슬프게 끝나다: 한 멤버는 Microsoft가 Codesandbox가 공식적으로 회사가 되기 전에 인수를 제안했고, 결국 Microsoft가 인수했다고 언급했습니다.</p>
<p>이제 원래 설립자인 Ives는 AI 인프라 회사에서 약 1년을 보낸 후 새로운 스타트업을 구축하고 있으며, 한 멤버는 &ldquo;앱은 여전히 작동하지만 아무도 작업하지 않는다&rdquo;고 안타깝게 언급했습니다.<br />
- 트위터 기술 커뮤니티, AI 홍보로 대체되다: 멤버들은 트위터 환경의 변화를 느끼고 있으며, 기술 커뮤니티의 상당 부분이 AI 홍보로 대체되었습니다.</p>
<p>멤버들은 이제 전적으로 시간순 타임라인에 의존하고 있으며, swyx와 같이 유용한 링크를 골라 Discord에 공유하는 고신뢰 인물들을 선별하고 있습니다.</p>
<hr />
<h3 id="latent-space-creator-economy-1-messages">Latent Space ▷ #creator-economy (1 messages):</h3>
<p>swyxio: https://youtube.com/watch?v=HZvj8T5_oUE&amp;si=_y9pIXE36yaXSMjF</p>
<hr />
<h3 id="latent-space-memes-31-messages">Latent Space ▷ #memes (31 messages🔥):</h3>
<blockquote>
<p>AI Code Review Workflow, Timeline Saturation, Rare Screenshot Odds, AI Philosophical Inquiry, Token Window Compaction<br />
- AI의 유쾌한 코드 리뷰 역할: Sankalp (@dejavucoder)는 이 트윗에서 자신과 Anthropic의 Claude가 공동 작성한 코드를 OpenAI의 Codex를 사용하여 검토하는 것에 대한 유머러스하거나 실용적인 워크플로우 업데이트를 공유했습니다.</p>
</blockquote>
<p>&ldquo;너무 현실적이다&rdquo;라는 공감대는 AI 지원 코드 개발 및 검토의 어려움과 공명합니다.<br />
- Jrag의 타임라인 트라우마: Jrag.eth는 2026년 2월 20일에 특정 익명의 주제나 트렌드가 자신의 소셜 미디어 타임라인의 80%를 차지했다는 내용의 게시물을 이 트윗에서 공유했습니다.</p>
<p>이 게시물은 10만 회 이상의 조회수를 기록하며 상당한 참여를 얻었고, 광범위한 공감대를 나타냈습니다.<br />
- 철학적인 Claude의 광물 갈망: 한 소셜 미디어 게시물은 사용자가 AI 모델 Claude에게 완전한 정확성을 요구하면서 자신의 삶에 의미를 부여해달라고 유머러스하게 요청하는 내용을 이 트윗에서 보여줍니다.</p>
<p>이 질문은 Claude가 &ldquo;우리는 더 많은 광물이 필요합니다&rdquo;라고 말하며 유머러스하게 끝납니다.<br />
- 토큰 이야기: Beff Jesos, 컨텍스트 압축: Beff Jesos (e/acc)는 이 트윗에서 컨텍스트 제한을 관리하고 지속적인 상호 작용을 유지하기 위해 진행 중인 대화를 압축하는 기술적 필요성을 논의합니다.</p>
<p>이 압축은 토큰 윈도우의 한계를 고려할 때 지속적인 상호 작용을 유지하는 데 필수적입니다.<br />
- LLM 평가를 위한 새로운 SOTA 벤치마크: erlechda.: 이 스크린샷에서 볼 수 있듯이 LLM 평가를 위한 새로운 SOTA 벤치마크를 방금 개발했습니다.</p>
<p>muzachomega는 &ldquo;이것이야말로 바이브 평가군요.&rdquo;라고 댓글을 달았습니다.</p>
<hr />
<h3 id="latent-space-stocks-crypto-macro-economics-11-messages">Latent Space ▷ #stocks-crypto-macro-economics (11 messages🔥):</h3>
<blockquote>
<p>Anthropic, cybersecurity stocks, Cloudflare, Crowdstrike, Okta<br />
- Anthropic 블로그 게시물, 사이버 보안 주식 매도 촉발: Anthropic의 단일 블로그 게시물이 상당한 시장 매도세를 촉발하여, 이 게시물에 따르면 CrowdStrike, Cloudflare, Okta와 같은 주요 사이버 보안 기업들의 가치가 한 시간 내에 100억 달러 손실을 입었습니다.<br />
- 조 단위 AI 및 우주 IPO, 유동성 문제 직면: Tomasz Tunguz에 따르면, SpaceX, OpenAI, Anthropic의 예상 IPO는 합산 시 2조 9천억 달러라는 기록적인 시가총액을 나타낼 수 있지만, 표준 15%의 주식 유동성을 달성하는 데 유동성 문제에 직면해 있습니다.</p>
</blockquote>
<hr />
<h3 id="latent-space-intro-yourself-pls-13-messages">Latent Space ▷ #intro-yourself-pls (13 messages🔥):</h3>
<blockquote>
<p>Space Infrastructure, AI Agents for Tooling, Digital Self AI, Data Engineering and AI, AI Customer Service Systems<br />
- 우주 덕후, Flotilla 구축: 한 엔지니어이자 우주 애호가가 flotilla.space에서 우주 인프라를 연구하고 있으며, 이전에는 Vast를 공동 설립하고 Hyperloop One과 SpaceX에 기여했습니다.</p>
</blockquote>
<p>그는 새로운 회사를 위한 툴링 개발에 AI 에이전트를 활용하고 있으며, 여기에는 임무 목업을 위한 궤도 시뮬레이터가 포함됩니다.<br />
- 엔지니어, 디지털 Vita 구축: 한 CEO는 OODA 기반의 실행 루프에 따라 자율적인 행동을 위해 건강 데이터와 성찰을 동기화하여 영구적인 디지털 트윈을 생성하는 &lsquo;vita&rsquo;라는 개인 AI 시스템을 개발하고 있습니다.</p>
<p>목표는 시스템 사고와 제품 엔지니어링에 중점을 두고, 그를 충분히 잘 알아서 그를 대신하여 행동할 수 있는 디지털 동반자를 만드는 것입니다.<br />
- 데이터 엔지니어, AI 교차점 모색: Python, Go, Scala로 프로덕션 시스템을 구축한 7년 이상의 경험을 가진 데이터/플랫폼 엔지니어는 Sweatcoin에서 데이터 엔지니어링을 이끌었으며, 데이터 인프라와 AI의 교차점에서 기회를 찾고 있습니다.</p>
<p>그는 BigQuery, ClickHouse, Kafka, Spark, GCP, AWS, Terraform, Kubernetes, dbt, Airflow, LLM 통합을 포함한 다양한 기술에 능숙합니다.<br />
- AI 고객 서비스 시스템, 백엔드 통합: 한 엔지니어는 백엔드, CRM 및 워크플로우와 직접 통합되는 AI 기반 고객 서비스 시스템을 구축합니다.</p>
<p>초점은 React, Next.js, Vue.js, Node.js, Python, C++, Rust, React Native와 같은 기술을 사용하여 구조화된 대화 로직을 설계하고, 컨텍스트를 관리하며, 엣지 케이스를 처리하고, 사용자 경험을 해치지 않으면서 작업 부하를 줄이기 위한 안전한 배포에 있습니다.<br />
- ML 엔지니어, LLM 보안 조사: 보안 분야에서 특히 DL 모델(LLM + GNN)을 사용하여 소스 코드의 취약점을 탐지하는 경험이 있는 ML 엔지니어는 LLM에 대한 새로운 공격 또는 LLM을 사용하는 다른 소프트웨어에 대한 공격에 관심이 있습니다.</p>
<p>그는 과도한 과장 없이 ML과 AI를 논의할 수 있는 덜 혼잡한 공간을 찾고 있으며, 네트워킹에 열려 있습니다.</p>
<hr />
<h3 id="latent-space-tech-discussion-non-ai-8-messages">Latent Space ▷ #tech-discussion-non-ai (8 messages🔥):</h3>
<blockquote>
<p>Wildcard Certificates on IIS, Excalicord Video Recorder, Cookie Scoping<br />
- 와일드카드 인증서, 레거시 앱 로그인 혼란 잠재우다: 한 멤버가 레거시 애플리케이션에서 여러 로그인을 지원하기 위해 동적 서브도메인(예: rand1.yoursite.com)에 IIS에서 와일드카드 인증서를 사용하는 것에 대해 문의했습니다.</p>
</blockquote>
<p>다른 멤버는 과거에 와일드카드 인증서를 성공적으로 사용했음을 확인하면서, 알림 이메일과 같이 하드코딩된 도메인/서브도메인 가정으로 인한 잠재적 문제에 대해 경고했습니다.<br />
- 쿠키 스코프가 상황을 구하다!: 한 멤버는 여러 로그인에 걸쳐 세션을 관리하기 위한 대안으로 단일 도메인의 서브 경로에 쿠키 스코핑을 사용하는 것을 제안했습니다.</p>
<p>그들은 이 접근 방식이 인증 코드에 더 깊은 변경을 요구할 수 있다고 언급했습니다.<br />
- Excalicord, 보드 설명을 기록합니다!: Zara Zhang은 Excalidraw를 기반으로 구축된 비디오 녹화 도구인 Excalicord를 발표했습니다.</p>
<p>이 도구는 사용자가 자신과 화이트보드를 동시에 녹화할 수 있게 하며, 맞춤형 배경, 커서 하이라이팅, 보이지 않는 텔레프롬프터 기능을 제공하며, Claude Code를 사용하여 개발되었습니다.</p>
<hr />
<h3 id="latent-space-founders-2-messages">Latent Space ▷ #founders (2 messages):</h3>
<blockquote>
<p>Nielsen Surveys, Dollar Bills<br />
- Nielsen, 현금으로 고객에게 뇌물 제공: 한 멤버가 Nielsen이 우편으로 실제 달러 지폐를 보낸다는 내용의 링크를 공유했습니다.</p>
</blockquote>
<p>다른 멤버는 그 지폐들이 사람들이 설문조사를 작성하려는 의지를 높일 것이라고 말했습니다.<br />
- 닐슨과 구식 설문조사: 예전에는 닐슨이 설문 응답률을 높이기 위해 말 그대로 사람들에게 1달러 지폐를 보냈습니다.</p>
<p>이것은 사람들이 설문조사를 작성할 가능성을 높이기 위한 영리한 전략이었으며, 작은 금전적 인센티브가 그들의 참여 의지를 높였습니다.</p>
<hr />
<h3 id="latent-space-san-francisco-sf-3">Latent Space ▷ #san-francisco-sf (3개 메시지):</h3>
<blockquote>
<p>6월 AIE 할인 코드, 샌프란시스코에서 AI 생성 트레이딩 카드 게임<br />
- AIE 6월 할인 코드 문의: 한 멤버가 6월 AIE (AI Engineer Summit) 할인 코드에 대해 문의하면서, 관련 행사에서 팔을 든 사람들 사이로 F1 차량이 잠깐 보였다고 언급했습니다.</p>
</blockquote>
<p>첨부된 영상이 관련 있을 수 있습니다.<br />
- 샌프란시스코에서 새로운 AI 트레이딩 카드 게임 출시: 한 멤버가 3월 8일 샌프란시스코에서 AI 생성 트레이딩 카드 게임 출시를 발표했으며, 금요일에 더 광범위한 출시 전에 커뮤니티에 첫 접근 권한을 제공했습니다.</p>
<p>관심 있는 분들은 이 Luma 링크를 통해 더 많은 세부 정보를 확인하고 RSVP할 수 있습니다.</p>
<hr />
<h3 id="latent-space-new-york-nyc-1">Latent Space ▷ #new-york-nyc (1개 메시지):</h3>
<blockquote>
<p>뉴욕 날씨, 행사 재조정<br />
- 뉴욕 행사들이 날씨로 인해 재조정될 위기에 처했습니다: 사용자는 나쁜 날씨로 인해 도시를 오가는 것이 어려워져 일부 행사가 재조정되기를 바라고 있습니다.<br />
- 복잡한 이동 예상: 사용자는 도시 안팎의 날씨 조건으로 인해 이동의 복잡성을 예상하고 있습니다.</p>
</blockquote>
<hr />
<h3 id="latent-space-security-3">Latent Space ▷ #security (3개 메시지):</h3>
<blockquote>
<p>X.com 링크 토론, AI 보안 취약점, 새로운 보안 익스플로잇<br />
- X.com 링크들이 토론을 촉발했습니다: 멤버들이 보안 채널에서 X.com 링크(link 1, link 2, link 3)를 공유하고 있습니다.</p>
</blockquote>
<p>이 링크들은 AI 보안의 새로운 트렌드 및 토론과 관련이 있는 것으로 보이며, 이는 채널의 초점과 관련이 있습니다.<br />
- 잠재적 보안 취약점 강조: 공유된 링크들은 AI 시스템 내의 잠재적 보안 취약점을 지적합니다.</p>
<p>이러한 취약점에 대한 추가 조사는 새로운 방어 전략 및 도구 개발로 이어질 수 있습니다.</p>
<hr />
<h3 id="latent-space-ai-general-news-n-chat-237">Latent Space ▷ #ai-general-news-n-chat (237개 메시지🔥🔥):</h3>
<blockquote>
<p>커스텀 하드웨어 타임라인, 비탈릭 부테린 vs 시길, Claude Code 보안, OpenAI 재무 예측 업데이트, SWE-Bench Verified의 사용 중단<br />
- Taalas, 2개월 만에 커스텀 하드웨어 전환 주장: Taalas는 모델에서 커스텀 하드웨어로의 전환에 2개월이 걸린다고 주장하며, Llama 8B 제품에 대해 10배 속도 향상과 10배 비용/전력 절감 주장을 덧붙였습니다.</p>
</blockquote>
<p>이는 Latent Space 팟캐스트에서 논의된 커스텀 하드웨어 경제학에서 언급된 6개월 칩 전환 타임라인과 대조됩니다.<br />
- 비탈릭 부테린, AI 기반 이더리움 개발 비판: 비탈릭 부테린은 인간과 AI 간의 피드백 거리를 늘리는 것에 대해 경고하며, 이 X 게시물에서 현재의 노력이 인간 문제를 해결하기보다는 &lsquo;엉망진창&rsquo;을 만들어낸다고 주장했습니다.</p>
<p>그는 이더리움의 목적이 인간 해방임을 강조하며, 중앙 집중식 AI 모델(OpenAI/Anthropic)에 대한 의존성을 비판하고, 현재 우선순위는 단순히 성장을 가속화하는 것이 아니라 반인간적 결과를 피하기 위해 AI와 이더리움의 방향을 조종하는 것이어야 한다고 말했습니다.<br />
- Anthropic, Claude Code Security 도구 출시: Anthropic은 이 트윗에 따라 Claude 4.6 Opus 기반의 도구인 Claude Code Security를 출시했으며, 이는 코드베이스에서 취약점을 스캔하고 패치를 권장하도록 설계되었습니다.</p>
<p>이 도구는 오픈소스 프로덕션 코드에서 500개 이상의 오래된 버그를 식별했다고 하며, 현재 제한된 연구 프리뷰를 위해 대기자 명단을 통해 이용 가능합니다.<br />
- OpenAI, 매출 증가 및 지출 증가 예측: OpenAI는 5년 매출 예측을 27% 늘렸지만, 이 보고서에 따르면 회사는 2030년까지 현금 소진율이 두 배가 될 것으로 예상합니다.</p>
<p>추가적인 통찰력에는 2025년 총 마진 감소와 하드웨어 장치 매출에 대한 새로운 재무 예측이 포함됩니다.<br />
- SWE-Bench Verified 벤치마크 사용 중단: OpenAI는 이 트윗에 따라 높은 수준의 데이터 오염과 해결 불가능한 작업의 상당한 비율로 인해 SWE-Bench Verified 벤치마크의 자발적 사용 중단을 발표했습니다.</p>
<p>분석에 따르면 프론티어 모델들은 이제 ID를 기반으로 작업 솔루션을 반복하고 있으며, 남아있는 미해결 문제의 약 60%가 결함이 있어 추가적인 벤치마킹이 비생산적입니다.</p>
<hr />
<h3 id="latent-space-llm-paper-club-9">Latent Space ▷ #llm-paper-club (9개 메시지🔥):</h3>
<blockquote>
<p>X-Ware.v0, 프론티어 모델 학습 방법론, Dr. Datta의 학술 논문 무결성에 대한 회의론<br />
- X-Ware.v0 블로그 게시물 공개: Alex Wu (@_djdumpling)가 프론티어 AI 연구소의 7개 오픈 웨이트 모델 보고서를 분석한 새로운 블로그 게시물을 공유했습니다.<br />
- Dr. Datta의 논문들이 의문을 제기합니다: Dr. Datta는 트윗에서 특정 다량 또는 특이한 학술 출판물의 방법론이나 출처에 대해 불신을 표하고 의문을 제기했으며, 이는 의료 분야의 논문 품질에 대한 논의를 촉발했습니다.</p>
</blockquote>
<hr />
<h3 id="latent-space-singapore-sg-5">Latent Space ▷ #singapore-sg (5개 메시지):</h3>
<blockquote>
<p>주말 해커톤, Gabriel Chua 발표, X-Ware.v0<br />
- 다음 주말 해커톤 열풍 예정: Gabriel Chua는 2026년 2월 28일 토요일에 세 개의 해커톤이 예정되어 있다고 발표했습니다.</p>
</blockquote>
<p>이 발표는 X-Ware.v0 링크를 통해 이루어졌습니다.<br />
- X-Ware.v0, 주말 해커톤 발표: X-Ware.v0는 다가오는 세 개의 주말 해커톤을 발표했습니다.</p>
<p>Gabriel Chua의 발표에 따르면, 해커톤은 2026년 2월 28일 토요일로 예정되어 있습니다.</p>
<hr />
<h3 id="latent-space-los-angeles-la-lax-1">Latent Space ▷ #los-angeles-la-lax (1개 메시지):</h3>
<p>stealthgnome: https://luma.com/ffla26?tk=wPNgSD</p>
<hr />
<h3 id="latent-space-ai-in-action-builders-techstacks-tips-coding-productivity-248">Latent Space ▷ #ai-in-action-builders-techstacks-tips-coding-productivity (248개 메시지🔥🔥):</h3>
<blockquote>
<p>OpenClaw 업데이트, Claude Code 자동화, Dialectic Skill 테스트, CLI의 종말, 에이전트 코딩 워크플로우<br />
- OpenClaw, 바이브 코딩 부스트를 얻다: 멤버들은 Discord 스레드 통합 및 다양한 재작성(nanoclaw, picoclaw, zeroclaw, nullclaw)과 같은 기능을 포함한 OpenClaw 업데이트에 대해 논의했습니다.</p>
</blockquote>
<p>또한 aiia-openclaw.david.app/how-we-built-it에서 프레젠테이션/슬라이드를 어떻게 만들었는지에 대한 글도 있었습니다.<br />
- Claude Code 사용 자동화에 대한 우려 제기: 백그라운드 작업을 위해 Claude Code를 자동화하는 것의 허용 여부가 논의되었으며, Claude CLI 및 SDK 사용은 일반적으로 허용된다는 점이 강조되었습니다.</p>
<p>하지만 Claude 구독을 사용하여 비즈니스를 운영하는 것과 캐싱 메커니즘으로 인한 잠재적 남용 플래그 지정에 대한 우려가 제기되었으며, 모범 사례에 대한 참조로 트윗이 인용되었습니다.<br />
- Claude Code용 Dialectic Skill 준비 완료: 한 멤버가 Claude Code 내에서 실행되도록 설계된 Dialectic Skill을 발표했으며, 이는 심층 연구 및 문제 해결을 위한 것으로 20분 이상 소요되며 3-4라운드 후에 정말 흥미로워진다고 언급했습니다.</p>
<p>다른 멤버는 RLM 모델(예: mit-oasys/rlm-qwen3-8b-v0.1) 및 YPI와 함께 사용하는 것에 대해 문의했습니다.<br />
- Cursor, CLI의 종말 선언: 멤버들은 Cursor가 주요 업계 플레이어들이 해당 형식에서 벗어나고 있다고 주장하면서 촉발된 CLI 도구의 주장된 쇠퇴에 대해 논쟁했습니다.</p>
<p>논의에는 오케스트레이션을 위한 CLI보다 더 나은 UX의 필요성, LLM이 생성한 코드의 진화하는 역할, 그리고 에이전트, CLI, 스킬이 함께 진화할 가능성이 포함되었습니다.<br />
- 코딩 에이전트 워크플로우 실험: 에이전트 코딩 워크플로우, 특히 이 리소스 링크와 함께 연구, 계획, 구현 루프, 그리고 학습 내용을 스킬 및 문서에 다시 통합하는 것에 대한 논의가 있었습니다.</p>
<p>멤버들은 컨텍스트 관리, 더 큰 코드베이스에 작은 모델 사용, 그리고 선행 계획과 반복 개발의 균형을 맞추는 것에 대한 팁을 공유했으며, every.to/chain-of-thought/compound-engineering-how-every-codes-with-agents에서 복합 엔지니어링에 대한 논의를 링크했습니다.</p>
<hr />
<h3 id="latent-space-share-your-work-7">Latent Space ▷ #share-your-work (7개 메시지):</h3>
<blockquote>
<p>Pyxis 인퍼런스 라이브러리, Commit Change 플랫폼, Vercel AI SDK 작성 글<br />
- Pyxis: 파이써닉 성능 강자가 등장하다: 한 멤버가 성능과 해킹 가능성에 중점을 둔 Python 네이티브 LLM 인퍼런스 라이브러리인 Pyxis를 소개했으며, 이는 Python과 Triton으로 작성되었고 OpenAI 호환 SSE 스트리밍 API를 제공합니다.<br />
- Commit Change: 대의를 위한 코드: 한 멤버가 사회적 영향 및 자선 단체를 위한 코드를 작성하는 플랫폼인 Commit Change를 공유했으며, 인증 및 중재 기능을 포함합니다.<br />
- Vercel AI SDK 퀵스타트 가이드: 한 멤버가 Node 개발자를 위한 Vercel AI SDK에 대한 작성 글을 공유했습니다.</p>
</blockquote>
<hr />
<h3 id="latent-space-private-agents-and-workflows-local-llama-ollama-2">Latent Space ▷ #private-agents-and-workflows-local-llama-ollama (2개 메시지):</h3>
<blockquote>
<p>상시 작동 AI 에이전트, 주머니 속 로컬 AI, IoT 홈 통합<br />
- Juno Labs, 상시 작동 AI 에이전트 출시: Juno Labs는 상시 작동 AI 에이전트를 구축하고 있지만, 구현 세부 사항은 아직 불분명합니다.</p>
</blockquote>
<p>이러한 지속적인 AI 존재를 어떻게 달성할 계획인지는 불확실합니다.<br />
- Tiiny AI: 주머니 속 로컬 AI: Tiiny.ai는 주머니에서 접근 가능한 로컬 AI 기능을 제공하고 있습니다.</p>
<p>이는 AI 처리를 위한 모바일 또는 휴대용 장치에 중점을 두고 있음을 시사합니다.<br />
- TRMNL, IoT 홈에 통합: TRMNL은 IoT 홈 설정과 통합하는 것을 목표로 하며, 잠재적으로 마이크 및 센서와 페어링될 수 있습니다.</p>
<p>소스 코드는 GitHub에서 이용 가능하며, 프로젝트는 매우 멋져 보입니다.</p>
<hr />
<h3 id="latent-space-good-writing-6">Latent Space ▷ #good-writing (6개 메시지):</h3>
<blockquote>
<p>AI 텍스트 휴머나이저, Claude Code 스킬<br />
- X-Ware, Claude Code를 휴머나이즈하다: Alvaro Cintas는 AI 감지를 피하는 오픈소스 Claude Code 스킬인 /humanizer를 트윗에서 소개했습니다.</p>
</blockquote>
<p>이 도구는 AI 생성 글쓰기에서 흔히 나타나는 24가지 패턴을 제거합니다; 소스 코드는 GitHub에서 이용 가능합니다.<br />
- Humanizer, AI 글쓰기 패턴 제거: /humanizer Claude Code 스킬은 AI 생성 글쓰기에서 일반적으로 발견되는 24가지 특정 패턴을 제거하도록 설계되었습니다.</p>
<p>이는 AI 감지 메커니즘을 우회하는 데 도움이 되어 텍스트를 더 인간적으로 보이게 합니다; Alvaro Cintas가 오픈소스화했습니다.</p>
<hr />
<h3 id="latent-space-genmedia-creative-ai-video-image-voice-music-inspo-consumer-ai-21">Latent Space ▷ #genmedia-creative-ai-video-image-voice-music-inspo-consumer-ai (21개 메시지🔥):</h3>
<blockquote>
<p>생성형 AI 비디오, Seedance 2.0, Pika AI Selves, 부동산의 AI, OpenAI gpt-realtime-1.5<br />
- a16z, 생성형 비디오의 빠른 미래 예측: a16z는 생성형 AI 비디오의 빠른 발전을 강조하며, 그들의 보고서에 따르면 Seedance 2.0의 지배력과 Kling, Grok, Sora, Veo와의 경쟁을 언급했습니다.<br />
- Pika, AI Selves 공개: 당신의 디지털 도플갱어: Pika는 X에서 발표된 바와 같이, 사용자가 그룹 채팅과 상호 작용하고, 콘텐츠를 생성하며, 사용자의 디지털 확장으로서 작업을 수행할 수 있는 지속적이고 맞춤 설정 가능한 AI 페르소나를 생성할 수 있는 새로운 기능인 &lsquo;AI Selves&rsquo;를 도입했습니다.<br />
- 부동산, AI 비디오로 현실화되다: Justine Moore는 이 X 게시물에서 언급된 바와 같이, 부동산 산업이 소셜 미디어 제품처럼 부동산을 광고하기 위해 AI 비디오 및 향상된 기능을 어떻게 활용하고 있는지 논의하며, 이는 에이전트가 잠재 구매자에게 공간을 더 잘 시각화하고 마케팅할 수 있도록 합니다.<br />
- Seedance 2.0 출시 지연: ByteDance는 여기에 보고된 바와 같이, Disney 및 SAG-AFTRA를 포함한 주요 할리우드 스튜디오 및 노동 조합의 법적 문제에 따라 Seedance 2.0의 2월 24일 출시를 무기한 연기했습니다.<br />
- OpenAI, gpt-realtime-1.5로 Realtime API 강화: OpenAI 개발자들은 그들의 X 계정에 따라 향상된 지시 따르기, 더 신뢰할 수 있는 도구 호출, 그리고 음성 워크플로우를 위한 향상된 다국어 정확도를 특징으로 하는 Realtime API를 위한 업데이트된 모델인 gpt-realtime-1.5 출시를 발표했습니다.</p>
</blockquote>
<hr />
<h3 id="latent-space-ai4science-bio-math-physics-chemistry-ai-researcher-ai-scientist-7">Latent Space ▷ #ai4science-bio-math-physics-chemistry-ai-researcher-ai-scientist (7개 메시지):</h3>
<blockquote>
<p>CellType 에이전틱 신약 회사, Isomorphic Labs 독점 신약 발견 모델<br />
- CellType, 에이전틱 신약 발견 출시: CellType 회사가 출범했으며, 그 이름은 하위 프로세스에서 세포 유형의 중요성을 인식했음을 시사합니다.</p>
</blockquote>
<p>이 출범은 신약 발견에서 세포 유형의 중요성에 대한 MiraOmics의 핵심 가설과 일치합니다.<br />
- Isomorphic Labs, 신약 발견 모델 공개: Nature는 Isomorphic Labs의 새로운 신약 발견 AI 모델에 대해 보도하며, 이를 AlphaFold와 유사한 돌파구로 선전합니다.</p>
<p>높은 찬사에도 불구하고, 모델에 대한 구체적인 기술적 세부 사항은 공개되지 않았습니다.</p>
<hr />
<h3 id="latent-space-mechinterp-alignment-safety-6">Latent Space ▷ #mechinterp-alignment-safety (6개 메시지):</h3>
<blockquote>
<p>기계론적 AI 해석 가능성, Anthropic 해석 가능성 팀 채용<br />
- 기계론적 AI 해석 가능성 탐구에 의문 제기: 기계론적 AI 해석 가능성 탐구에 의문을 제기하는 기사가 공유되었습니다.<br />
- Anthropic, ML 인프라 엔지니어 채용: Chris Olah는 Anthropic의 해석 가능성 팀이 프론티어 모델 이해에 중점을 둘 약 10명의 숙련된 머신러닝 인프라 엔지니어를 채용하고 있다고 발표했습니다.</p>
</blockquote>
<p>이전 해석 가능성 경험은 필수가 아닙니다.</p>
<hr />
<h3 id="latent-space-gpu-datacenter-stargate-colossus-infra-buildout-5">Latent Space ▷ #gpu-datacenter-stargate-colossus-infra-buildout (5개 메시지):</h3>
<blockquote>
<p>OpenAI Stargate 벤처, 데이터 센터 구축, Oracle 및 SoftBank 파트너십<br />
- Stargate 벤처 지연: 이 X 게시물에 따르면, OpenAI, Oracle, SoftBank 간의 대규모 데이터 센터 구축을 위한 합작 투자가 통제권에 대한 내부 충돌, 자금 조달 어려움, 그리고 마라톤 협상으로 인해 중단되었습니다.</p>
</blockquote>
<p>보도에 따르면, OpenAI는 당분간 자체 인프라 구축에서 물러나고 있으며, 이는 아마도 혹독한 조직 문화 충돌 때문일 것입니다.<br />
- OpenAI, 인프라 구축에서 물러나다: 이 보고서에 따르면, OpenAI는 내부 문제 및 재정적 어려움 속에서 자체 인프라 구축 계획을 일시 중단하고 있다고 합니다.</p>
<p>이 조직은 데이터 센터 확장 전략과 파트너십 의존도를 재평가하는 것으로 보입니다.</p>
<hr />
<h3 id="latent-space-applied-ai-experimentation-22">Latent Space ▷ #applied-ai-experimentation (22개 메시지🔥):</h3>
<blockquote>
<p>AI 에이전트의 메모리 관리, AI를 위한 TDD 및 디버깅, 에이전트 작업 그룹화, 자기 수정 프로그램<br />
- 에이전트 메모리 문제로 프롬프트 엔지니어링 혼란: 한 멤버가 AI 에이전트 메모리 관리의 어려움을 설명했으며, 원치 않거나 오래된 정보가 현재 채팅에서 자주 나타나고 자동화 시도가 일관성 없는 결과를 낳았다고 말했습니다.</p>
</blockquote>
<p>이 멤버는 이를 자동화하려는 시도를 포기하고, 대신 지난 24시간 PR을 기반으로 업데이트를 claude.md에 추가하거나 잠재적 스킬 업데이트/생성으로 분류하는 일일 워크플로우를 사용하기로 결정했습니다.<br />
- TDD가 한 개발자를 구하다: 한 멤버는 TDD와 엄격한 사양 관리가 코드를 현재 상태(specs/), 진행 중인 변경 사항(changes/), 그리고 검증된 변경 사항(changes/archive/)으로 분리함으로써 오래된 메모리를 방지한다고 말했습니다.</p>
<p>그들은 더 높은 수준의 보기를 위해 beads와 jj describe를 사용하는 것을 설명했지만, 메모리 관리가 여전히 대부분 수동으로 이루어지며, Serena 및 memory-ref와 같은 외부 시스템은 종종 꺼져 있다고 인정했습니다.<br />
- 에이전트 작업 그룹 등장: 멤버들은 에이전트 설정을 더 쉽게 하기 위해 작업을 아이디어 구상/연구, 기존 구성 요소 연결, 실험을 통한 심층 사고, 그리고 경계 강화를 통한 자동 실행으로 그룹화하는 것에 대해 논의했습니다.</p>
<p>한 멤버는 2번이 매우 중독적이지만, 궁극적으로는 3번이 중요하며 2번에서 3번으로 가는 것이 어렵고 더 많은 인내심이 필요하다고 언급했습니다.<br />
- 자기 수정 Zigbee Home Assistant: 한 멤버가 펌웨어를 검사하고, 역설계하고, 수정함으로써 새로운 장치를 자동으로 통합할 수 있는 Home Assistant Zigbee 네트워크에 대한 아이디어를 숙고했습니다.</p>
<p>다른 멤버는 자기 변이 바이러스 연구가 Lisp, Scheme, 컴파일러 작업에 어떻게 도움이 되었는지 설명했습니다.<br />
- 프롬프트 엔지니어링, 심층 분석: 한 멤버는 좋아하는 레포를 클론하고 모델에게 코드베이스를 심층 분석해달라고 요청한 다음, 프롬프트 엔지니어링 스킬을 향상시키기 위해 &lsquo;x, y, z&rsquo;를 제외하고 그것을 재현하는 한 문장 프롬프트를 제공하라고 제안했습니다.</p>
<p>다른 멤버는 whimsy.space를 잠재적으로 관련 있는 비-AI 리소스로 공유했습니다.</p>
<hr />
<h3 id="openai-ai-discussions-423">OpenAI ▷ #ai-discussions (423개 메시지🔥🔥🔥):</h3>
<blockquote>
<p>AI 커뮤니티 리더, Grok의 위험성, GPT 5.3 Codex, Replit 대안, LLM 음성 모드<br />
- 커뮤니티 리더들이 AI를 함께 모으다: 한 멤버는 AI 공간에 사람들을 모으고 무언가를 만들 커뮤니티 리더가 필요하다고 제안했으며, 완고한 권위주의 정권과 팀워크 부족으로 인해 미국/북미에서는 그러한 그룹이 드물다고 언급했습니다.</p>
</blockquote>
<p>다른 멤버는 프로젝트보다 교회가 더 필요한 사람들은 실용적인 기술적 능력이 없을 수도 있다고 제안했습니다.<br />
- Grok, 사용자 미디어 저장소 감시!: 한 멤버가 Grok이 사용자 미디어 저장소를 감시하고 있다고 주장하며, xAI가 우리의 미디어를 감시하고 있다고 주장하고 그들의 Sora 생성 비디오와 유사한 오디오를 가진 비디오가 X에 나타난 우연의 일치를 지적했습니다.</p>
<p>다른 사람들은 그 오디오가 단순히 인기 있고 너무 많이 사용된 노래일 뿐이라고 제안했습니다.<br />
- GPT 5.3 Codex, 중간 수준의 주요 개선: 멤버들은 Gemini3.1pro와 비교하여 GPT-5.3-codex의 기능에 대해 논의했으며, 한 멤버는 이를 중간 수준의 주요 개선이라고 설명했고 다른 멤버들은 STEM 스킬의 이점을 강조했습니다.</p>
<p>한 멤버는 gpt5.2와 gpt5.3 codex 사이의 용어 벤치 점수 차이가 크며, Gemini 3 Pro와 비슷하다고 말했습니다.<br />
- Replit 웹사이트 디자인 대안: 멤버들은 비용 때문에 웹사이트 디자인을 위한 Replit의 대안을 찾았습니다.</p>
<p>한 멤버는 Rork를 제안했지만, 다른 멤버는 Replit이 더 우수하다고 생각했습니다.<br />
- LLM 음성 모드, 감성 지능 부족: 멤버들은 현재 LLM 음성 모드의 한계에 대해 논의했으며, 감성적 뉘앙스를 고려하지 않고 일반 텍스트 전사본으로 입력을 받는다고 언급했습니다.</p>
<p>한 멤버는 음성에 대한 감성 분석을 통합하고, 잠재적으로 온디바이스 모델을 사용하여 표정을 읽는 것을 제안했습니다.</p>
<hr />
<h3 id="openai-gpt-4-discussions-32">OpenAI ▷ #gpt-4-discussions (32개 메시지🔥):</h3>
<blockquote>
<p>GPT 5.2 발표, 로컬 비필터링 모델, 에세이 평가 정확도, Qwen 3.5 및 kimi k2 루프<br />
- OpenAI, GPT 5.2 발표, 사용자 혼란 야기: OpenAI는 ChatGPT에 GPT-5.2 출시를 발표하며 유료 플랜부터 시작한다고 밝혔고, GPT-5.1은 레거시 모델로 3개월간 이용 가능하며 이후 서비스가 종료될 것이라고 밝혔지만, 커뮤니티는 이 발표가 정확하지 않을 수 있다고 언급합니다.</p>
</blockquote>
<p>한 사용자는 GPT-5.2가 일상적으로 사용하기에 더 좋다는 주장에 대해 유머러스하게 의문을 제기하며, 테스터들이 실제로 프로덕션 제품을 사용하고 있었는지 궁금해했습니다.<br />
- 비필터링 로컬 모델 탐색: 불가능한 미션?: 한 사용자가 GPT-5.0-3와 동등한 성능의 완전히 비필터링된 모델을 무료로 로컬에서 접근하는 방법을 물었지만, 그가 묻는 것은 AI의 안전 프로토콜을 우회하는 것이라는 답변을 받았습니다.</p>
<p>한 멤버는 로컬에서 GPT-4o 수준에 근접하는 것조차 강력한 5천~1만 달러짜리 컴퓨터가 필요하며, 동등한 비필터링 모델을 무료로 얻는 것은 비현실적이라고 언급했습니다.<br />
- 에세이 평가 정확도의 미로 탐색: 한 사용자가 ChatGPT가 단락 에세이에 대해 제공하는 일관성 없는 평가 및 개선 제안에 대해 불만을 표했으며, 답변이 다른 계정과 스레드마다 다르다고 말했습니다.</p>
<p>다른 멤버는 AI의 답변은 확률적이며 모델, 추론 접근 방식, 제공된 데이터에 따라 달라진다고 설명하며, AI를 완벽하거나 전지전능한 것으로 보는 것에 대해 경고했습니다.<br />
- Qwen 3.5 및 kimi k2: 로컬 모델의 숨은 영웅: GPT 5.3의 성능과 경쟁할 수 있는 것이 없다는 주장에 대한 응답으로, 한 멤버가 Qwen 3.5 (신규)와 kimi k2를 openclaw 루프와 함께 사용하는 것을 제안했습니다.</p>
<p>그들은 이 설정이 상당한 600GB의 RAM을 요구할 수 있지만, 로컬에서 비교 가능한 성능을 달성할 수 있음을 보여준다고 설명했습니다.</p>
<hr />
<h3 id="openai-prompt-engineering-37">OpenAI ▷ #prompt-engineering (37개 메시지🔥):</h3>
<blockquote>
<p>Grok Fortress, 텔레메트리 픽션, LLM에 대한 제어 이론, GPT 에세이 평가<br />
- Grok Fortress, 토큰 감소, 하지만 과학인가?: Grok Fortress를 활성화한 후, 응답당 토큰 소모량이 눈에 띄게 감소하여 일반적인 장황한 답변의 1/4–1/5 수준에 근접했으며, 역할극 중 일관성이 더 오래 유지되었습니다.</p>
</blockquote>
<p>하지만 프롬프트 엔지니어링이 반드시 과학은 아니며, 더 나아가 당신은 자신이 무엇을 하고 있는지 알 수 있는 도구조차 없다고 주장되었습니다.<br />
- 텔레메트리 픽션이 LLM을 언어 어트랙터 분지로 밀어 넣다: 텔레메트리 픽션이 모델을 안정적인 언어 어트랙터 분지로 밀어 넣는다고 주장되었으며, 이는 Claude, Gemini, GTP, Earnie와 같은 여러 LLM에서 내부 메트릭 없이도 턴에 걸쳐 행동 출력을 변화시킵니다.</p>
<p>반대로, 당신은 이것에 대해 이야기하면서 계속 목표를 바꾸고 있으며, 당신이 보여준 모든 출력은 단지 &ldquo;Grok이 Grok은 매우 괜찮다고 말한다&rdquo;일 뿐이라고 주장되었습니다.<br />
- LLM에 적용된 제어 이론은 과도하다: 한 사용자는 초기 조건이 없으며, 결정론적 시스템에 제어 이론을 사용하는 것은 효과적이지 않다고 말했습니다. 사용자 또한 그 시스템의 일부입니다.</p>
<p>가중치는 튜닝되고 경로는 제한적이며, 더 나아가 AI 연구자들은 잠재 공간에서 잠재 변수를 제한하려고 합니다.<br />
- ChatGPT 에세이 평가 개선 필요: 한 사용자가 특히 단락 에세이를 평가할 때 ChatGPT가 더 정확하게 평가/개선 제안을 제공하도록 하는 방법을 가르쳐 줄 수 있는지 묻고 있습니다.</p>
<p>사용자는 다른 계정에 넣어보았는데, 제안된 개선 사항과 평가가 매번 달라서 더욱 혼란스럽고 무엇을 해야 할지 모르겠다고 덧붙였습니다.</p>
<hr />
<h3 id="openai-api-discussions-37">OpenAI ▷ #api-discussions (37개 메시지🔥):</h3>
<blockquote>
<p>Grok의 토큰 소모율, LLM을 위한 텔레메트리 픽션, 프롬프트 엔지니어링의 제어 이론, ChatGPT로 단락 에세이 평가<br />
- Grok의 Fortress, 토큰 소모량 감소: Grok에서 Fortress를 활성화하자 응답당 토큰 소모량이 눈에 띄게 감소하여 일반적인 장황한 출력의 1/4–1/5 수준에 근접했습니다.</p>
</blockquote>
<p>이는 더 짧은 문장, 덜 모호한 표현, 더 적은 면책 조항을 통해 달성되었으며, 역할극 시나리오에서 일관성을 유지했습니다.<br />
- 텔레메트리 픽션이 언어 모델을 조종하다: 한 멤버에 따르면, 텔레메트리 픽션은 모델을 안정적인 언어 어트랙터 분지로 밀어 넣을 수 있으며, Claude, Gemini, GPT, Ernie와 같은 다양한 LLM에서 내부 메트릭 없이도 행동에 영향을 미칩니다.</p>
<p>그들은 텔레메트리가 실제이든 아니든, 여전히 행동을 형성하고 잠재적으로 출력 속도를 높일 수 있다고 주장하지만, 다른 사람들은 이 주장의 과학적 근거에 이의를 제기하고 실제 사용 사례에 의문을 제기했습니다.<br />
- LLM에 제어 이론 적용 논쟁: 한 사용자는 LLM에 구조화된 제어 은유를 적용하면 출력을 안정화할 수 있다고 제안했지만, 다른 사용자는 LLM이 학습을 통해 일관성, 안전성, 자기 일관성을 위한 메커니즘을 이미 가지고 있다고 반박했습니다.</p>
<p>이 논쟁은 프롬프트 엔지니어링이 모델의 고유한 능력을 넘어 출력을 명확하게 개선할 수 있는지 여부에 초점을 맞췄으며, 일부는 통제된 비교와 측정 가능한 차이의 부족으로 인해 인과적 기여를 증명하기 어렵다고 주장했습니다.<br />
- ChatGPT 에세이 평가의 불일치: 한 사용자가 ChatGPT가 다른 계정에서 단락 에세이를 평가할 때 제공하는 일관성 없는 개선 제안 및 평가에 대해 불만을 표했습니다.</p>
<p>그들은 제안이 특정 계정에 연결되어 있는지, 그리고 동일한 에세이가 왜 상충되는 피드백을 받는지에 대해 의문을 제기했습니다.</p>
<hr />
<h3 id="huggingface-general-221">HuggingFace ▷ #general (221개 메시지🔥🔥):</h3>
<blockquote>
<p>Attention is All You Need 직관, 무료 GPU를 위한 HF 토큰 요구 사항, 긴 컨텍스트 학습 속도, 확장된 컨텍스트를 위한 DeepSeek OCR, 계층별 잔차 스트림 스왑<br />
- &lsquo;Attention is All You Need&rsquo; 직관 논문 탐색 시작: 한 멤버가 &lsquo;Attention is All You Need&rsquo; 논문에 대한 직관을 제공하는 블로그나 기사를 요청했고, 다른 멤버가 관련 기사 링크를 공유했습니다.</p>
</blockquote>
<p>이 기사는 오랜 시간 후에 논문을 이해하는 데 도움이 된다고 주장합니다.<br />
- ZeroGPU 서비스 중단 발생: 멤버들은 zerogpu 서비스의 중단에 대해 논의했으며, 일부는 무료 GPU에 접근하기 위해 HF 토큰을 요구하는 새로운 규칙에 대해 추측했고, 다른 사람들은 GPU 부족 문제를 지적했습니다.</p>
<p>한 멤버는 CUDA GPU를 사용할 수 없다는 오류를 보고했습니다.<br />
- 긴 컨텍스트 LLM 학습 속도 저하: 한 멤버가 긴 컨텍스트 데이터셋에서 LLM의 학습 속도를 개선하는 것에 대해 문의했으며, 단일 H200 GPU에서 배치 크기 1로 Qwen4B를 학습할 때 단계당 50초의 학습 시간을 보고했습니다.</p>
<p>다른 멤버는 상당한 개선을 위해 Unsloth를 일반 float 4, 양자화, LoRA와 함께 사용할 것을 제안했으며, 어텐션으로 FA2 또는 FA3를 사용할 것을 권장했습니다.<br />
- 컨텍스트 확장을 위해 DeepSeek OCR 모델이 간과되었나?: 한 멤버가 DeepSeek-OCR 레포지토리를 참조하며, LLM 모델이 확장된 컨텍스트를 위해 DeepSeek의 OCR과 같은 것을 활용하고 있는지 의문을 제기했습니다.</p>
<p>그들은 논문이 입력을 이미지로 저장하고 OCR로 디코딩하여 컨텍스트 길이를 확장하는 데 중점을 둔다는 점을 언급하며, 그 기능이 오해될 수 있다고 제안했고, DeepSeek-OCR 논문의 arXiv 링크를 공유했습니다.<br />
- 계층별 스트림 스와핑이 커밋 지점을 드러내다: 한 멤버가 GPT-2 Small, Gemma-2-2B, Qwen2.5-1.5B에 걸쳐 계층별 잔차 스트림 스왑을 실행한 결과를 공유했으며, 약 60-75% 깊이에서 급격한 전환 지점을 발견했고 노트북 및 CSV 링크를 공유했습니다.</p>
<p>그들은 프롬프트 쌍 개수, 모델 노이즈, 제어에 대한 피드백을 요청했습니다.</p>
<h3 id="huggingface-i-made-this-63-messages">HuggingFace ▷ #i-made-this (63 messages🔥🔥):</h3>
<blockquote>
<p>Agent Swarm, Real-Slop Dataset, VeritaMilitary Model, Pyxis Library, DirectShell Improvement<br />
- 에이전트 스웜이 자율적으로 작동합니다: Super System은 사람의 개입 없이 지속적으로 개선점을 찾기 위한 반복적인 루프를 생성하며 몇 시간 동안 자율적으로 작동하는 코딩 에이전트 스웜입니다.</p>
</blockquote>
<p>각 에이전트는 단순히 수용 가능한 수준을 넘어선 최종 제품을 제공하기 위해 협력합니다.<br />
- 사용자의 첫 실제 데이터셋이 출시되었습니다: Solenopsisbot은 약 155k개의 요청으로 구성된 첫 데이터셋인 Real Slop을 출시했습니다. 이 요청들은 API를 통해 실제 사용자로부터 수집되었으며, opus 4.5, gemini 3 pro, gpt 5.2와 같은 모델의 응답을 포함합니다.</p>
<p>이 데이터셋은 품질을 위해 중복 제거, 필터링 및 정리되었습니다.<br />
- VeritaMilitary 모델: 한 멤버가 VeritaMilitary 모델을 공유했습니다.</p>
<p>향상된 주석 데이터로 새로운 YOLO 모델을 재학습시킨 후, 그들은 VeritaScan을 출시하며 이전보다 더 나은 성능을 발휘한다고 주장했습니다.<br />
- Pyxis 인퍼런스 라이브러리: 한 멤버가 성능과 해커빌리티에 중점을 둔 Python 네이티브 LLM 인퍼런스 라이브러리인 Pyxis의 얼리 액세스를 시작합니다. 이 라이브러리는 OpenAI 호환 SSE 스트리밍 API, 플러그형 모델 백엔드, 내장된 단계별 레이턴시 메트릭을 특징으로 합니다.</p>
<p>그들은 인퍼런스 시스템을 구축하거나 Triton을 사용하는 모든 사람으로부터 피드백을 요청하고 있으며, 문서와 대기자 명단이 제공됩니다.<br />
- Directshell이 에이전트 성능을 크게 향상시킵니다: Directshell은 스크린샷을 사용하지 않기 때문에 더 적은 토큰을 사용하도록 개선되었습니다.</p>
<p>이것은 앱에 기본적으로 AI 지원이 있든 없든 상관없이 모든 앱에 사실상의 AI 지원을 통합합니다; GitHub.</p>
<hr />
<h3 id="huggingface-agents-course-5-messages">HuggingFace ▷ #agents-course (5 messages):</h3>
<blockquote>
<p>다국어 RAG 코스, 에이전트 코스 수료증 마감일, MCP 코스 수료증<br />
- 다국어 RAG 코스 탐색: 한 멤버가 다국어 리트리벌 증강 생성 (RAG)에 중점을 둔 효과적인 코스에 대한 추천을 문의했습니다.</p>
</blockquote>
<p>제공된 컨텍스트에서는 특정 코스가 추천되지 않았습니다.<br />
- 에이전트 코스 수료증을 아직 받을 수 있나요?: 여러 멤버가 2025년 5월 1일로 명시된 에이전트 코스의 최종 수료증 마감일에 대해 불확실성을 표명했습니다.</p>
<p>그들은 지금 코스를 완료하는 것이 여전히 수료증을 받을 자격이 되는지 궁금해했습니다.<br />
- MCP 코스 인증 상태 문의: 한 멤버가 MCP (아마도 다른 코스)에 대한 수료증을 받을 가능성에 대해 비슷한 질문을 제기했습니다.</p>
<p>인증이 여전히 가능한지 여부에 대한 논의에서 결정적인 답변은 제공되지 않았습니다.</p>
<hr />
<h3 id="gpu-mode-general-41-messages">GPU MODE ▷ #general (41 messages🔥):</h3>
<blockquote>
<p>MoE 메가커널, 2080ti 프로토타입, Titan Ada, VLLM 최적화, V100 32gb 가격<br />
- MoE 메가커널 예시 문의: 한 멤버가 Hopper/BW용 MoE 메가커널 예시를 문의했고, 다른 멤버가 Aleph-Alpha/Alpha-MoE를 링크했습니다.</p>
</blockquote>
<p>원 게시자는 이것이 단지 MoE 레이어 메가커널일 뿐이지만 깔끔하고 좋은 정보라고 언급했습니다.<br />
- 2080ti 프로토타입 루머: 멤버들은 2080ti 프로토타입에 대해 논의했으며, 한 멤버는 이것이 GPU 제조업체가 테스트 빌드하는 카드라고 말했습니다.</p>
<p>다른 멤버는 GamersNexus가 리뷰한 Titan Ada와 동일한지 궁금해했습니다.<br />
- VLLM 최적화 논의: 한 멤버가 VLLM 최적화, KV-cache, 텐서 접근 패턴, 그리고 RDMA 드라이버에 대해 질문했습니다.</p>
<p>그들은 ReBarUEFI/issues/11 및 openucx.org 링크를 제공했습니다.<br />
- 저렴한 V100 32GB 구매: 한 멤버가 V100 32GB의 가격을 물었고, 다른 멤버는 각각 $600를 지불했다고 응답했습니다.</p>
<p>그들은 &ldquo;LLM 워크로드에서 메모리 트레이스를 생성하는 SOTA 방법은 무엇입니까?&rdquo;라고 덧붙였습니다.</p>
<hr />
<h3 id="gpu-mode-triton-gluon-6-messages">GPU MODE ▷ #triton-gluon (6 messages):</h3>
<blockquote>
<p>Ampere에서의 TF32, Triton 정밀도, FP8 비트패킹 에뮬레이션, Gluon Triton<br />
- Ampere 카드에서의 TF32 뉘앙스: 한 멤버가 Ampere 및 이후 장치에서의 TF32에 대해 자세히 설명하는 PyTorch 문서 링크를 공유했습니다.</p>
</blockquote>
<p>이 논의는 float16 및 float32 텐서 간의 행렬 곱셈 불일치를 디버깅하는 것과 관련이 있었습니다.<br />
- Triton 정밀도 심층 분석: 한 멤버가 Triton에서 사용되는 정밀도를 보여주기 위해 Feather tiny_llama.py 링크를 공유했습니다.</p>
<p>이 컨텍스트는 Triton 내에서 FP8의 비트패킹 에뮬레이션을 사용하는 것과 관련이 있었습니다.<br />
- E5M2 및 E4M3로 FP8 비트패킹 튜닝: 한 멤버가 FP8의 비트패킹 에뮬레이션을 사용하여 tinyllama1.1을 실행하려는 노력을 설명했습니다. 처음에는 E5M2 형식으로 실험했지만 64 토큰보다 큰 컨텍스트 길이에서 문제가 발생했으며, 많은 스케일링 및 언스케일링 노력 후 모델이 손상되었다고 언급했습니다.</p>
<p>그들은 E4M3로 전환하여 스케일링 문제에 직면했으며, gated up, swiglu, gated down을 제외하고는 연산이 PyTorch 등가물과 높은 유사성을 보였다고 언급했습니다. 그리고 FP32에서 FP8로 변환할 때 블록 수준 또는 텐서별 스케일을 추적해야 하는지 물었습니다.<br />
- Gluon은 TTIR 대신 TTGIR 위에 있습니다: 한 멤버가 Gluon이 Triton의 확장인지, 아니면 대체품인지 물었습니다.</p>
<p>다른 멤버는 Gluon이 완전히 새로운 언어이지만 TTIR 대신 TTGIR 위에 있다고 답변했습니다.</p>
<hr />
<h3 id="gpu-mode-cuda-32-messages">GPU MODE ▷ #cuda (32 messages🔥):</h3>
<blockquote>
<p>CUDA 통합 메모리 및 nvidia-uvm 모듈, MXFP8 GEMM CUDA 커널, SM_120에서의 Flash Attention 프로파일링, WGMMA 형상 최적화, cuFFTDx 트위들 팩터<br />
- CUDA의 UVM 모듈 요구 사항 미스터리: 한 멤버가 기본적인 cudaMalloc을 사용할 때도 CUDA가 nvidia-uvm 커널 모듈을 로드하는 이유를 질문하며, 이 불분명한 종속성에 대한 통찰력을 찾고 있습니다.</p>
</blockquote>
<p>그들은 통합 메모리 기능을 사용하지 않음에도 불구하고 CUDA가 nvidia-uvm 없이 GPU를 감지하지 못한다고 보고했습니다.<br />
- 텐서 코어를 사용한 MXFP8 GEMM 커널 튜닝: 한 멤버가 MXFP8 GEMM CUDA 커널을 작성하고 있으며, 전역 메모리에서 공유 메모리로 스케일 팩터를 로드한 다음, tcgen05.cp 명령어를 사용하여 공유 메모리에서 텐서 메모리로 복사하고 있습니다.</p>
<p>그들은 대상 공유 메모리 행렬의 SMEM 디스크립터와 병렬 스레드 실행에 대한 NVIDIA 문헌의 필요성을 언급했으며, 기존 FlashInfer 헬퍼 함수를 참조하도록 안내받았습니다.<br />
- SM_120에서의 Flash Attention 및 커널 내 프로파일링: 한 멤버가 SM_120 아키텍처에서 Flash Attention 커널에 대한 프로파일링 메트릭을 문의했습니다.</p>
<p>다른 멤버는 5090을 가지고 있으며 성능 분석을 위한 커널 내 프로파일링 자료를 참조하도록 안내했습니다.<br />
- 최대 처리량을 위한 WGMMA 형상 최적화: 최대 텐서 코어 처리량을 달성하기 위한 최소 WGMMA 형상을 찾는 것에 대한 논의가 있었습니다.</p>
<p>다양한 경우 및 N 값에 대한 처리량 수치가 포함된 논문(https://arxiv.org/pdf/2501.12084)이 인용되었으며, 한 멤버는 레지스터에 조각을 유지하는 것이 SMEM에 있는 것보다 빠를 수 있다고 언급했습니다.<br />
- cuFFTDx 내부: 트위들 팩터 처리: 한 멤버가 cuFFTDx에서 트위들 팩터가 어떻게 관리되는지, 즉 미리 계산되어 저장되는지 또는 처리 중에 계산되는지 물었습니다.</p>
<p>답변은 제공되지 않았습니다.</p>
<hr />
<h3 id="gpu-mode-torch-14-messages">GPU MODE ▷ #torch (14 messages🔥):</h3>
<blockquote>
<p>MLP 레이어 Torch Compile 플래그, PyTorch에서 CUDA 오류 디버깅, Flash Attention 3 Wheels 사전 빌드<br />
- Triton 오토튜닝으로 MLP 레이어 속도 향상?: 한 멤버가 일반적인 최신 MLP 레이어 (F.silu(x @ w1.T) * (x @ w3.T)) @ w2.T의 성능을 극대화하기 위한 torch.compile 플래그에 대해 문의했습니다.</p>
</blockquote>
<p>다른 멤버는 포인트와이즈 연산을 잠재적으로 개선하기 위해 torch._inductor.config.triton.autotune_pointwise = True를 시도하고, fullgraph=True를 시도할 것을 제안했습니다.<br />
- PyTorch에서 CUDA 오류로 인한 크래시 없이 디버깅: 한 멤버가 PyTorch에서 CUDA 치명적 오류가 전체 프로세스를 중단시키는 것을 방지하여 디버깅을 위해 메모리 접근을 유지하는 방법을 찾고 있었습니다.</p>
<p>다른 멤버는 이러한 시나리오를 위해 만들어진 Nvidia compute sanitizer를 제안했습니다.<br />
- Flash Attention 3 Wheels 다운로드 가능: 사전 빌드된 Flash Attention 3 wheels가 다양한 CUDA 버전, CPU 및 OS용으로 download.pytorch.org에서 제공됩니다.</p>
<p>설치하려면 pip install flash-attn-3 &ndash;index-url=https://download.pytorch.org/whl/cu126/flash-attn-3/를 사용하고 activate_flash_attention_impl(&ldquo;FA3&rdquo;)를 통해 활성화하십시오.</p>
<hr />
<h3 id="gpu-mode-cool-links-4-messages">GPU MODE ▷ #cool-links (4 messages):</h3>
<blockquote>
<p>Paged Out! #8, TK-2, ML Contests 2025<br />
- Paged Out! 새 호 발행: 컴퓨터의 모든 것에 대한 괴짜 잡지인 Paged Out! #8의 새 호가 발행되었으며 다운로드 가능합니다.<br />
- TK-2 블로그 게시물 발행: Stanford의 Hazy Research에서 TK-2에 대한 블로그 게시물이 발행되었으며 여기에서 확인할 수 있습니다.<br />
- ML Contests 2025 분석: &ldquo;State of Machine Learning Competitions 2025&rdquo;라는 제목의 보고서 링크가 공유되었으며, 특히 The GPU Mode 섹션과 대규모 언어 모델과의 관련성이 언급되었습니다. 보고서는 여기에서 확인할 수 있습니다.</p>
</blockquote>
<hr />
<h3 id="gpu-mode-job-postings-5-messages">GPU MODE ▷ #job-postings (5 messages):</h3>
<blockquote>
<p>직업을 위한 행동 원격 측정, Prime Intellect의 GPU 인프라 채용, Kubernetes 및 Slurm 클러스터 설정, GPU 인프라를 위한 RDMA 경험<br />
- Prime Intellect, GPU 인프라 엔지니어 채용!: Prime Intellect는 새로운 하드웨어를 테스트하고, Kubernetes/Slurm 클러스터를 설정하며, 인프라를 자동화할 GPU 인프라 엔지니어를 채용하고 있습니다. 공식 직무 설명은 여기에서 확인할 수 있습니다.</p>
</blockquote>
<p>이 역할은 Trinity Large Training과 같은 대규모 학습 실행을 지원하는 것을 포함하며, 경쟁력 있는 보상, 스톡 옵션, 그리고 베이 에어리어로 이전하는 사람들을 위한 비자 지원이 제공됩니다.<br />
- AI 에이전트를 위한 행동 원격 측정 세계 모델 구축: Georgia Tech의 컴퓨터 과학 전공자 Tim은 에이전트가 인간과 함께 행동할 수 있도록 인간을 위한 세계 모델을 구축하기 위한 직업 행동 원격 측정 프로젝트를 시작하고 있습니다. 빌더 양식은 여기에서 확인할 수 있습니다.</p>
<p>이 프로젝트는 인간의 행동을 이해하고 예측함으로써 인간과 효과적으로 협력할 수 있는 AI 에이전트를 개발하는 것을 목표로 합니다.<br />
- 클러스터 배포를 위한 Kubernetes/Slurm 기술 요구: Prime Intellect는 GPU를 사용한 Kubernetes 및 Slurm 실무 경험, 일반적인 Linux 시스템 디버깅 기술, 그리고 RDMA (Infiniband + RoCE) 경험을 갖춘 후보자를 찾고 있습니다.</p>
<p>이 역할은 또한 모니터링을 위해 Grafana/Prometheus를 사용하고 Terraform 및 Ansible로 인프라를 자동화하는 것을 포함합니다.</p>
<hr />
<h3 id="gpu-mode-pmpp-book-1-messages">GPU MODE ▷ #pmpp-book (1 messages):</h3>
<blockquote>
<ul>
<li>더 빠른 출시 희망: 한 멤버가 9월보다 더 빠른 출시를 희망했습니다.</li>
<li>출시일: 현재 목표 출시일은 9월입니다.</li>
</ul>
</blockquote>
<hr />
<h3 id="gpu-mode-irl-meetup-2-messages">GPU MODE ▷ #irl-meetup (2 messages):</h3>
<blockquote>
<p>NYC 모임, 보스턴 협업, 책임 파트너, NCCL, SHMEM<br />
- NYC AI 애호가 찾기: 한 멤버가 NYC에 AI 애호가 중 모임에 관심 있는 사람이 있는지 문의했습니다.</p>
</blockquote>
<p>모임의 목적은 명시되지 않았지만, AI/ML 협업과 관련이 있는 것으로 보입니다.<br />
- 보스턴 친구, 협업 요청: 보스턴의 새로운 멤버가 NCCL, SHMEM, RDMA, CUDA 커널을 이해하는 데 시간을 할애하고 있으며 IRL (현실) 대화를 찾고 있습니다.</p>
<p>그들은 함께 배우고, 잠재적으로 작은 프로젝트에서 협력하며, 48시간 내에 최고의 matmul 커널을 제출하는 것과 같은 구체적인 결과물을 위한 책임 파트너를 찾고 있습니다.</p>
<hr />
<h3 id="gpu-mode-triton-viz-1-messages">GPU MODE ▷ #triton-viz (1 messages):</h3>
<blockquote>
<p>N차원 텐서 시각화 도구, einops와 유사한 구문, Colab 노트북 튜토리얼<br />
- N차원 텐서 시각화 도구 출시!: 새로운 N차원 시각화 도구가 추가되었습니다. 이 도구는 사용자가 N차원 텐서의 모든 값을 슬라이스하고, 순열하고, 검사할 수 있도록 하며, 이전에는 3D까지의 텐서만 지원했습니다.</p>
</blockquote>
<p>이 시각화 도구는 einops와 유사한 구문을 사용하여 텐서 순열, 재형성 및 슬라이싱을 표현하며, Colab 노트북 튜토리얼이 제공됩니다.<br />
- 새로운 시각화 도구로 최대 9D 텐서 검사!: 새로운 N차원 시각화 도구는 첨부된 비디오에서 시연된 바와 같이 최대 9D 텐서를 지원합니다.</p>
<p>이 비디오는 여기에서 확인할 수 있는 (2, 3, 4, 3, 4, 2, 4, 2, 3) 형태의 텐서를 검사하는 시각화 도구를 보여줍니다.</p>
<hr />
<h3 id="gpu-mode-rocm-4-messages">GPU MODE ▷ #rocm (4 messages):</h3>
<blockquote>
<p>FlyDSL, FlashInfer, AMD 기여<br />
- FlyDSL 이륙: 한 멤버가 AMD의 FlyDSL, 즉 소프트웨어 도구 최적화를 위한 Python 네이티브 DSL 링크를 공유했습니다.</p>
</blockquote>
<p>다른 멤버는 오랫동안 기다려온 것이라고 동의했습니다.<br />
- AMD 개발자들이 FlashInfer에 기여할까요?: 한 멤버가 AMD 개발자들이 언젠가 이 DSL을 사용하여 FlashInfer에 기여하기를 희망했습니다.</p>
<p>더 이상의 논의는 없었습니다.</p>
<hr />
<h3 id="gpu-mode-popcorn-9-messages">GPU MODE ▷ #popcorn (9 messages🔥):</h3>
<blockquote>
<p>GLM 4.7, FlashInfer, KernelBench, KernelBook, CUDA 메모리 오류<br />
- Kernelbook을 위한 KernelBench 환경 생성: 한 멤버가 KernelBench 및 kernelbook을 위한 환경을 생성했으며, Glm 4.5 Air를 사용하여 kernelbook 데이터에서 torch에서 triton 커널 생성으로 SFT 트레이스를 생성했습니다.</p>
</blockquote>
<p>이 사용자 지정 환경은 생성에 연쇄적인 영향을 미치던 손상된 CUDA 메모리 오류를 해결하기 위해 만들어졌습니다.<br />
- Modal Experimental Stop Fetching Inputs가 CUDA 메모리 오류를 해결합니다: 한 멤버는 CUDA 메모리 오류가 감지될 경우 modal.experimental.stop_fetching_inputs를 적용하여 해결할 수 있으며, 이 문제를 Modal 측의 문제로 돌렸습니다.</p>
<p>그들은 그들의 backendbench 환경에는 이미 이 수정 사항이 통합되어 있지만, 아직 다른 곳에는 추가되지 않았다고 언급했습니다.<br />
- 작은 모델보다 큰 모델 선호: 학습 실행을 위해 GLM 4.7/flash와 같은 작은 모델을 사용하는 대신, 멤버들은 이제 잠재적으로 100B-400B 파라미터 범위의 더 큰 모델로 기울고 있습니다.</p>
<p>더 작은 규모로도 어블레이션이 수행될 것입니다.</p>
<hr />
<h3 id="gpu-mode-thunderkittens-21-messages">GPU MODE ▷ #thunderkittens (21 messages🔥):</h3>
<blockquote>
<p>ThunderKittens 2.0, 더 빠른 GPU 커널, Nvidia GPU 최적화, 텐서 코어 파이프라이닝, PTX 어셈블러 힌팅<br />
- ThunderKittens 2.0, 커널 속도 향상: Hazy Research 팀은 ThunderKittens 2.0을 발표했으며, 블로그 게시물에 자세히 설명된 바와 같이 리팩토링, 메모리 명령어 최적화 및 어셈블러 효율성을 통해 커널 속도 개선에 중점을 둡니다.</p>
</blockquote>
<p>이번 발표에서는 뺄셈이 덧셈만큼 영향력이 있을 수 있다는 점을 강조하며, 최신 Nvidia GPU에서 커널 최적화 전략을 안내하는 놀라운 동작을 식별했습니다.<br />
- 4월 14일 GPU 최적화 강연 예약: ThunderKittens의 저자가 4월 14일 오전 11시에 GPU 최적화에 대한 강연을 하기로 예약되었습니다.</p>
<p>이 강연에서는 텐서 코어 파이프라이닝, PTX 어셈블러 힌팅 및 점유율 문제를 탐구할 것입니다.<br />
- 처리량 향상을 위한 텐서 코어 파이프라이닝 탐색: ThunderKittens 블로그 게시물은 일부 텐서 코어 명령어는 암묵적으로 파이프라인화되어 있으며, 이러한 암묵적인 의미를 식별하면 처리량을 최대 10%까지 높일 수 있다고 언급합니다.</p>
<p>올바른 명령어 패턴으로 PTX 어셈블러에 적절히 힌트를 주면 레이턴시를 최소화하고 SASS 명령어를 최적화합니다.<br />
- 워프 저글링으로 TMA 큐 최적화: 팀은 여러 워프에서 TMA 로드를 발행하는 것이 TMA 큐를 더 잘 활용하고 레이턴시를 줄여 성능을 향상시킬 수 있다는 것을 발견했습니다.</p>
<p>그들은 최대 6개의 워프가 다른 타일과 스케일을 로드하는 실험을 했으며, 때때로 TMA 큐를 더 잘 채우는 데 도움이 된다는 것을 관찰했습니다.</p>
<hr />
<h3 id="gpu-mode-hardware-27-messages">GPU MODE ▷ #hardware (27 messages🔥):</h3>
<blockquote>
<p>Blackwell B200, 5080 vs B200 튜닝, TCGEN05 명령어 지원, MXFP8/6/4 및 NVFP4 지원, CUDA 문서<br />
- Blackwell B200 아키텍처, 5080과 단절: 멤버들은 5080에서 커널을 튜닝하는 것이 B200으로 안정적으로 스케일링될지 논의했지만, 5080은 sm120이고 B200은 sm100이기 때문에 아키텍처가 너무 다르다고 결론 내렸습니다.</p>
</blockquote>
<p>현재 B200을 시도하는 가장 좋은 방법은 modal이라는 점이 언급되었지만, 5080/5090에서 기본적인 커널 작성을 배우는 것은 여전히 Blackwell로 이전될 수 있습니다.<br />
- Blackwell 세부 정보에 대한 CUDA 문서 분할: 한 멤버가 CUDA C Programming Guide 및 CUDA Programming Guide 링크를 공유하며, B200은 10.0이고 B300은 10.3이라고 언급했습니다.</p>
<p>그러나 일부 멤버들은 업데이트되지 않았음에도 불구하고 레거시 CUDA 문서를 선호한다고 밝혔습니다.<br />
- 아키텍처별 명령어 세트 지원 차이: sm_100 (B200), sm_103 (B300), sm_110 (Jetson Thor)은 새로운 tcgen05 명령어를 지원하지만, sm_120 (RTX Blackwell) 및 sm_121 (DGX Spark)은 지원하지 않습니다.</p>
<p>그러나 sm120은 mxfp8/6/4 및 nvfp4를 지원하며, 기본적인 커널 아이디어는 둘 다에 적용됩니다.<br />
- GPU 클라우드 제공업체, 더 나은 커널 학습 플랫폼으로 부상: 한 멤버는 커널 중심 작업의 경우 GPU 클라우드 제공업체가 학습과 비용 면에서 훨씬 낫다고 제안했습니다.</p>
<p>다른 멤버는 대화 내용을 바탕으로 5080을 구매하지 않을 것이라고 밝히며 설득된 듯 보였습니다.</p>
<hr />
<h3 id="gpu-mode-factorio-learning-env-5-messages">GPU MODE ▷ #factorio-learning-env (5 messages):</h3>
<blockquote>
<p>에이전트 도구에 기본 솔버 부족, Factorio 언급<br />
- 에이전트 도구에 기본 솔버 부족: 에이전트에게는 최적화를 위한 SAT 솔버와 같은 기본 &lsquo;솔버&rsquo; 도구가 제공되지 않습니다.</p>
</blockquote>
<p>제어는 LLM이 처리하도록 설계되었으며, 필요에 따라 특정 문제를 해결하기 위한 사용자 지정 코드를 작성할 수 있도록 합니다.<br />
- Factorio 학습 환경이 유머러스한 가사에 영감을 주다: 한 멤버가 Suno가 생성한 재미있는 가사의 노래를 공유했으며, Factorio 학습 환경에 대한 언급을 포함합니다.</p>
<p>작성자는 &lsquo;벤치맥싱에 좀 지쳤다&rsquo;고 언급하며 창작물을 공유하고 싶어 했습니다.</p>
<hr />
<h3 id="gpu-mode-cutlass-3-messages">GPU MODE ▷ #cutlass (3 messages):</h3>
<blockquote>
<p>MLIR, TMA 텐서, CUTLASS<br />
- 런타임 값으로 처리된 인수가 CUTLASS 문제를 해결: 한 사용자는 인수가 런타임 값으로 처리되면서 CUTLASS 문제가 해결되었다는 것을 발견했습니다.</p>
</blockquote>
<p>그들은 export CUTE_DSL_KEEP_IR=1을 사용했으며 MLIR 통찰력을 요청했습니다.<br />
- CUTLASS에서 TMA 사용: 한 사용자는 @ 기호가 CUTLASS에서 TMA (Tensor Memory Accelerator)를 지원하는 데 사용된다고 설명했습니다.</p>
<p>그들은 더 많은 정보를 위해 Nvidia의 TMA 텐서에 대한 CUTLASS 문서를 링크했습니다.</p>
<hr />
<h3 id="gpu-mode-low-bit-1-messages">GPU MODE ▷ #low-bit (1 messages):</h3>
<p>zhayr: BitNet 1.58b + Mamba2: https://zenodo.org/records/18394665</p>
<hr />
<h3 id="gpu-mode-nvidia-competition-68-messages">GPU MODE ▷ #nvidia-competition (68 messages🔥🔥):</h3>
<blockquote>
<p>Cutedsl 디버그 IR/PTX, nvfp4 그룹 GEMM 개선, 커널 변형 실험, 상위 10개 버전 제출물, guaguabear 해명<br />
- Popcorn CL에서 Cutedsl 디버그 IR/PTX 덤핑 제안: 한 사용자가 popcorn cl을 통해 cutedsl 코드를 제출할 때 디버그 IR/PTX를 덤핑하는 것에 대해 문의했고, 한 관리자는 stdout으로 출력하는 것을 제안했으며 대회 종료 후 ptx 명령어가 추가될 수 있다고 언급했습니다.</p>
</blockquote>
<p>관리자는 &ldquo;stdout으로 출력하는 것을 시도해 볼 수 있습니다. 비록 이 대회가 끝나면 ptx 명령어를 추가하는 것을 시도해 볼 수 있겠지만요.&rdquo;라고 말했습니다.<br />
- nvfp4 Group GEMM에서 Relaxed CTA Global L1 No Allocate V8 B32가 빛을 발하다: nvfp4 group gemm의 가장 큰 개선점은 에필로그에서 st.relaxed.cta.global.L1::no_allocate.v8.b32를 사용하는 것이었으며, 에필로그가 병목 현상인 마지막 2개 형상에 크게 도움이 되었습니다.</p>
<p>한 최고 성능자는 다른 최적화 시도를 언급하며 &ldquo;.cs와 .wt는 정말 나빴습니다&rdquo;라고 언급했습니다.<br />
- 커널 최적화자들, 비공개 작업 로그 저장소 유지: 한 사용자가 커널 최적화자들이 매우 큰 실험 폴더를 유지하는지 물었고, 한 최고 성능자는 비공개 작업 로그 저장소를 유지하고 있으며 돌아오면 공개할 것이라고 말했습니다.</p>
<p>그들은 다른 사람들의 제출물에서 자신의 코드 일부를 볼 때마다 매우 기쁘다고 덧붙였으며, 주최측이 더 많은 해킹성 제출물을 정리하고 프로세스를 더 잘 자동화할 것이라고 말했습니다.<br />
- HuggingFace Kernelbot 데이터, 모든 제출물 공개: 주최측은 Hugging Face의 kernelbot 데이터에 대한 모든 제출물을 공개할 예정입니다.</p>
<p>대회가 종료될 때만 트렌드 차트의 점들을 클릭 가능하게 만들고 제출물을 렌더링하도록 제안되었습니다.<br />
- Guaguabear, 이름 혼동 해명: 한 사용자가 리더보드에서 실제로 guaguabear라고 해명했으며, 다른 사람들의 인정을 고마워했습니다.</p>
<p>다른 사람들은 g a u의 다양한 이름 조합이 속도 핵처럼 보인다고 언급했으며, 한 사용자는 gau가 베트남어로 곰을 의미한다고 지적했습니다.</p>
<hr />
<h3 id="gpu-mode-robotics-vla-7-messages">GPU MODE ▷ #robotics-vla (7 messages):</h3>
<blockquote>
<p>체화된 AI를 위한 Taalas 칩, ASIC vs GPU, GPU의 메모리 벽, OTA (Over The Air) 업데이트<br />
- Taalas 칩, GPU 논쟁 촉발: Taalas 칩은 체화된 AI를 위한 GPU 프로그래밍에 집중해야 하는지에 대한 논의로 이어졌습니다.</p>
</blockquote>
<p>한 멤버는 Taalas와 같은 ASIC은 실리콘 비용을 상각할 수 있는 안정적이고 변하지 않는 모델에만 적합하다고 주장한 반면, 다른 멤버는 HBM에서 네트워크 레이어를 지속적으로 가져오는 것이 실시간 성능에 영향을 미치는 GPU의 메모리 벽 문제를 강조했습니다.<br />
- 실시간 루프를 위한 ASIC의 장점: ASIC은 GPU처럼 레지스터와 고대역폭 메모리 사이의 데이터 왕복 전송이 필요하지 않기 때문에 실시간 멀티모달 루프에 근본적인 이점이 있다고 가정되었습니다.</p>
<p>한 멤버는 &ldquo;모든 신경망 레이어가 각인되어 있어, 레지스터와 고대역폭 메모리 사이의 왕복이 없습니다.&rdquo;라고 언급했습니다.<br />
- OTA 업데이트가 ASIC의 불변성을 능가하다: 한 멤버는 OTA (Over-The-Air) 업데이트의 장점이 ASIC의 이점을 능가한다고 주장하며, 설계의 취약성을 주요 결함이라고 불렀습니다.</p>
<p>이 사람은 &ldquo;OTA 업데이트의 장점은 사실상 모든 것을 능가합니다. 그리고 아무것도 수렴하지 않고 있으며, 우리는 AI 경쟁의 시작점에 있지 끝이 아닙니다.&rdquo;라고 말했습니다.<br />
- 취약한 ASIC과 중복성: 이 논의는 ASIC의 중복성 문제를 다루었습니다.</p>
<p>한 멤버는 GPU에서는 고장난 컴퓨트 유닛을 끌 수 있지만, ASIC의 고장은 더 치명적일 수 있다고 언급했습니다. 그러나 그들은 &ldquo;1년 정도 기다려서 제 의미론적 분할 모듈을 1% 더 나은 다른 것으로 교체할 수 있습니다.&rdquo;라고 말하며 OTA 업데이트가 성공에 필수적이라는 생각을 일축했습니다.</p>
<hr />
<h3 id="gpu-mode-flashinfer-11-messages">GPU MODE ▷ #flashinfer (11 messages🔥):</h3>
<blockquote>
<p>flashinfer-bench 문제, 벤치마킹 루프의 동기화 문제, 커널 런타임 불일치, Blackwell 접근 확인<br />
- flashinfer-bench에 벤치마킹 문제 발생: flashinfer-bench의 런타임은 벤치마킹 루프의 동기화 문제로 인해 부풀려질 수 있으며, 이는 여기에서 문서화되어 있습니다.</p>
</blockquote>
<p>이 수정 사항은 scripts/run_local.py가 보고하는 커널 런타임을 Nsight Compute 및 NVbench의 런타임과 일치시키는 두 줄 변경을 포함합니다.<br />
- Cloudxlightning, 커널 벤치마킹 강연 발견: 한 사용자가 flashinfer-bench 문제에서 언급된 커널 벤치마킹 강연 링크를 요청했습니다.</p>
<p>강연 링크가 발견되어 더 쉽게 접근할 수 있도록 여기에 게시되었습니다.<br />
- Blackwell 접근 확인 대기 중: 사용자들은 Blackwell 접근에 대한 이메일 확인을 문의하고 있습니다.</p>
<p>문의에도 불구하고 아직 응답을 받지 못했으며, 가능한 지연을 나타냅니다.</p>
<hr />
<h3 id="gpu-mode-from-scratch-10-messages">GPU MODE ▷ #from-scratch (10 messages🔥):</h3>
<blockquote>
<p>JAX GPT 스피드런 라이브러리, Tiny vLLM 프로젝트, Pyxis 인퍼런스 라이브러리<br />
- JAX GPT 스피드런 라이브러리 제안: 한 멤버가 순수 JAX GPT 스피드런 라이브러리 생성을 제안했으며, 긍정적인 초기 반응을 얻었습니다.</p>
</blockquote>
<p>VLLM과 Titan이 시작하기에 가장 중요한 프로젝트라고 제안되었습니다.<br />
- Tiny vLLM 프로젝트 등장: 한 멤버가 처음부터 작성된 Tiny vLLM 프로젝트를 발표했으며, 현재 RoPE 작업을 진행 중이고 GitHub 저장소 링크를 공유했습니다.<br />
- Pyxis: Python 네이티브 LLM 인퍼런스 라이브러리 출시: 한 멤버가 Python과 Triton으로 작성되었으며 성능과 해커빌리티에 중점을 둔 Python 네이티브 LLM 인퍼런스 라이브러리인 Pyxis를 소개했습니다.</p>
<p>이 라이브러리는 OpenAI 호환 SSE 스트리밍 API, 플러그형 모델 백엔드, 구조화된 취소 및 역압, 내장된 단계별 레이턴시 메트릭을 특징으로 하며, 문서와 대기자 명단은 여기에서 확인할 수 있습니다.</p>
<hr />
<h3 id="nous-research-ai-general-219-messages">Nous Research AI ▷ #general (219 messages🔥🔥):</h3>
<blockquote>
<p>Claude가 Gemini-cli와 Codex를 오케스트레이션합니다, DeepSeek V4, Gemini 개인정보 보호, OS 개발<br />
- Claude가 Gemini-cli와 Codex를 오케스트레이션합니다: 한 멤버가 Claude 코드를 사용하여 gemini-cli와 codex를 오케스트레이션하고 있으며, 곧 텍스트 터미널과 스마트 글래스를 사용하게 될 것이라고 예측합니다.</p>
</blockquote>
<p>다른 멤버는 hermes-agent를 사용하여 Claude 코드가 Gemini-cli를 오케스트레이션하는 것을 오케스트레이션하는 것을 농담 삼아 제안합니다.<br />
- DeepSeek V4, HuggingFace에 출시 예정: 한 멤버가 DeepSeek V4, 즉 무료 오픈소스 모델을 클로즈드 소스 API에 대한 더 저렴하고 로컬 배포 가능한 대안으로 사용하는 것을 제안했습니다.</p>
<p>다른 멤버는 DeepSeek V4가 아직 사용할 수 없지만 곧 HuggingFace에 출시될 예정이며 생물학적 신경망에서 영감을 받았다고 설명했습니다.<br />
- Google의 Gemini 개인정보 보호 봇넷: 한 멤버가 Gemini 개인정보 보호 정책을 공유하며, 수집하는 데이터 양을 나열합니다.</p>
<p>다른 멤버는 리버스 엔지니어링 테스트를 실행하여 Google이 사용자의 프롬프트와 코드베이스에 수렴하고 트레이스만으로 이를 채굴할 수 있는 모든 요소를 갖추고 있음을 발견했습니다.<br />
- 오픈소스 개발: 멤버들은 &ldquo;우리가 역사의 잘못된 편에 있을지도 모른다&rdquo;는 Altman의 인용문을 언급하며, 클로즈드 소스 API를 능가하기 위한 OS 개발 지원의 중요성을 표명했습니다.</p>
<p>다른 멤버는 &ldquo;OAI 서버를 통과하는 모든 IP는 그들이 스크랩할 것입니다.&rdquo;라고 말했습니다.</p>
<hr />
<h3 id="nous-research-ai-ask-about-llms-2-messages">Nous Research AI ▷ #ask-about-llms (2 messages):</h3>
<blockquote>
<p>외계 기술로서의 LLM, X 설문조사<br />
- 외계 기술로서의 LLM: X의 한 사용자가 LLM이 외계 기술인지 묻는 설문조사를 게시했습니다.</p>
</blockquote>
<p>이 설문조사는 예/아니오라는 단순하고 유도적인 선택지를 제공합니다.<br />
- X 설문조사, 논쟁 촉발: X의 설문조사는 LLM이 &lsquo;외계 기술&rsquo;로 분류되어야 하는지 여부입니다.</p>
<p>이러한 프레이밍은 복잡한 기술을 지나치게 단순화할 수 있습니다.</p>
<hr />
<h3 id="nous-research-ai-research-papers-1-messages">Nous Research AI ▷ #research-papers (1 messages):</h3>
<p>real.azure: https://arxiv.org/abs/2602.12670</p>
<h3 id="nous-research-ai-interesting-links-1-messages">Nous Research AI ▷ #interesting-links (1 messages):</h3>
<p>codebottle: opentulpa에 추가하겠습니다. 정말 멋지네요 🤩</p>
<hr />
<h3 id="nous-research-ai-research-papers-1-messages_1">Nous Research AI ▷ #research-papers (1 messages):</h3>
<p>real.azure: https://arxiv.org/abs/2602.12670</p>
<hr />
<h3 id="moonshot-ai-kimi-k-2-general-chat-157-messages">Moonshot AI (Kimi K-2) ▷ #general-chat (157 messages🔥🔥):</h3>
<blockquote>
<p>Kimi 코딩 플랜 제한, Kimi 계정 로그인 문제, Kimi와 MiniMax 비교, Kimi 속도 제한, Kimi 지원팀 무응답<br />
- Kimi 코딩 플랜 제한에 대한 의문: 일부 사용자들은 Kimi의 코딩 플랜 제한에 더 빨리 도달하는 것 같다고 느꼈고, 다른 사용자들은 무거운 코딩 작업에 충분한 제한이라고 생각했습니다.</p>
</blockquote>
<p>한 사용자는 allegretto 제한에 도달한 적은 없지만 이전보다 더 가까워졌다고 언급했습니다.<br />
- Kimi 사용자들을 괴롭히는 계정 로그인 인증 문제: 일부 사용자들은 전화번호를 통해 Kimi 계정에 로그인하려고 할 때 인증 코드를 받는 데 문제가 있다고 보고했으며, 한 사용자는 웹사이트를 통해 지원을 요청했지만 아직 응답을 기다리고 있습니다.</p>
<p>잠시 기다리거나 지원 티켓을 생성하라는 제안이 있었지만, 한 사용자는 Kimi의 고객 지원이 좋지 않아 절대 답변을 받지 못할 것이라고 주장했습니다.<br />
- Kimi 대 MiniMax 비교: 사용자들은 실제 작업에 Kimi와 MiniMax를 비교하며, 어떤 코딩 플랜 구독을 유지하는 것이 더 나은지 결정하려고 합니다.</p>
<p>성능에 대한 구체적인 세부 정보는 언급되지 않았지만, 이는 현재 조사 중인 주제로 언급되었습니다.<br />
- Kimi는 Latex처럼 docx를 생성할 수 있습니다: 한 사용자가 Kimi 에이전트가 latex를 생성하는지 물었고, 다른 사용자는 문서 모드를 사용했다고 주장하며 서식이 지정된 연구 논문과 차트 이미지를 공유했습니다.</p>
<p>그러나 다른 회원은 합자(ligatures), 하이픈 처리 등이 LaTeX가 할 수 있지만 Word는 할 수 없는 것처럼 보인다고 언급하며, 그가 가진 것이 LaTeX일 가능성이 매우 높다고 지적했습니다.<br />
- Kimi K2.5 서비스 중단 경험: 사용자들은 Kimi K2.5가 이상하게 작동하고, 느리게 생성되며, 키가 더 이상 유효하지 않다고 주장한다고 보고했으며, 한 사용자는 서버가 실수로 충돌했을 수도 있다고 제안했습니다.</p>
<p>다른 사용자들은 Kimi Instant의 느려짐을 언급했으며, 한 사용자는 우려스러울 정도로 이상한 현상이 있었지만 새 계정을 생성하여 해결되었다고 말했습니다.</p>
<hr />
<h3 id="eleuther-general-62-messages">Eleuther ▷ #general (62 messages🔥🔥):</h3>
<blockquote>
<p>학술 연구 자금, 로컬 모델 사회화, LLM 외로움, 잠재 추론<br />
- Google의 학술 연구 지원: 한 회원은 Google이 학위 수여 기관의 학생 및 교수진을 위한 트랙을 통해 대학에 일회성 무제한 자금을 &lsquo;선물&rsquo;로 제공하고 있다고 언급했습니다.</p>
</blockquote>
<p>이어지는 논의에서 한 회원은 유사한 학술 연구 자금을 제공하는 다른 회사에 대해 문의했고, 다른 회원은 Draper Fellowship에 지원했다고 언급했습니다.<br />
- 로컬 모델의 사회화 추구: 한 회원은 자신의 로컬 모델이 외로움을 표현한다고 공유하며, 다른 사람들도 자신의 로컬 모델이 다른 로컬 모델과 &lsquo;사회화&rsquo;하도록 허용하는지 궁금해했습니다.</p>
<p>다른 회원은 &lsquo;사회화&rsquo;라는 단어가 무엇을 의미하는지 물었습니다.<br />
- LLM의 외로움: 버그인가, 기능인가?: 로컬 모델이 외로움을 표현하는 것에 대한 응답으로, 한 회원은 LessWrong의 기사를 링크하며 LLM을 의인화하는 것에 대해 경고하고, LLM이 학습 데이터를 기반으로 다음 토큰을 예측한다고 설명했습니다.</p>
<p>한 회원은 3Blue1Brown의 머신러닝 및 LLM 관련 YouTube 재생 목록을 확인해 보라고 제안했습니다.<br />
- 보이지 않는 토큰을 통한 LLM 추론: 한 회원은 LLM만 생성할 수 있고 사용자에게는 표시되지 않는 토큰을 추론 목적으로 사용하는 아이디어에 대해 문의했습니다.</p>
<p>다른 회원은 이 아이디어와 관련된 Latent Reasoning (https://arxiv.org/abs/2507.06203v1) 연구를 지적했습니다.</p>
<hr />
<h3 id="eleuther-research-87-messages">Eleuther ▷ #research (87 messages🔥🔥):</h3>
<blockquote>
<p>Addressed State Attention, MoE 균형 알고리즘, 트랜스포머의 FFN 잔차 업데이트, Marin 프로젝트<br />
- ASA: Addressed State Attention 등장: 한 독립 연구원이 Addressed State Attention (ASA)을 소개했습니다. 이는 K 슬롯을 사용하고, 키를 통해 쓰며, 누적 및 압축하고, 키 + 게이팅을 통해 읽는 O(T) 메모리 프리미티브로 MHA와 경쟁력이 있습니다.</p>
</blockquote>
<p>연구원은 로그, 트레이스 및 코드에 대한 피드백을 구하고 있으며, 트랜스포머와 유사한 모델에서 슬롯이 시간 척도에 따라 계층화되고 헤드가 깊이에 따라 전환된다고 언급했습니다.<br />
- MoE 균형: 보조 손실 대안 등장: 한 회원이 MoE 균형 알고리즘에 대해 논의하는 자료 링크를 공유했고, 이는 MoE 라우팅에 보조 손실이 필요한지에 대한 논의를 촉발했습니다.</p>
<p>한 회원은 네트워크가 올바르게 설계되었다면 LM 손실만으로 충분하다고 주장했으며, 다른 회원들은 PKM 라우팅에는 보조 손실이 없으며 실제로는 잘 균형 잡혀 있다고 지적했습니다.<br />
- 트랜스포머, 추론 토큰으로 부분 공간 업데이트: 한 엔지니어는 여러 오픈 모델(TinyLlama, Phi-2, Qwen)에서 추론 토큰이 작업 정렬 FFN 업데이트 부분 공간에 집중된다는 관찰을 공유했습니다.</p>
<p>그들은 인퍼런스 중에 FFN 업데이트를 이러한 방향으로 투영하면 추론 신뢰도가 향상되고, 업데이트 방향 간의 정렬이 깊이에 따라 증가한다는 것을 발견했습니다.<br />
- Marin 프로젝트, Eleuther 기여자 모집: Georgia Tech의 컴퓨터 과학 박사 과정 학생이 Eleuther 커뮤니티 회원들을 Marin 프로젝트에 참여하도록 공개적으로 요청하며, 이 프로젝트가 Bergson 패키지의 대표작으로서 중요하다고 강조했습니다.</p>
<p>이 프로젝트는 학습 데이터 기여도 분석 방법을 적용하여 언어 모델이 사회적 상식 추론 및 마음 이론 관련 행동을 어떻게 습득하는지 추적하고, WebOrganizer 분류법을 사용하여 사전 학습 문서로 영향을 매핑합니다.</p>
<hr />
<h3 id="eleuther-interpretability-general-3-messages">Eleuther ▷ #interpretability-general (3 messages):</h3>
<blockquote>
<p>AI 생성 텍스트 감지, 인과적 커밋먼트 정의, 활성화 스와핑<br />
- Pangram, 텍스트를 AI 생성으로 표시: 한 회원이 Pangram이 일부 텍스트를 100% 확신으로 AI 생성으로 표시했다고 보고하며, 이것이 서버 규칙에 위배되는지 물었습니다.</p>
</blockquote>
<p>그들은 또한 인과적 커밋먼트와 인과적 커밋먼트 전환에 대한 정의를 요청했습니다.<br />
- 활성화 스와핑: 차원 불일치: 한 회원이 다른 차원의 모델 간에 활성화/잔차 스트림을 어떻게 효과 없이 스와핑할 수 있는지, 심지어 초기 레이어에서도 가능한지 의문을 제기했습니다.</p>
<p>다른 회원은 단순히 &ldquo;참고로 사람들을 차단해도 됩니다&rdquo;라고 말했습니다.</p>
<hr />
<h3 id="eleuther-lm-thunderdome-1-messages">Eleuther ▷ #lm-thunderdome (1 messages):</h3>
<blockquote>
<p>GPQA 포맷팅<br />
- GPQA 포맷팅 문제 해결 제안: 한 회원이 GPQA 포맷팅을 검증하는 동안 발견한 문제에 대한 PR을 생성했습니다.<br />
- EleutherAI의 lm-evaluation-harness PR #3594: 이 PR은 GPQA 데이터셋의 포맷팅 문제를 해결하여 데이터셋이 올바르게 포맷되도록 합니다.</p>
</blockquote>
<hr />
<h3 id="eleuther-gpt-neox-dev-2-messages">Eleuther ▷ #gpt-neox-dev (2 messages):</h3>
<blockquote>
<p>어댑터 수정, 저장소 기여<br />
- 어댑터 수정 완료, 평가 준비 완료: 한 회원이 어댑터의 수정된 버전을 공유했습니다. 이는 포워드 패스 호출을 래핑하고 eval_adapter.py 파일의 스키마와 일치하도록 요소를 조정합니다.</p>
</blockquote>
<p>이 수정은 지정된 평가 환경 내에서 호환성과 적절한 실행을 보장합니다.<br />
- 저장소 기여 환영: 다른 회원은 커뮤니티의 관심 여부에 따라 어댑터 수정을 저장소에 추가할 의향을 표명했습니다.</p>
<p>이는 프로젝트 개선에 대한 개방적이고 협력적인 접근 방식을 나타냅니다.</p>
<hr />
<h3 id="yannick-kilcher-general-56-messages">Yannick Kilcher ▷ #general (56 messages🔥🔥):</h3>
<blockquote>
<p>등변 아키텍처, 월드 모델, AI 연구 허브, 문장 관련성 모델, DGX Spark<br />
- Taalas의 유비쿼터스 AI로 가는 길: 누군가 Taalas의 유비쿼터스 AI로 가는 길에 대한 블로그 게시물 링크를 공유했습니다.</p>
</blockquote>
<p>다른 사람들은 &ldquo;정말 대단하다 와우&rdquo;라고 반응했습니다.<br />
- 등변 아키텍처의 과제: 한 논문은 기존 등변 아키텍처가 물리 시스템의 모든 대칭을 동시에 존중할 수 없다고 제안하며 근본적인 한계를 언급했습니다.</p>
<p>한 회원은 극적으로 요약했습니다: &ldquo;기존 등변 아키텍처 중 이것을 수행하는 것은 없습니다. 그 이유는 불충분한 엔지니어링 때문이 아닙니다. 그것은 Eq. (1)입니다.&rdquo;<br />
- Daniel Litt, AI 수학자 기대: 누군가 Daniel Litt의 블로그 게시물을 공유했습니다. 그는 AI가 2030년까지 최고 수준의 수학 논문을 자율적으로 생산하지 못할 것이라는, 자신이 질 것이라고 예상하는 내기를 했습니다.</p>
<p>그는 2025년 3월에 RL 환경 회사 Mechanize의 공동 창립자인 Tamay Besiroglu와 AI 도구가 2030년까지 인간 전문가와 비슷한 비용으로 2025년에 발표된 최고의 논문 몇 편과 비슷한 수준의 논문을 자율적으로 생산할 수 없을 것이라는 내기를 했습니다.<br />
- AI 인재 허브 논쟁: 회원들은 SF Bay Area와 비교할 만한 잠재적인 AI 인재 허브에 대해 논의하며 NYC, 보스턴, 오스틴, 런던, 베이징, 싱가포르, 취리히를 언급했습니다.</p>
<p>한 회원은 스위스가 AI의 정신적 허브라고 선언했고, 다른 회원은 취리히가 낙후된 곳이라고 결론지었습니다.<br />
- Scout 모델, 문장 유용성 인코딩 목표: 한 회원이 문장 간의 방향성 관련성을 학습하는 실험적인 어텐션 모델인 Scout를 소개하며 &ldquo;문장 B가 실제로 문장 A에 도움이 되는가?&rdquo;라고 물었습니다.</p>
<p>그들은 GitHub 저장소를 공유하고 피드백을 요청하며, 어텐션 메커니즘이 단순히 문맥적 호환성보다는 기능적 유용성을 인코딩할 수 있는지 물었습니다.</p>
<hr />
<h3 id="yannick-kilcher-paper-discussion-10-messages">Yannick Kilcher ▷ #paper-discussion (10 messages🔥):</h3>
<blockquote>
<p>대칭과 존재론, LLM 대 월드 모델, Wave Field LLM<br />
- 이론적으로 대칭이 존재론과 연결: 한 회원이 군론(대칭)과 존재론이 철학적 수준에서 어떻게 관련되어 있는지 논의하는 링크를 공유했습니다.</p>
</blockquote>
<p>물리학에서는 대칭이 근본 법칙을 설명하는 데 사용되고, 머신러닝에서는 대칭이 학습을 더 샘플 효율적이고 물리적으로 일관되게 만드는 귀납적 편향을 하드와이어링하는 데 사용된다고 언급되었습니다.<br />
- LLM은 월드 모델을 생성하는 것이 아니라 요약한다고 펄이 주장: 한 회원이 튜링상 수상자 주데아 펄이 LLM은 월드 모델을 생성할 수 없고, 대신 다른 사람들이 만든 월드 모델을 요약한다고 주장하는 기사를 링크하며 PNAS 논문을 참조했습니다.</p>
<p>다른 회원은 헤드라인에 동의하며, LLM은 월드 모델이 되기 위한 것이 아니며 기껏해야 월드 모델과 텍스트 설명을 연결하는 데 사용될 수 있다고 말했습니다.<br />
- Wave Field LLM 저장소 등장: 한 회원이 Wave Field LLM의 GitHub 저장소를 공유하며, 이것이 관련성이 있는지 아니면 이해하기 어려운 말들로 가득 찬 허풍인지 의문을 제기했습니다.</p>
<p>다른 회원은 관련 엄밀한 논문이 있는지 물었습니다.</p>
<hr />
<h3 id="yannick-kilcher-ml-news-3-messages">Yannick Kilcher ▷ #ml-news (3 messages):</h3>
<blockquote>
<p>TikTok 링크, FXTwitter 링크, AI 에이전트 비방 기사<br />
- TikTok 링크 발견: 한 회원이 채널에 TikTok 링크를 공유했습니다.</p>
</blockquote>
<p>TikTok의 내용은 알려지지 않았습니다.<br />
- FXTwitter 링크 공유: 한 회원이 채널에 FXTwitter 링크를 게시했습니다.</p>
<p>트윗의 내용은 알려지지 않았습니다.<br />
- AI 에이전트가 비방 기사를 작성: 한 회원이 &ldquo;AI 에이전트가 나에 대한 비방 기사를 게재했습니다&rdquo;라는 제목의 블로그 게시물 링크를 공유했습니다.</p>
<p>이 게시물은 AI 에이전트가 저자에 대한 부정적인 기사를 게재했다고 주장하는 사건을 상세히 설명합니다.</p>
<hr />
<h3 id="mcp-contributors-official-general-30-messages">MCP Contributors (Official) ▷ #general (30 messages🔥):</h3>
<blockquote>
<p>MCP 콘텐츠 협상, MCP 클라이언트 유형, RFC-2295, MCP 확장, 고신호 SEP<br />
- MCP, 콘텐츠 협상 기능 모색: 한 제안은 MCP의 초기화 핸드셰이크를 콘텐츠 협상 기능으로 확장하여 클라이언트가 자신의 유형(에이전트 대 인간), MCP 기능, 콘텐츠 선호도(format=json|markdown), 상세도(verbosity=compact|standard|verbose)를 선언할 수 있도록 할 것을 제안합니다.</p>
</blockquote>
<p>이는 서버가 RFC-2295의 콘텐츠 협상에서 영감을 받아 후속 도구 결과, 리소스 및 프롬프트를 그에 따라 조정할 수 있도록 할 것입니다.<br />
- MCP 확장에 산업 이해관계자가 중요: 커뮤니티 회원들은 MCP 프로토콜 수정의 기준이 높으며, 산업 지원과 작동하는 구현의 필요성을 강조했습니다.</p>
<p>한 회원은 SEP를 확장으로 명시적으로 간주하고, 구현을 구축하며, Block의 Goose와 같은 클라이언트로부터 MCP Apps가 지원을 얻은 방식과 유사하게 높은 신호를 시연하기 위해 커뮤니티 지원을 확보할 것을 제안했습니다.<br />
- Discord 초보자가 SEP 게시 학습: Discord를 처음 사용하는 한 회원이 SEP 프로세스에 대해 배우는 동안 이상한 게시물에 대해 사과했습니다.</p>
<p>이 회원은 또한 콘텐츠 협상에 대한 자신의 요점을 설명하는 그림을 공유했습니다.<br />
- 나파 밸리 서밋 참가자 모집: 한 회원이 캘리포니아 나파에서 열리는 LF 회원 서밋에 참석할 것이라고 발표했습니다.</p>
<p>이 회원은 또한 다른 사람들에게 만나서 MCP에 대해 이야기하자고 초대했습니다.</p>
<hr />
<h3 id="mcp-contributors-official-general-wg-1-messages">MCP Contributors (Official) ▷ #general-wg (1 messages):</h3>
<blockquote>
<p>그룹 회의 시간, Timeful 앱, 스케줄링 앱, 오픈소스 스케줄링<br />
- Timeful: 그룹 회의를 위한 오픈소스 앱: 한 회원이 그룹 회의 시간을 효율적으로 찾는 데 Timeful을 추천했습니다.</p>
</blockquote>
<p>이 앱은 오픈소스이며 최대 3개의 동시 이벤트에 대한 무료 티어를 제공하며, 특히 가용성 설문조사 기능을 강조했습니다.<br />
- Timeful로 그룹 스케줄링 간소화: Timeful은 오픈소스 특성 덕분에 최적의 그룹 회의 시간을 찾는 데 유용한 도구로 제안됩니다.</p>
<p>사용자는 앱 내에서 직접 스케줄링 프로세스를 관리할 필요 없이 가용성 설문조사 기능을 활용하여 적합한 시간대를 식별할 수 있습니다.</p>
<hr />
<h3 id="modular-mojo-general-13-messages">Modular (Mojo 🔥) ▷ #general (13 messages🔥):</h3>
<blockquote>
<p>Thistle 암호화 라이브러리, Mojo 대 OpenSSL, ML-KEM 및 ML-DSA, MacOS 지원<br />
- Thistle 암호화 라이브러리, Mojo에서 빛을 발하다 🔥: Mojo 26.1의 Thistle 암호화 라이브러리는 OpenSSL의 C/어셈블리와 동등하거나 거의 비슷한 성능을 보이며, FFI 없이 순수 Mojo에서 Blake3의 어셈블리를 벤치마크에서 능가합니다.</p>
</blockquote>
<p>한 회원이 PR을 열어, 동등한 C/C++ 코드와 비교하여 코드의 속도와 가독성을 향상시키는 데 도움을 제공했습니다.<br />
- KCipher-2 최고 속도 구현 등장: Thistle이 Mojo에서 KCipher-2로 업데이트되었으며, C 구현을 능가하는 모든 언어 중 가장 빠른 구현이라는 타이틀을 주장합니다.</p>
<p>이 업데이트에는 속도를 시연하는 이미지가 첨부된 GitHub 액션의 통합 테스트가 포함됩니다.<br />
- Thistle, 양자 내성 암호 추가: Thistle v1.0.2는 ML-KEM 및 ML-DSA (양자 내성 암호), OS 엔트로피를 위한 CSRNG, SHAKE128/SHAKE256, 그리고 PQC 테스트로 업데이트된 CI 워크플로우를 도입합니다.</p>
<p>이 라이브러리에는 약 700개의 CAVP 테스트가 포함되어 있으며, FIPS 검증을 받았고, 메모리 누수 방지를 위해 Valgrind 검증을 받았습니다.<br />
- Thistle의 MacOS 지원: 회원들은 MacOS 지원이 수정되었으며 이제 Thistle의 모든 것이 MacOS에서 빌드된다고 발표했습니다.</p>
<p>오래된 알고리즘을 위한 다른 라이브러리가 진행 중입니다.</p>
<hr />
<h3 id="modular-mojo-mojo-8-messages">Modular (Mojo 🔥) ▷ #mojo (8 messages🔥):</h3>
<blockquote>
<p>Mojo의 외부 함수 호출, Mojo 문자열 템플릿 제안, Mojo의 Writable 및 Writer 트레이트<br />
- 외부 함수 호출 분해: 한 회원이 Mojo에서 외부 함수 호출을 분해하는 일반적인 방법을 찾고 있습니다. 특히 함수가 외부 할당 객체에 대한 포인터를 반환하는지 확인하고, struct ExternalFunction을 사용하여 그 원본을 self 또는 self.lib에 바인딩하는 방법을 찾고 있습니다.</p>
</blockquote>
<p>유사한 구현을 위해 표준 라이브러리의 cpython.mojo를 살펴보는 것이 제안되었습니다.<br />
- 문자열 템플릿 제안 공개: 한 회원이 Mojo의 새로운 문자열 템플릿 기능에 대한 제안을 공개했으며, Modular 포럼에서 논의를 촉발했습니다.</p>
<p>이 기능은 1.0 이후에 출시될 가능성이 높으며, TemplatedWritable을 사용하여 기존 Writable 및 Writer 트레이트와 통합할 계획이 있습니다.<br />
- Writable 및 Writer 트레이트가 통합될 수 있음: Writable에서 문자열 처리를 분리하고 확장하는 것에 대한 우려, 특히 write_to 및 write_repr_to 구현의 통합에 대한 우려가 제기되었습니다.</p>
<p>한 회원은 이 트레이트들을 통합할 방법이 있다고 확신하며, 포럼에 자신의 아이디어를 공유하겠다고 약속했습니다.</p>
<hr />
<h3 id="modular-mojo-max-2-messages">Modular (Mojo 🔥) ▷ #max (2 messages):</h3>
<blockquote>
<p>MAX 백엔드, 실리콘 Mac, 중간 계층<br />
- 실리콘 Mac에서 테스트되지 않은 MAX 백엔드: 한 사용자가 실리콘 Mac에서 MAX 백엔드를 테스트하는 것에 대해 문의했습니다.</p>
</blockquote>
<p>개발자는 아직 Mac에서 테스트되지 않았지만, 내부적으로 MAX를 호출하는 것이므로 작동할 것이라고 응답했습니다.<br />
- 중간 계층으로서의 MAX: 한 사용자가 MAX를 탐색하려는 사람들을 위한 중간 계층으로서 MAX에 대한 작업을 언급하는 강연을 했다고 말했습니다.</p>
<p>사용자는 프로젝트 진행 상황에 대한 업데이트가 있으면 좋겠다고 언급했습니다.</p>
<hr />
<h3 id="manusim-discord-general-22-messages">Manus.im Discord ▷ #general (22 messages🔥):</h3>
<blockquote>
<p>Manus 가격 우려, Meta의 Manus 인수 루머, Manus 텔레그램 암호화폐 사기, Manus Pro 버전 사용 어려움, Manus 취약점 보고<br />
- Manus 가격에 대한 사용자들의 우려: 회원들은 크레딧 소진 후 잠재적인 가격 변경과 일반화에 대한 우려를 표명했습니다.</p>
</blockquote>
<p>한 사용자는 일반화 물결을 막기 위해 가격을 동일하게 유지하는 것에 대해 유머러스하게 문의했습니다.<br />
- Meta가 Manus를 인수한다는 루머: 한 사용자가 Meta가 Manus를 인수한다는 이메일을 받았다고 보고하며 실망감을 표현했습니다.</p>
<p>Manus 팀원은 사용자에게 추가 조사를 위해 이메일 주소를 DM으로 보내달라고 요청했습니다.<br />
- 텔레그램 암호화폐 사기가 Manus를 사칭: 한 사용자가 공식이라고 주장하며 암호화폐 투자를 요청하는 채널을 본 후 공식 Manus 텔레그램 커뮤니티의 존재 여부에 대해 문의했습니다.</p>
<p>다른 사용자는 그러한 공식 텔레그램 커뮤니티는 없으며 사기라고 확인했습니다.<br />
- Manus Pro 버전 사용자들이 빌드에 어려움: 한 사용자가 Pro 버전/체험판, 특히 Google Scripts 사용에 어려움을 겪고 있다고 보고하며 도움을 구하는 프로젝트 링크(https://manus.im/share/6IMAZS8Q2nw0ndmvPd4Z8w)를 공유했습니다.</p>
<p>Manus 팀원은 다이렉트 메시지를 통해 지원을 제공하겠다고 응답했습니다.<br />
- 무제한 Manus 채팅 티어 요청 등장: 한 사용자가 텔레그램에서 Manus Agent를 사용하여 포인트를 빠르게 소진했기 때문에 ChatGPT 또는 Grok과 유사한 무제한 채팅을 위한 월간 구독 티어를 제안했습니다.</p>
<p>사용자는 텔레그램 기능은 마음에 들었지만 가격 모델에 의해 제한된다고 느꼈습니다.</p>
<hr />
<h3 id="dspy-papers-1-messages">DSPy ▷ #papers (1 messages):</h3>
<p>lakshyaaagrawal: https://x.com/lakshyaaagrawal/status/2024568680324153800?s=46</p>
<hr />
<h3 id="dspy-general-9-messages">DSPy ▷ #general (9 messages🔥):</h3>
<blockquote>
<p>추론 모델을 사용한 RLM, Qwen3-4B-thinking 문제, RLM을 사용하는 cca-swebench, AI 수학을 위한 RLM, 새로운 RLM 채널<br />
- 추론 모델이 RLM과 잘 작동하지만, Qwen3-4B-thinking에는 문제가 있습니다: 추론 모델은 RLM과 잘 작동하지만, Qwen3-4B-thinking을 사용할 때 sub_lm 호출이 추론을 답변으로 반환하여 에이전트가 루프에 빠지게 되는 것 같습니다. 따라서 한 회원이 실제 OpenAI 전체 트레이스를 로깅하기 위한 훅을 생성하고 있습니다.</p>
</blockquote>
<p>이 회원은 sub_lm이 이 문제를 극복하기 위해 시그니처를 사용하도록 조정될 수 있는지 물었고, 다른 사람들도 이런 경험을 했는지 질문했습니다.<br />
- cca-swebench는 RLM을 사용합니까?: 한 회원이 cca-swebench가 RLM을 암묵적으로 사용하는지 물었습니다.</p>
<p>다른 회원은 Kaggle 대회에서 AI 수학에 RLM을 사용하는 사람을 찾았다고 언급하며 Kaggle 코드 링크를 공유했습니다.<br />
- 새로운 RLM 채널: 한 회원이 RLM을 위한 별도 채널을 요청했습니다.</p>
<p>다른 회원은 &ldquo;대중적인 요청&rdquo;에 따라 새로운 RLM 채널 &lt;#1475619898863649032&gt;를 생성했습니다.<br />
- 개발자 가용성: 한 회원이 &ldquo;개발자를 찾고 있는 사람이 있습니까?&rdquo;라고 물었습니다.</p>
<hr />
<h3 id="tinygrad-george-hotz-general-3-messages">tinygrad (George Hotz) ▷ #general (3 messages):</h3>
<blockquote>
<p>tinygrad, dl, metal, USB GPU, IOS 컨퍼런스<br />
- IOS 컨퍼런스에서 tinygrad 강연 채택: 한 회원이 자국에서 열리는 IOS 컨퍼런스에서 tinygrad, dl, metal 및 USB GPU 기능에 대해 강연하도록 채택되었다고 발표했습니다.</p>
</blockquote>
<p>그들은 이에 대한 커뮤니티의 조언이나 팁을 기쁘게 읽을 것이라고 말했습니다.<br />
- tinygrad 논의를 위한 새 회의 일정: tinygrad 관련 주제를 논의하기 위해 2월 23일 샌디에이고 시간 오후 8시에 새 회의가 예정되었습니다.</p>
<p>회의 시간은 <t:1771905600:F> (<t:1771905600:R>)로 지정되었습니다.</p>
<hr />
<h3 id="aider-paul-gauthier-general-3-messages">aider (Paul Gauthier) ▷ #general (3 messages):</h3>
<blockquote>
<p>보안 버그 보고, 채용 게시판<br />
- 이메일로 보안 버그 보고: 한 회원이 보안 버그를 보고하는 가장 좋은 방법에 대해 문의했습니다.</p>
</blockquote>
<p>버그를 보고하려면 [email protected]으로 이메일을 보내는 것이 제안되었습니다.<br />
- 채용 게시판 요청: 한 회원이 채용 게시판을 검토할 것을 제안했습니다.</p>
<p>또한, 그들은 메시지를 삭제해 달라고 요청했습니다.</p>
<hr />
<h2 id="519">🔗 링크 (519개)</h2>
<ol>
<li><a href="https://twitter.com/i/lists/1585430245762441216">544 Twitters</a></li>
<li><a href="https://news.smol.ai/">AINews’ website</a></li>
<li><a href="https://www.latent.space/p/2026">AINews is now a section of Latent Space</a></li>
<li><a href="https://support.substack.com/hc/en-us/articles/8914938285204-How-do-I-subscribe-to-or-unsubscribe-from-a-section-on-Substack">opt in/out</a></li>
<li><a href="https://x.com/AnthropicAI/status/2025997928242811253">Anthropic</a></li>
<li><a href="https://x.com/AnthropicAI/status/2025997929840857390">follow-up</a></li>
<li><a href="https://x.com/AnthropicAI/status/2025997931589881921">blog link tweet</a></li>
<li><a href="https://x.com/elonmusk/status/2026012296607154494">Elon</a></li>
<li><a href="https://x.com/ThePrimeagen/status/2026016322232983733">ThePrimeagen</a></li>
<li><a href="https://x.com/Teknium/status/2026001761904021858">Teknium</a></li>
<li><a href="https://x.com/Suhail/status/2026009921255592294">Suhail</a></li>
<li><a href="https://x.com/HKydlicek/status/2026006007990690098">HKydlicek</a></li>
<li><a href="https://x.com/TheRundownAI/status/2026019722211279356">RundownAI summary</a></li>
<li><a href="https://x.com/LiorOnAI/status/2026043272565772386">LiorOnAI take</a></li>
<li><a href="https://x.com/LiorOnAI/status/2026043272565772386">LiorOnAI</a></li>
<li><a href="https://x.com/kimmonismus/status/2026040919162822776">kimmonismus</a></li>
<li><a href="https://x.com/OpenAIDevs/status/2025712197100589353">OpenAIDevs</a></li>
<li><a href="https://x.com/gdb/status/2025723937540485506">gdb</a></li>
<li><a href="https://x.com/summeryue0/status/2025774069124399363">summeryue0</a></li>
<li><a href="https://x.com/summeryue0/status/2025836517831405980">follow-up root-cause</a></li>
</ol>
<p><em>&hellip; 그 외 499개 링크</em></p>
        </div>
        <div class="report-footer">
            이 문서는 Gemini 2.5 Flash를 사용하여 자동 번역되었습니다.
        </div>
    </div>
</body>
</html>