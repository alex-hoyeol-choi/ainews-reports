# OpenAI closes $110B raise from Amazon, NVIDIA, SoftBank in largest startup fundraise in history @ $840B post-money - 요약

**원문 URL**: https://www.latent.space/p/ainews-openai-closes-110b-raise-from
**번역일**: 2026-02-28 12:35
**발행일**: 2026-02-28

---

다음은 바쁜 기술 경영진과 AI 엔지니어를 위한 AI 뉴스 브리핑입니다.

### 🔥 주요 뉴스
**[OpenAI, $110B 투자 유치 및 기업 가치 $840B 달성]** — OpenAI가 SoftBank, NVIDIA, Amazon으로부터 총 $110B 규모의 투자를 유치하며 투자 후 기업 가치 $840B를 기록했습니다. 이는 역사상 최대 규모의 스타트업 자금 조달 기록입니다.
**[OpenAI 사용자 지표 급증]** — OpenAI는 Codex 주간 사용자 수가 160만 명으로 세 배 이상 증가했으며, 9백만 명 이상의 유료 비즈니스 사용자와 9억 명 이상의 ChatGPT 주간 활성 사용자를 확보했다고 밝혔습니다. 또한 5천만 명 이상의 소비자 구독자를 보유하고 있습니다.
**[Anthropic, 미 국방부 계약 거부 및 공급망 위험 지정]** — Anthropic이 대규모 국내 감시 및 완전 자율 무기 개발을 위한 미 국방부(DoD)와의 계약을 공개적으로 거부했습니다. 이에 DoD는 Anthropic을 "국가 안보에 대한 공급망 위험"으로 지정했으며, Anthropic은 이에 대해 공식 성명을 발표하고 법적 이의를 제기할 의사를 밝혔습니다.
**[Sakana AI, Doc-to-LoRA 및 Text-to-LoRA 공개]** — Sakana AI가 단일 포워드 패스로 LoRA 어댑터를 생성하는 하이퍼네트워크 기반의 Doc-to-LoRA 및 Text-to-LoRA 방법을 소개했습니다. Doc-to-LoRA는 기반 모델 컨텍스트 윈도우보다 약 5배 긴 시퀀스에서 거의 완벽한 정확도를 보고하며, 크로스-모달 정보 전송도 시연했습니다.
**[Alibaba, Qwen3.5 모델 확장 출시]** — Alibaba가 Qwen3.5의 27B dense, 122B A10B MoE, 35B A3B MoE 버전을 Apache 2.0 라이선스로 출시했습니다. 이 모델들은 262K 컨텍스트를 지원하며, YaRN을 통해 1M까지 확장 가능합니다.

### 📊 모델 & 벤치마크
*   Alibaba의 Qwen3.5 27B, 122B A10B, 35B A3B MoE 모델이 출시되었으며, Artificial Analysis 인텔리전스 인덱스 점수는 각각 42, 42, 37을 기록했습니다.
*   2026년 2월 Arena 리더보드에서 텍스트 부문 상위 3개 모델은 GLM-5 (1455), Qwen-3.5 397B A17B (1454), Kimi-K2.5 Thinking (1452)으로 나타났습니다.
*   코드 Arena 리더보드에서는 GLM-5 (1451)가 1위를 차지했으며, Kimi-K2.5와 MiniMax-M2.5가 공동 2위를 기록했습니다.
*   Perplexity가 양방향 "Qwen3-retrained" 임베딩 모델(0.6B/4B)을 MIT 라이선스로 오픈소스화하여 리트리벌을 위한 문서 수준 이해를 개선했다고 밝혔습니다.

### 🛠️ 제품 & 도구
*   OpenAI가 배포 안전성 문서를 위한 검색 가능한 웹사이트인 "Deployment Safety Hub"를 출시하여 "시스템 카드"에 대한 접근성을 높였습니다.

### 🔬 연구 & 논문
*   Sakana AI는 하이퍼네트워크를 사용하여 단일 포워드 패스로 LoRA 어댑터를 생성하는 Doc-to-LoRA 및 Text-to-LoRA 방법을 발표했습니다. Doc-to-LoRA는 긴 컨텍스트에서 높은 정확도를 보이며 크로스-모달 트릭을 시연했습니다.
*   DeepSeek, THU, PKU 연구팀은 RDMA를 통해 디코드 노드의 유휴 스토리지 NIC 대역폭을 활용하여 에이전틱 긴 컨텍스트 인퍼런스의 KV-cache 이동 병목 현상을 해결하는 시스템 수준 재설계를 제안했습니다.
*   Databricks MosaicAI는 GRPO와 같거나 능가하는 성능을 약 3배 적은 학습 세대로 달성하는 안정적인 오프-폴리시 RL 대안인 OAPL (Optimal Advantage-based Policy Optimization with lagged inference policy)을 발표했습니다.
*   Turing Post는 에피소드 내 반성/재시도 및 디스틸레이션을 삽입하는 경험적 강화 학습(ERL)이 표준 RLVR 대비 Sokoban에서 +81%의 성능 향상을 보고했다고 설명했습니다.

### 💰 산업 동향
*   OpenAI의 $110B 투자 라운드에는 SoftBank가 $30B, NVIDIA가 $30B, Amazon이 $50B를 투자했습니다.
*   Amazon은 OpenAI에 초기 $15B 투자 후 특정 조건 충족 시 수개월 내 $35B를 추가 투자하며, Amazon Bedrock에서 OpenAI가 구동하는 "Stateful Runtime Environment"를 제공하고 AWS가 OpenAI Frontier의 독점적인 서드파티 클라우스 제공업체가 될 예정입니다.
*   미 국방부가 Anthropic을 "국가 안보에 대한 공급망 위험"으로 지정하고 계약자/파트너에게 압력을 가하려는 움직임을 보였습니다.

### ⚡ 인프라 & 하드웨어
*   NVIDIA의 OpenAI 투자는 3 GW의 전용 인퍼런스 용량과 Vera Rubin 시스템에서의 2 GW 학습 사용을 포함합니다.
*   Amazon은 AWS 인프라를 통해 "8년간 $100B" 가치의 2기가와트 Trainium 용량(Trainium3 및 차세대 Trainium4 칩 모두 포함)을 OpenAI에 제공할 예정입니다.
*   vLLM은 ROCm용 7가지 어텐션 백엔드를 발표하며 AMD GPU(MI300X/MI325X/MI355X)에서 최대 4.4배의 디코드 처리량 향상을 보고했습니다.

---

*이 문서는 Latent Space AINews 뉴스레터를 자동 요약한 것입니다.*
