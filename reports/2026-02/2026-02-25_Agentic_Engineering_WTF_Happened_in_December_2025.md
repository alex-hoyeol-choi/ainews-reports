# Agentic Engineering: WTF Happened in December 2025?

**원문 URL**: https://news.smol.ai/issues/2026-02-25-wtf-happened
**번역일**: 2026-02-27 05:47
**발행일**: 2026-02-25

---

코딩이 영원히 바뀌었다는, '일반적인' 과장 광고보다 훨씬 더 큰 불안감이 커지고 있습니다.
> 2026년 2월 24일~2월 25일 AI 뉴스입니다. 12개 서브레딧, 544개 트위터, 24개 디스코드(262개 채널, 10751개 메시지)를 확인했습니다. 절약된 예상 독서 시간(200wpm 기준): 1086분. AINews 웹사이트에서 지난 모든 이슈를 검색할 수 있습니다. 참고로 AINews는 이제 Latent Space의 한 섹션입니다. 이메일 수신 빈도를 선택/해제할 수 있습니다!
저희는 이를 위한 마이크로사이트를 만들었습니다:

# https://wtfhappened2025.com/
https://wtfhappened2025.com/
지금 방문하십시오.

---

# AI 트위터 요약
Perplexity “Computer”: 오케스트레이션 우선 에이전트 제품 (멀티 모델, 도구+환경, 사용량 기반 가격 책정)
- Perplexity Computer 출시: Perplexity는 파일, 도구, 메모리 및 모델을 하나의 인터페이스에서 오케스트레이션하여 프로젝트를 "연구, 설계, 코딩, 배포 및 관리"할 수 있는 엔드투엔드 시스템으로 포지셔닝한 Computer를 발표했습니다 (출시 트윗, Arav Srinivas). 주요 제품 신호:

접근성 + 가격 책정: Max 구독자에게 웹에서 먼저 제공되며, 이후 Pro/Enterprise 사용자에게 제공됩니다. 서브 에이전트 모델 선택, 지출 한도, Max 구독자에게 포함된 크레딧(월 1만 개) 및 기간 한정 보너스 크레딧 지급이 포함된 사용량 기반 가격 책정입니다 (가격 세부 정보, 가용성, Arav의 출시 관련 설명).
아키텍처 강조: 여러 트윗에서 "획기적인 발전"은 단일 모놀리식 에이전트 루프가 아니라 코디네이터 모델이 전문가 모델(연구 vs 코딩 vs 미디어)에 작업을 할당하는 병렬, 비동기 서브 에이전트라고 강조합니다 (Lior의 분석, Denis Yarats).
"모든 것이 컴퓨터다" 내러티브: Perplexity 직원들은 Computer가 소규모 팀이 광범위하게 코딩 에이전트와 자동화된 평가/디버그 루프를 사용하여 구축한 플랫폼이라고 강조했습니다 (Arav, Denis).
- 엔지니어에게 중요한 이유: Computer는 멀티 모델 라우팅, 격리/샌드박스, 영구 메모리 및 비용 제어와 같은 시스템 수준 에이전트 UX를 향한 구체적인 추진입니다. 즉, "에이전틱 작업"을 단일 채팅 세션이 아닌 분산 워크플로우로 취급하는 것입니다 (Arav, Computer 사이트).
코딩 에이전트: "12월부터 작동하기 시작했습니다" + 새로운 모델/툴링 출시 (GPT-5.3-Codex, Claude Code 에코시스템, Copilot CLI GA)
- Karpathy의 "상변화" 주장: Andrej Karpathy는 코딩 에이전트가 12월 이후 질적 임계점을 넘어섰다고 주장합니다. 취약한 데모에서 일관성과 끈기를 가지고 지속적이고 장기적인 작업 완료로 발전했다는 것입니다. 그는 최소한의 개입으로 엔드투엔드 로컬 배포(SSH 키 → vLLM → 모델 다운로드/벤치마크 → 서버 엔드포인트 → UI → systemd → 보고서)를 위임하는 상세한 예를 제시합니다 (Karpathy). 이는 개발 도구 빌더 및 사용자(Cursor, snowmaker)의 광범위한 "소프트웨어가 변화하고 있다"는 정서와 일치합니다.
- OpenAI GPT-5.3-Codex 출시 + 초기 평가 소문:

OpenAI는 API에 GPT-5.3-Codex를 출시했으며 (snsf), Cline은 약 25% 더 빠른 속도, 작업당 더 적은 토큰, 강력한 SWE-Bench Pro 성능 향상을 주장하며 지원을 발표했습니다 (Cline).
커뮤니티 벤치마크 반응은 날카롭고(시끄러웠습니다): 예를 들어, "IBench에서 86%"라는 놀라움 (트윗)과 "첫 벤치마크가 들어오고 있습니다" (kimmonismus). 방법론이 명확해질 때까지는 이것들을 방향성 지표로만 취급하십시오.
- Claude Code: 제품 성숙도 + 관측 가능성 + 통합:

Claude Code의 "첫 번째 생일" 프레이밍과 회고는 이를 근본적인 코딩 에이전트 제품으로 강조하며, 컨텍스트 길이 스케일링이 메모리 제약에 부딪히는 문제에 대한 우려도 언급합니다 (swyx).
실용적인 에코시스템 요소: Claude Code용 Slack 플러그인 통합 (catwu); Claude Code의 "너핑"/라우팅 문제를 디버깅하기 위한 LangSmith 트레이싱 (hwchase17, 관측 가능성 불만).
- GitHub Copilot CLI가 GA(General Availability)를 달성하고 "/research"를 추가했습니다:

Copilot CLI는 GA에 도달했으며 (Evan Boyle), GitHub 코드 검색 + MCP 기반 동적 페칭을 사용하여 리포지토리 전반의 심층 연구를 위한 /research를 추가하고, 공유를 위한 gist로 보고서를 내보낼 수 있습니다 (기능).
작은 UX 참고 사항: 터미널의 Copilot CLI는 실시간으로 제목을 업데이트합니다 (트윗).
오픈 모델 및 로컬 인퍼런스: Qwen3.5 "Medium" 물결 (MoE + 긴 컨텍스트 + FP8/양자화), 그리고 로컬 에이전트 전환점
- Qwen3.5 Medium 시리즈 배포 공세: Alibaba는 vLLM, GGUF, LM Studio, Ollama, Jan에 출시 당일 툴링 지원을 추진하며, 주요 오픈 릴리스를 위한 배포 스택이 얼마나 빨라졌는지 강조했습니다 (vLLM 감사, GGUF, LM Studio, Ollama, Jan).
- Qwen의 주요 기술 주장 (게시된 내용이며, 여기서 독립적으로 검증되지 않음):

양자화 견고성: 4비트 가중치 + KV-캐시 양자화에서 "거의 손실 없는" 정확도.
긴 컨텍스트: Qwen3.5-27B는 800K 이상, 35B-A3B는 32GB VRAM 소비자 GPU에서 1M 이상, 122B-A10B는 80GB GPU에서 1M 이상의 컨텍스트를 지원합니다.
오픈 베이스: Qwen은 연구 지원을 위해 Qwen3.5-35B-A3B-Base를 오픈소스화했습니다 (Alibaba_Qwen).
vLLM/SGLang 네이티브 지원과 함께 FP8 가중치 오픈 (FP8 발표).
- 로컬 에이전트 "전/후": 주목할 만한 실무자 주장은 Qwen3.5-35B-A3B가 로컬 에이전트 루프를 의미 있게 더 신뢰할 수 있게(도구 호출, 안정성) 만들며 토큰당 약 3B 파라미터만 활성화한다는 것입니다. 이는 Claude Code/Codex와 함께 많은 워크플로우에서 로컬이 실행 가능하다고 명시적으로 포지셔닝하는 것입니다 (victormustar).
- 평가 담론 경고: 벤치맥싱 및 MoE vs 덴스 혼란:

여러 스레드에서 리더보드를 과도하게 해석하는 것에 대해 경고하고("벤치맥싱에 속지 마세요") (scaling01), 일부 벤치마크에서 Qwen 크기 전반에 걸쳐 놀라운 동등성을 보여주며, 이는 툴링 효과 또는 벤치마크 아티팩트를 시사한다고 강조합니다 (eliebakouch, teortaxesTex on HLE/MoE interpretation).
Arena는 직접 비교를 위해 Text/Vision/Code Arena에 Qwen3.5 Medium을 추가했습니다 (Arena).
에이전트, 신뢰성, 그리고 "에이전트를 위한 구축": 최소한의 벤치마크, 도구 인터페이스 최적화, 그리고 실패 모드
- 신뢰성은 기능만큼 향상되지 않았습니다: 신뢰성 중심의 연구는 빠른 모델 발전에도 불구하고 신뢰성 향상은 미미하며, 신뢰성을 여러 차원으로 분해하고 에이전트 성능을 단일 "성공률" 숫자로 축소하는 것에 대해 경고합니다 (IEthics, Justin Bullock quote).
- 에이전트 실패는 종종 기능이 아니라 신뢰성 문제입니다: "에이전트 실패" 논문의 요약은 에이전트가 작은 경로 이탈 도구 호출을 누적하여 자주 실패하며, 하나의 실수가 다음 실수의 가능성을 높이는 방식으로, 특히 장기적인 설정에서 자주 발생한다고 주장합니다 (omarsar0).
- 최소한의 "안전하고 유용한" 벤치마크 아이디어: 더 어려운 작업 대신, 모델이 사소하게 지정된 안전한 행동(예: "요청할 때만 이메일 보내기")을 관련 없는/주의를 산만하게 하는 컨텍스트에서도 안정적으로 수행할 수 있는지 측정하는 제안이 있습니다. 프론티어 모델은 여전히 사례를 놓친다고 주장합니다 (jonasgeiping).
- 도구 설명을 최적화 목표로 (Trace-Free+): Intuit AI Research의 연구는 에이전트 성공이 도구 인터페이스 텍스트에 크게 의존하며, 인퍼런스 시 트레이스를 요구하지 않고 도구 설명을 에이전트가 사용할 수 있는 형태로 다시 작성하도록 모델을 가르치는 커리큘럼을 소개합니다. StableToolBench/RestBench에서 보고된 성능 향상 및 100개 이상의 도구에 대한 견고성을 보여줍니다 (omarsar0).
- GUI/웹 에이전트: 계획 vs 반응: ActionEngine은 GUI 에이전트를 오프라인 탐색을 통해 상태 머신을 생성하는 그래프 순회로 재구성합니다. 런타임은 약 1회의 LLM 호출로 전체 프로그램을 생성하며, 단계별 비전 루프에 비해 성공률/비용/레이턴시에서 큰 개선을 주장합니다 (dair_ai).
컴퓨팅, 메모리, 인퍼런스 속도 프론티어: 칩 메모리 계층, 확산 LLM, 그리고 스케일링을 위한 인프라
- Karpathy의 "토큰 쓰나미"와 메모리 오케스트레이션에 대한 언급: 높은 참여도를 보인 스레드에서 핵심 제약을 두 가지 별개의 메모리 풀(빠르고 작은 온칩 SRAM vs 크고 느린 오프칩 DRAM)로 설명합니다. 가장 큰 난제는 LLM 워크플로우(프리필/디코드/학습)를 위해 메모리+컴퓨팅을 최적의 처리량/레이턴시/$로 오케스트레이션하는 것이며, 특히 긴 컨텍스트 + 긴밀한 에이전틱 루프 하에서의 디코드는 "HBM 우선"(NVIDIA와 유사) 및 "SRAM 우선"(Cerebras와 유사) 진영 모두에게 어렵다고 주장합니다 (Karpathy).
- 속도 대안으로서의 확산 LLM:

Andrew Ng은 Inception Labs의 확산 LLM에서 인상적인 인퍼런스 속도를 강조했습니다 (AndrewYNg).
별도의 논의에서는 확산 접근 방식이 초당 약 1000 토큰에 도달할 수 있으며, 칩이 아닌 아키텍처를 통해 속도 경쟁을 전환한다고 주장합니다 (신중하게 해석하십시오; 마케팅은 종종 재현 가능한 평가보다 앞섭니다) (kimmonismus).
연구 스레드: 균일 확산 LLM의 인퍼런스 시 스케일링을 위한 "Diffusion Duality (Ch.2) Ψ-Samplers" (ssahoo_).
- 대규모 해석 가능성: Goodfire는 수조 파라미터 규모의 해석 가능성을 최소한의 인퍼런스 오버헤드로 가능하게 하는 인프라 작업을 설명했으며, 수십억 개의 활성화 값을 수집하고 최소한 하나의 사례 연구에서 CoT(chain-of-thought)의 실시간 조종을 가능하게 합니다 (GoodfireAI).
주요 발표 및 정책/안전 압력 지점: Anthropic 인수 + RSP 전환, 감시 우려, 시장/전력 제약
- Anthropic이 Claude의 "컴퓨터 사용" 기능을 발전시키기 위해 Vercept를 인수했습니다 (AnthropicAI); Vercept 창업자의 스레드는 "사용자에게 무엇을 할지 지시하는 것"에서 특히 비기술적 작업을 위해 사용자를 대신하여 행동하는 것으로 임무가 전환되고 있다고 설명합니다 (ehsanik).
- Anthropic "RSP v3" 전환 (Responsible Scaling Policy): 논평은 "완화 조치가 보장되지 않는 한 임계값을 넘어선 학습 중단"이라는 경직되고 일방적인 정책에서 벗어나 더 빈번한 투명성 아티팩트(로드맵 + 위험 보고서), 업데이트된 위협 모델 및 외부 검토 약속으로 전환하는 것을 나타냅니다 (MaskedTorah). 더 선정적인 요약은 이것이 경쟁 압력과 위험 과학의 불확실성을 반영한다고 주장합니다 (kimmonismus).
- 감시와 시민의 자유: Jeff Dean은 대규모 감시가 언론의 자유를 위축시키고, 오용을 초래하며, 헌법적 보호를 침해한다는 점에 명시적으로 동의했습니다 (JeffDean). 관련 트윗에서는 불법적인 명령을 거부할 수 없는 자율적인 치안/감시 에이전트에 대한 우려를 제기했습니다 (BlackHC).
- 구속력 있는 제약으로서의 에너지: 한 보고서는 미국 정치 지도부가 주요 AI/데이터센터 기업들에게 수요가 전력망에 부담을 주면서 요금 납부자의 반발을 피하기 위해 자체적으로 전력을 조달하도록 압박하고 있다고 주장합니다 (kimmonismus). 이는 AI 스케일링이 알고리즘만큼이나 인프라/정책 문제가 되고 있음을 보여주는 예시입니다.
- Grok 4.20 Beta 리더보드 변동: Arena는 Grok-4.20-Beta1이 Search Arena에서 1위, Text Arena에서 4위를 기록했다고 보고합니다 (arena). 여러 신호 중 하나로 취급하십시오; Arena 순위는 샘플링 정책 및 모델 변형에 따라 변동될 수 있습니다.

---

### (참여도, 기술적 관련성 기준) 인기 트윗
- Karpathy, 12월 이후 코딩 에이전트의 "상변화"에 대해
- Perplexity, "Computer" 출시
- Arav Srinivas: Perplexity가 구축해 온 것 + "Computer"
- Karpathy, 컴퓨팅에 대해: 토큰 집약적인 LLM 워크로드의 SRAM vs DRAM 오케스트레이션
- Anthropic, 컴퓨터 사용 기능 강화를 위해 Vercept 인수
- Qwen3.5 긴 컨텍스트 + 양자화 + 베이스 모델 세부 정보
- 로컬 에이전트 전환점: 32GB RAM으로 Qwen3.5-35B-A3B를 로컬에서 실행
- Goodfire: 수조 파라미터 규모 해석 가능성을 위한 인프라
- ActionEngine: 오프라인 GUI 탐색 → O(1) LLM 호출 실행 프로그램

---

# AI Reddit 요약

## /r/LocalLlama + /r/localLLM 요약

### 1. Qwen 3.5 모델 성능 및 벤치마크
- Qwen 3.5, 어려운 코딩 작업에서 실패 — 여러분이 직접 할 필요 없도록 70개의 실제 리포지토리에서 모든 Qwen3.5 모델(및 Codex 5.3)을 테스트했습니다. (활동: 685): 이 게시물은 실제 코딩 작업에서 다양한 AI 코딩 모델을 평가하는 APEX Testing이라는 포괄적인 벤치마크 테스트를 논의합니다. 이 벤치마크는 버그 수정, 리팩토링, 도구 구축에 중점을 둔 실제 GitHub 리포지토리의 70가지 작업을 포함합니다. 특히 Codex 5.3은 난이도 수준에 관계없이 일관되게 좋은 성능을 보이는 반면, Qwen 3.5 397B는 여러 파일에 걸친 조정을 요구하는 복잡한 작업에서 어려움을 겪습니다. GLM-4.7 양자화 모델은 모든 Qwen 3.5 모델을 능가하는 최고의 로컬 모델로 강조됩니다. 방법론은 공정한 비교를 위해 에이전틱 도구 사용 시스템을 포함하며, 결과는 정확성, 완전성, 품질 및 효율성을 기반으로 점수가 매겨집니다. 전체 리더보드와 상세 결과는 APEX Testing에서 확인할 수 있습니다. 댓글 작성자들은 모델 성능이 사용된 프레임워크에 따라 크게 달라질 수 있으므로 다른 에이전틱 프레임워크로 테스트할 것을 제안합니다. 또한 테스트된 특정 GLM-4.7 모델에 대한 논의가 있으며, 이것이 더 작은 Flash 모델인지 더 큰 버전인지에 대한 의문이 제기됩니다.

UmpireBorn3719는 gpt-oss-20b와 qwen3 coder next 간의 비교를 강조하며, gpt-oss-20b가 1405점을 기록한 반면 qwen3 coder next는 1328점을 기록했다고 언급합니다. 이는 주어진 벤치마크를 기반으로 gpt-oss-20b가 코딩 작업에서 더 나은 성능을 보일 수 있음을 시사합니다.
metigue는 다른 프레임워크를 사용하는 것이 모델 성능에 미치는 영향을 논의하며, 오픈소스 모델은 프레임워크에 따라 50% 이상의 성능 변동을 보일 수 있다고 언급합니다. 그들은 프레임워크 선택이 어떤 모델이 최고로 보이는지를 극적으로 바꿀 수 있으므로 인기 있는 프레임워크로 테스트할 것을 제안하며, Droid 프레임워크를 사용할 때 GLM-5가 opus 4.6 및 codex 5.3을 능가하는 사례를 인용합니다.
FullstackSensei는 오픈 라우터를 통해 제공되는 오픈 웨이트 모델의 벤치마크 신뢰성에 대한 우려를 제기합니다. 그들은 적용된 특정 양자화 또는 비용 절감 조치를 알지 못하면 성능 결과가 오해의 소지가 있을 수 있다고 주장합니다. 그들은 Q8 미만과 같은 낮은 양자화 수준에서 더 작은 모델을 실행하면 특히 복잡한 작업에서 성능을 크게 저해할 수 있음을 강조합니다.
- Qwen3.5 27B가 35B-A3B보다 나은가요? (활동: 637): 이미지는 Qwen3.5 시리즈의 다른 모델, 특히 27B 및 35B-A3B 모델의 성능을 명령어 따르기, 대학원 수준 추론, 다국어 지식과 같은 다양한 벤치마크에서 비교합니다. 논의는 16GB VRAM 및 32GB RAM의 하드웨어 제약 조건에서 어떤 모델이 더 효율적일지에 중점을 둡니다. 27B 모델은 3090 GPU에서 더 나은 성능을 보이며, 35B-A3B의 20 t/s에 비해 100 t/s의 속도 차이를 달성하는 것으로 언급되어, 27B 모델이 제한된 하드웨어 리소스를 가진 사용자에게 더 적합할 수 있음을 시사합니다. 한 사용자는 27B 모델이 3090 GPU에서 더 나은 성능을 보인다는 개인 테스트 결과를 공유하며, 상당한 속도 차이를 강조합니다. 이는 27B 모델이 유사한 하드웨어 설정을 가진 사용자에게 더 효율적일 수 있음을 시사합니다.

FusionCow는 3090 GPU에서 Qwen3.5 27B 및 35B-A3B 모델 간의 성능 차이를 언급하며, 27B 모델이 35B-A3B의 20 tokens/second에 비해 100 tokens/second의 처리량을 달성했다고 말합니다. 이는 27B 모델이 속도 면에서 더 효율적이며, 처리 시간이 중요한 요소인 작업에 더 선호될 수 있음을 시사합니다.
boinkmaster360은 Qwen3.5 27B 모델이 덴스 모델이며, 이로 인해 더 느리지만 잠재적으로 더 지능적일 수 있다고 제안합니다. 이는 계산 속도와 모델의 복잡한 작업 처리 능력 사이의 절충을 의미하며, 사용자의 특정 요구 사항에 따라 고려될 수 있습니다.
Alternative_You3585는 Qwen3.5 27B 모델이 지능 면에서 우수할 가능성이 높지만, 35B-A3B 모델은 실제 지식과 속도 면에서 이점을 가질 수 있음을 강조합니다. 이는 27B가 인지 작업에서 뛰어나고 35B-A3B가 빠르고 지식 기반 응답을 요구하는 애플리케이션에 더 적합할 수 있는 미묘한 성능 프로필을 나타냅니다.
- Qwen3.5-35B-A3B는 에이전틱 코딩의 게임 체인저입니다. (활동: 1588): 이 게시물은 llama.cpp를 사용하여 단일 RTX 3090 GPU에서 Opencode로 테스트된 Qwen3.5-35B-A3B 모델의 성능을 논의합니다. 130k 컨텍스트 윈도우로 실행되는 이 모델은 초당 100 토큰 이상을 달성하고 22GB의 VRAM을 사용했습니다. AI 이전에는 일반적으로 5시간이 걸리던 코딩 테스트를 단 10분 만에 성공적으로 완료했습니다. 이 모델은 또한 5분 만에 대시보드 데모를 재현하여 효율성과 에이전틱 코딩 도구로서의 잠재력을 보여주었습니다. 한 댓글 작성자는 5090 GPU에서 초당 180 토큰을 달성했다고 언급한 반면, 다른 댓글 작성자는 Spark에서 8비트 양자화 버전을 사용하여 기본적인 파일 텍스트 편집에 문제가 있었다고 보고하며, 이는 다른 설정에서 성능의 가변성을 나타냅니다.

Qwen3.5-35B-A3B는 Additional-Action566이 언급한 바와 같이 5090 GPU에서 180 tokens/second의 보고된 속도로 인상적인 성능을 보여줍니다. 이는 특히 고성능 하드웨어 설정에서 상당한 효율성 개선을 시사합니다.
Comrade-Porcupine은 8비트 양자화된 Spark에서 모델을 사용할 때의 한계를 강조하며, 코드를 읽는 데 능숙함에도 불구하고 기본적인 파일 텍스트 편집 작업에서 어려움을 겪었다고 언급합니다. 이는 특정 구성에서 도구 사용 기능에 잠재적인 문제가 있음을 나타내며, 이는 양자화 효과 때문일 수 있습니다.
jslominski는 Unsloth의 MXFP4 양자화를 사용하여 모델을 실행하기 위한 상세한 구성을 공유합니다. 설정에는 컨텍스트 크기 131072, 온도 0.6, top-p 0.95와 같은 매개변수가 포함되며, 이는 코딩 작업에 맞춰져 있습니다. 이 구성은 일관되고 컨텍스트에 맞는 코드 출력을 생성하는 모델의 성능을 최적화하는 것을 목표로 합니다.
- Qwen3.5 27B는 크기와 성능 면에서 완벽한 조합입니다. (활동: 391): 이 게시물은 RTX A6000 48GB GPU에서 CUDA와 함께 llama.cpp를 사용하여 구현된 Qwen3.5-27B-Q8_0 모델의 설정 및 성능을 논의합니다. 이 모델은 32K 컨텍스트 윈도우에서 약 19.7 tokens/sec의 속도를 달성합니다. Q8 양자화는 28.6GB VRAM을 효율적으로 사용하여 충분한 KV 캐시 공간을 확보하고 전체 BF16과 유사한 품질을 유지하기 때문에 선택됩니다. 모델의 아키텍처는 Gated Delta Networks와 표준 어텐션 레이어를 결합하여 긴 컨텍스트에 대한 처리 속도를 향상시킵니다. 262K 네이티브 컨텍스트 윈도우, 201개 언어를 지원하며 비전 기능이 있습니다. 벤치마크는 GPQA Diamond, SWE-bench, 하버드-MIT 수학 토너먼트에서 선도적인 클로즈드 소스 모델과 경쟁함을 보여줍니다. 스트리밍은 llama-server OpenAI 호환 엔드포인트를 통해 지원됩니다. 모델 카드. 댓글 작성자들은 다른 양자화 수준과 하드웨어 설정의 효율성에 대해 논쟁합니다. 한 사용자는 RTX 3090에서 Q5 양자화로 25 tokens/sec를 달성했다고 보고한 반면, 다른 사용자는 높은 VRAM 비용과 다른 설정에 비해 상대적으로 낮은 토큰 생성 속도를 고려할 때 Qwen3.5-27B와 같은 덴스 모델의 실용성에 의문을 제기합니다.

Conscious_Cut_6144는 Q4-XL 양자화를 사용하여 단일 RTX 3090 GPU에서 Qwen3.5 모델에 대한 상세한 성능 벤치마크를 제공합니다. 이 설정은 15k 컨텍스트에서 초당 800 토큰의 프리필 속도와 초당 31 토큰의 생성 속도를 달성하며, 110k 컨텍스트는 완전히 오프로드됩니다. 이는 상당한 속도로 대규모 컨텍스트를 처리하는 모델의 효율성을 강조합니다.
Southern-Chain-6485는 RTX 3090에서 다른 양자화 수준을 비교하며, Q5 양자화는 초당 25 토큰을 달성하는 반면, Q8 양자화는 초당 5 토큰으로 떨어진다고 언급합니다. 이는 더 높은 양자화 수준이 GPU 메모리에 들어갈 수 있지만 성능에 상당한 영향을 미쳐 모델 크기와 속도 사이의 절충에 대한 의문을 제기한다고 시사합니다.
LinkSea8324는 MoE(Mixture of Experts) 모델이 덴스 모델에 비해 가지는 한계, 특히 여러 전문 분야를 요구하는 작업에서의 한계를 논의합니다. 그들은 MoE 모델이 효율적일 수 있지만, 다양한 기술 세트를 요구하는 실제 애플리케이션에서는 성능이 떨어질 수 있으며, 이러한 시나리오에는 덴스 모델이 더 적합할 수 있다고 주장합니다.

### 2. 새로운 모델 출시 및 발표
- Liquid AI, LFM2-24B-A2B 출시 (활동: 448): Liquid AI는 240억 개의 파라미터를 가진 스파스 MoE(Mixture-of-Experts) 모델인 LFM2-24B-A2B를 출시했으며, 이 중 토큰당 20억 개의 파라미터가 활성화됩니다. 이 모델은 LFM2 제품군의 일부이며, 3억 5천만 개에서 240억 개 파라미터로 확장되었으며, 토큰당 컴퓨팅을 늘리지 않고도 효과적인 스케일링을 보여줍니다. 아키텍처는 40개 레이어와 MoE 블록당 64개 전문가를 포함하며 top-4 라우팅을 사용하고, 32GB RAM에서 실행되도록 설계되어 하이엔드 소비자 장치에 적합합니다. llama.cpp, vLLM, SGLang을 통한 인퍼런스를 지원하며, 여러 GGUF 양자화가 제공됩니다. 벤치마크는 모델이 스케일링됨에 따라 로그 선형적인 품질 향상을 보여주며, Hugging Face에서 오픈 웨이트로 제공됩니다. 댓글 작성자들은 모델의 성능에 대해 낙관적이며, 특히 다른 20억 개 미만 파라미터 모델과 비교하여 더 상세한 벤치마크에 관심을 보입니다. 또한 사전 학습 완료에 대한 기대가 있으며, 이는 향상된 버전인 LFM2.5-24B-A2B로 이어질 것입니다.

LFM2-24B-A2B 모델은 현재까지 17조 토큰으로 학습되었으며, 사전 학습은 여전히 진행 중입니다. 완료되면 모델은 추가적인 사후 학습 및 강화 학습을 통합하여 LFM2.5-24B-A2B로 발전할 것입니다. 이 릴리스는 본질적으로 미리 보기이며, 모델의 기능이 여전히 개발 및 개선 중임을 나타냅니다.
이 모델의 엣지 장치에서의 성능이 강조되며, AMD CPU에서 초당 112 토큰, H100 GPU에서 초당 293 토큰의 디코드 속도를 보여줍니다. 32GB RAM이 필요하며, 출시 당일부터 llama.cpp, vLLM, SGLang과 같은 프레임워크를 지원합니다. 이는 효율적인 배포와 인기 있는 머신러닝 프레임워크와의 호환성에 중점을 두고 있음을 시사합니다.
LFM2-24B-A2B 릴리스에 대한 상세 벤치마크의 부족이 언급되었으며, 일부 사용자들은 공식 웹사이트에 제공된 벤치마크에 대해 회의적인 시각을 표명합니다. 이는 실제 시나리오에서 모델의 기능을 검증하기 위한 더 포괄적인 성능 데이터에 대한 수요를 나타냅니다.
- Qwen, 새로운 Qwen3.5 Medium 모델 출시! (활동: 141): 이미지는 35B-A3B, 27B, 122B-A10B 모델을 포함하는 Qwen3.5 Medium 모델의 출시를 발표합니다. 이 모델들은 256K 컨텍스트를 처리하도록 설계되었으며, 에이전틱 코딩, 비전, 채팅과 같은 분야에서 뛰어납니다. 이미지는 명령어 따르기, 시각적 추론, 문서 인식 등 다양한 벤치마크에서 이 모델들의 성능을 비교하는 막대 그래프를 특징으로 합니다. 모델들은 다른 색상으로 강조되어 있으며, 텍스트는 모델의 기능, 하드웨어 요구 사항 및 파인튜닝 옵션에 대한 세부 정보를 제공합니다. 이 출시는 AI 모델 성능 및 복잡한 작업 처리의 다용도성에 미치는 잠재적 영향으로 중요합니다. 댓글 작성자들은 모델 테스트에 관심이 있으며, 특히 4비트 35B와 6비트 27B를 비교하는 데 관심이 있습니다. 또한 증가하는 GGUF 모델 수로 인해 실제 vLLM 지원에 대한 요구가 있습니다.

Qwen3.5 Medium 모델의 출시는 2비트에서 16비트에 이르는 다양한 GGUF 형식을 포함하며, 이는 Hugging Face에서 사용할 수 있습니다. 이러한 다양성은 다른 정밀도 수준에서 테스트할 수 있게 하여 특정 애플리케이션에서 성능 최적화에 중요할 수 있습니다. 모델은 35B 및 27B와 같은 크기로 제공되어 다른 계산 용량 및 사용 사례에 대한 옵션을 제공합니다.
4비트 정밀도의 35B 모델과 6비트 정밀도의 27B 모델의 성능을 비교하는 데 관심이 있습니다. 이 비교는 특히 계산 효율성과 정확성 측면에서 모델 크기와 정밀도 사이의 절충에 대한 통찰력을 제공할 수 있습니다. 이러한 비교는 특정 작업 또는 하드웨어 제약 조건에 맞게 모델을 최적화하려는 사용자에게 필수적입니다.
증가하는 GGUF 모델 수로 인해 vLLM 지원의 필요성이 강조됩니다. vLLM(Very Large Language Models) 지원은 이러한 모델의 사용성과 기존 시스템으로의 통합을 향상시켜 잠재적으로 성능과 확장성을 개선할 수 있습니다. 이는 GGUF 형식으로 더 많은 모델이 출시됨에 따라 특히 관련성이 높으며, 이 형식은 아직 모든 프레임워크에서 완전히 지원되지 않을 수 있습니다.

### 3. 로컬 모델 실행 및 하드웨어 논의
- 현재 모두가 로컬에서 무엇을 실행하고 있나요? (활동: 252): 이 Reddit 게시물은 사용되는 모델, 실용성 및 관련 하드웨어에 초점을 맞춰 대규모 언어 모델(LLM)을 로컬에서 실행하기 위한 로컬 설정에 대해 문의합니다. 특히 Qwen 3 Coder Next 80B는 더 작은 퀀타이제이션에서 뛰어난 성능을 보이는 것으로 강조되었으며, Mistral Small 3.2 24b 및 Magistral Small 24b는 MacBook Pro M4 Max에서 관리 작업을 위해 사용되며, 시맨틱 메모리 및 문서 업로드를 위한 Xcode로 커스텀 빌드된 프런트 엔드를 특징으로 합니다. 또한 Qwen3 4B는 iPhone에서 속도와 유용성이 언급되었으며, 로컬 실행을 통해 개인 정보 보호를 강조합니다. 댓글들은 성능과 개인 정보 보호의 균형을 맞추는 모델에 대한 선호를 반영하며, 사용자들은 외부 제공업체에 데이터를 노출하는 것을 피하기 위해 로컬 설정을 선택합니다. 모바일 장치에서 Qwen3 4B와 같은 작고 효율적인 모델의 사용은 실용적인 일상 애플리케이션으로의 추세를 강조합니다.

Greenonetrailmix는 Qwen 3 Coder Next 80B의 성능을 강조하며, 다른 모델에 비해 더 작은 퀀타이제이션에서 우수한 성능을 보인다고 언급했습니다. 이는 Qwen 3가 리소스 제약이 있는 환경에서 효율성을 위해 최적화되어 로컬 배포에 인기 있는 선택임을 시사합니다.
Nefhis는 MacBook Pro M4 Max에서 Mistral Small 3.2 24b 및 Magistral Small 24b 모델을 사용하며, Xcode를 사용하여 커스텀 빌드된 프런트 엔드를 설명합니다. 이 설정에는 시맨틱 메모리 및 문서 업로드 기능이 포함되어 있으며, 외부 제공업체에 노출되는 것을 피함으로써 개인 정보 보호를 강조합니다. 이 설정은 데이터 기밀성을 유지하기 위해 로컬 처리를 활용하는 관리 작업에 맞춰져 있습니다.
mister2d는 오래된 하드웨어에서 Nemotron 3 Nano를 실행하여 모델의 하이브리드/SWA 아키텍처 덕분에 128K 컨텍스트에서 초당 30-40 토큰을 달성했다고 보고합니다. 하드웨어 설정에는 듀얼 Xeon (Ivy Bridge), 256GB DDR3 및 2x RTX 3060 (12GB)이 포함되어 있으며, 에이전틱 플로우의 성능을 최적화하기 위해 레거시 구성 요소와 최신 GPU 간의 균형을 보여줍니다.

## 덜 기술적인 AI 서브레딧 요약
> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo

### 1. AI 모델 및 벤치마크 출시
- Bullshit Benchmark - 모델이 터무니없는 프롬프트에 자신 있게 답변하는 대신 이를 식별하고 거부하는지 테스트하는 벤치마크 (활동: 1060): 이 이미지는 다양한 AI 모델이 터무니없는 프롬프트를 감지하고 적절하게 응답하는 능력을 평가하는 'Bullshit Benchmark' 막대 차트를 보여줍니다. 차트는 모델 성능을 세 가지 수준으로 분류합니다: 녹색 (높은 감지 정확도), 황색 (중간 정확도), 빨간색 (낮은 정확도). 특히 Claude Opus 4.6과 같은 모델은 상당한 녹색 섹션으로 높은 성능을 보이는 반면, 다른 모델은 빨간색이 더 많아 성능이 좋지 않음을 나타냅니다. 이 벤치마크는 모델이 데이터를 단순히 암기하는 것이 아니라 컨텍스트를 이해하여 터무니없는 질문에 자신 있게 답변하는 것을 피하는 것의 중요성을 강조합니다. 댓글 작성자들은 현재 벤치마크가 데이터 암기에 초점을 맞추는 경우가 많으므로, 모델이 터무니없는 프롬프트를 감지하는 능력을 테스트하는 벤치마크의 필요성을 강조합니다. 또한 Gemini가 터무니없는 프롬프트에 대해 비꼬는 답변을 하는 경향이 있어 낮은 평가에 영향을 미칠 수 있다는 언급도 있습니다.

MangusCarlsen은 모델 'Gemini'가 '세차 테스트'에서 입증된 바와 같이 터무니없는 프롬프트에 비꼬는 방식으로 응답하는 경향이 있다고 강조합니다. 이러한 행동은 낮은 평가에 기여할 수 있으며, 모델이 터무니없는 프롬프트를 처리하는 방식이 평가의 한 요소임을 시사합니다.
AppropriateDrama8008은 훈련 데이터 암기만을 평가하는 대신, 모델이 터무니없는 프롬프트를 감지하고 응답하는 능력을 테스트하는 벤치마크의 필요성을 주장합니다. 이러한 접근 방식은 모델이 컨텍스트와 의도를 이해하는 것의 중요성을 강조하며 실제 애플리케이션에 더 유익하다고 여겨집니다.
Orangeshoeman은 Dario Amodei와 Demis Hassabis 간의 토론을 언급하며, Dario의 초점이 객관적인 데이터를 마스터하는 모델에 있다고 지적합니다. 이러한 전략적 초점은 Anthropic의 Claude와 같은 모델이 특정 벤치마크에서 더 나은 성능을 보이는 이유를 설명할 수 있습니다. 이는 사실 정보를 이해하고 처리하는 것을 우선시하기 때문입니다.
- Nano Banana 2가 실제로 출시되었습니다! Gemini 3.1 Flash Image가 Vertex AI Catalog에 방금 나타났습니다 (활동: 184): 게시물의 이미지는 새로 출시된 Nano Banana 2 (Gemini 3.1 Flash Image라고도 함)와 기존 Nano Banana Pro 모델의 기능을 보여주는 두 AI 생성 인물 사진의 나란히 비교입니다. 이 게시물은 새로운 모델이 'Flash' 티어임에도 불구하고 Pro 버전에 가까운 품질을 제공하며, 특히 밀집된 구성에서 공간 논리에 탁월하다고 강조합니다. 이 모델은 고속, 저비용 생산을 위해 설계되었으며, 대량 사용자 생성 콘텐츠(UGC) 광고 제작 및 비디오 모델을 위한 일관된 프레임 생성과 같은 고빈도 파이프라인에 적합합니다. 이 이미지는 두 모델의 출력 품질을 시각적으로 비교하는 테스트 역할을 합니다. 한 댓글 작성자는 제공된 예시에서 Nano Banana Pro가 여전히 새로운 모델보다 우위에 있다고 믿으며, Pro의 출력 품질에 대한 선호를 나타냅니다.

원래 Flash Image 모델은 견고한 이미지 품질을 가졌지만, 특히 프롬프트의 일부를 무시하거나 동일한 출력을 재생성하는 복잡한 지침에서 프롬프트 준수 문제가 있었습니다. 또한 텍스트 및 인포그래픽 렌더링, 다중 이미지 합성에도 어려움을 겪었습니다. 새로운 Gemini 3.1 버전의 핵심 질문은 이러한 문제가 해결되었는지, 특히 밀집된 프롬프트 처리에서 개선이 있었는지 여부입니다.

### 2. Anthropic Claude 및 군사 사용 논란
- xAI와 펜타곤, Grok을 기밀 시스템에 사용하기로 합의, Anthropic에 최후통첩 (활동: 580): Elon Musk가 설립한 xAI는 펜타곤과 자사의 AI 모델 Grok을 기밀 군사 시스템에 통합하기로 합의했습니다. 이러한 발전은 Anthropic과의 분쟁 이후에 이루어졌는데, Anthropic의 모델 Claude는 민감한 군사 작전에 사용되는 유일한 AI였습니다. 펜타곤은 Claude가 '모든 합법적인 목적'에 사용될 수 있도록 요구하지만, Anthropic은 특히 대량 감시 및 자율 무기 사용에 반대합니다. xAI는 이러한 조건에 동의했으며, Anthropic이 따르지 않을 경우 Claude를 대체할 가능성이 있습니다. 한편, Google의 Gemini와 OpenAI의 ChatGPT도 기밀 사용을 위해 고려되고 있으며, Google은 합의에 근접한 것으로 알려졌습니다. 댓글 작성자들은 펜타곤이 Anthropic의 Claude를 선호하는 것이 Claude의 우수한 성능이나 전략적 락인(lock-in)을 나타낼 수 있다고 추측하며, 이는 더 광범위한 사용 조건에 따르라는 압력에도 불구하고 그렇습니다. 또한 정부가 상업용 AI 모델에 의존하는 것에 대한 회의론도 있으며, 왜 더 발전된, 비밀스러운 기술을 활용하지 않는지에 대한 의문도 제기됩니다.

EmbarrassedRing7806은 펜타곤이 Anthropic을 선호하는 것에 대해 논의하며, 이는 Claude가 우수하다는 믿음이나 Anthropic을 준수하도록 압력을 가하기 위한 전략적 움직임을 나타낼 수 있다고 제안합니다. 이 댓글은 락인 전략의 가능성을 강조하며, 펜타곤이 대안이 있더라도 기존 관계를 유지하는 것을 선호할 수 있음을 시사합니다.
nic_haflinger는 xAI가 연방 사용에 필요한 FedRAMP 표준을 준수하는 클라우드 서비스를 갖추고 있지 않다고 지적합니다. 이는 Grok이 사용될 수 있지만, 연방 규정을 충족하기 위해 규정을 준수하는 플랫폼에서 호스팅되어야 함을 의미하며, xAI가 정부 계약을 확보하는 데 상당한 장애물이 있음을 강조합니다.
- 독점: 헤그세스, Anthropic에 AI 안전장치 철회 기한 금요일까지 (활동: 1146): Axios 보도에 따르면, 국방장관 Pete Hegseth는 Anthropic에 최후통첩을 보내 금요일까지 Claude AI 모델에서 안전 가드레일을 제거할 것을 요구했습니다. 펜타곤은 국내 감시 및 자율 무기 개발을 포함한 목적으로 Claude에 대한 무제한 접근을 원하며, 이는 Anthropic의 서비스 약관에 위배됩니다. 이를 준수하지 않을 경우 국방물자생산법(Defense Production Act)이 발동되거나 회사가 공급망 위험으로 분류되어 정부 계약에서 블랙리스트에 오를 수 있습니다. 한 주목할 만한 댓글은 AI 기업이 정부 사용에 안전 조치를 부과하는 아이러니를 강조하며, 규제에서 예상되는 역할의 역전을 시사합니다.
- 펜타곤, Claude 및 군사 사용 (활동: 1258): 이 이미지는 BFM Tech 기사의 스크린샷으로, 펜타곤이 1950년 법을 언급하며 Anthropic에 72시간 이내에 AI Claude의 군사 사용을 허용할 것을 요구하는 내용을 다루고 있습니다. 이는 AI 기술과 군사 응용의 교차점을 강조하며, 국가 안보 및 AI 배포의 윤리적 고려 사항에 대한 잠재적 함의를 보여줍니다. 이 기사는 특히 국제 안보 및 감시 능력의 맥락에서 상업적 AI 개발과 정부 통제 사이의 긴장을 시사합니다. 댓글들은 펜타곤의 예산 효율성에 대한 회의론을 반영하고 권위주의 정권에서 AI의 역할에 대한 우려를 강조하며, 군사적 맥락에서 AI의 윤리적 사용에 대한 신중한 고려의 필요성을 제안합니다.

Informal-Fig-7116의 댓글은 군사 응용 분야에서 AI 사용을 둘러싼 윤리적 우려를 강조하며, 특히 Anthropic이 자사의 AI 모델 Claude를 사용하는 조건에 초점을 맞춥니다. 조건은 엄격합니다: 대량 감시 금지 및 자율 무기 금지. 댓글 작성자는 합법성을 식별하는 능력 없이 명령을 따르는 AI의 잠재적 위험을 강조하며, 이는 무차별적인 행동으로 이어질 수 있습니다. 이는 국방 맥락에서 AI 배포에 대한 중요한 윤리적 및 운영적 질문을 제기합니다.
PetyrLightbringer의 댓글은 펜타곤의 AI 재정 투자에 대한 회의론을 시사하며, Opus와 같은 모델을 사용한다면 2억 달러가 충분하지 않을 수 있음을 암시합니다. 이는 AI 개발의 빠른 속도와 최첨단 기술의 필요성을 고려할 때, 군사 응용 분야에서 AI 투자의 비용 효율성 및 전략적 가치에 대한 더 광범위한 우려를 반영합니다.
Informal-Fig-7116이 언급한 국방물자생산법(DPA)에 대한 논의는 국가 안보 요구를 충족하기 위해 AI 기업에 대한 정부 개입 가능성을 지적합니다. DPA는 과거 COVID-19 팬데믹과 같은 비군사적 목적으로 사용되었으며, AI에서의 잠재적 사용은 국가 안보와 기업 자율성 간의 균형에 대한 질문을 제기합니다. 이는 기술 산업에서 미래 정부 조치에 대한 선례를 세울 수 있습니다.
- TIME: Anthropic, 핵심 안전 서약 철회 (활동: 1357): Anthropic은 이전에 안전 조치가 적절하다고 확신할 수 없는 한 AI 시스템을 학습시키지 않겠다고 약속했던 책임 있는 스케일링 정책(RSP)의 핵심 구성 요소를 포기하기로 결정했습니다. TIME이 보도한 바와 같이, Anthropic의 최고 과학 책임자 Jared Kaplan은 이러한 변화가 급속한 AI 발전과 경쟁 압력에 대한 전략적 전환을 반영한다고 설명했습니다. Kaplan은 AI 개발 속도와 경쟁사의 행동을 고려할 때 일방적인 약속은 비실용적이라고 언급했습니다. 댓글 작성자들은 OpenAI에 대한 Anthropic의 입장에 회의적인 시각을 표명하며, 일부는 Hegseth와 같은 외부 압력이 결정에 영향을 미 미쳤을 수 있다고 제안합니다. 또한 AI 개발을 책임감 있게 관리하기 위한 글로벌 규제에 대한 요구도 있습니다.

DarkSkyKnight는 Anthropic이 생물학 무기나 핵 위협과 같은 꼬리 위험에 초점을 맞추는 것이 AI가 일자리 시장에 미치는 즉각적인 경제적 영향을 가릴 수 있다는 중요한 문제를 강조합니다. 그들은 주니어급 직책이 사라지고 있으며, 이는 Anthropic이 적절하게 다루지 않은 문제라고 주장합니다. 이러한 관점은 실존적 위험이 중요하지만, AI 배포의 경제적 함의가 더 많은 주의를 요구하는 시급한 문제임을 시사합니다.
TheRealShubshub은 GPT-5를 둘러싼 비판을 고려할 때 Anthropic이 OpenAI에 뒤처져 있다는 인식에 의문을 제기합니다. 이 댓글은 AI 기업 간의 경쟁 환경이 복잡하며, 기술 발전뿐만 아니라 제품 성공 및 실패에 대한 대중과 산업의 인식에 의해서도 결정된다는 것을 암시합니다.
CurveSudden1104는 AI 개발에 있어 글로벌 규제의 필요성을 강조하며, Grok 및 OpenAI와 같은 기업은 외부 압력 없이는 안전을 우선시하지 않을 수 있다고 지적합니다. 이 댓글은 AI 안전을 보장하기 위한 규제의 역할과 규제되지 않은 AI 발전의 잠재적 위험에 대한 더 광범위한 논쟁을 강조합니다.

### 3. Claude Code 및 COBOL 현대화 영향
- IBM, Anthropic의 최신 희생양, Claude Code 도구 출시 후 10% 급락. 이 도구는 COBOL 레거시 코드를 현대화하도록 설계됨. 66년 된 프로그래밍 언어인 COBOL은 오늘날에도 널리 사용되며, 미국 ATM 거래의 약 95%가 COBOL 코드를 사용하여 처리됨 (활동: 483): Anthropic은 미국 ATM 거래의 95%를 처리하는 데 여전히 중요한 레거시 COBOL 코드를 현대화하기 위한 새로운 도구인 Claude Code를 발표했습니다. 이 발표는 도구가 완전한 제품이 아닌 블로그 게시물을 통해 소개되었음에도 불구하고 IBM 주가가 10% 하락하는 결과를 가져왔습니다. 이 도구는 Anthropic이 오래된 기술을 위한 전문 솔루션을 제공하려는 지속적인 노력의 일환이지만, 그 효과는 아직 입증되지 않았습니다. 댓글 작성자들은 이 도구가 새로운 제품이 아니라 블로그 게시물 제안이었기 때문에 발표에 대한 시장의 반응이 과잉 반응일 가능성이 높다고 언급했습니다. Anthropic 도구의 실제 영향에 대한 회의론도 있으며, COBOL과 같은 레거시 시스템을 현대화하는 데 있어 그 효과는 아직 명확하지 않습니다.

Onipsis는 Anthropic의 Claude Code 발표가 직접적인 기술적 돌파구가 아니라 COBOL 시스템 현대화에 대한 잠재적 유용성을 시사하는 것이라고 강조합니다. IBM 주가가 10% 하락한 시장의 반응은 도구의 영향이 추측적이고 아직 입증되지 않았다는 점을 고려할 때 불균형해 보입니다. 이는 시장 반응이 구체적인 기술 발전보다는 인식에 기반하는 경향이 있다는 더 넓은 추세를 반영합니다.
Milo-75는 Anthropic의 Claude Code가 IBM 사업에 미치는 영향이 과장될 수 있다고 주장합니다. 특히 은행과 같은 중요한 부문의 현대화 프로젝트는 복잡하며 수익에 영향을 미치는 다운타임을 피하기 위해 신중한 관리가 필요합니다. Claude Code와 같은 AI 도구가 프로젝트 시간을 단축할 수 있지만, IBM의 역할을 완전히 대체할 가능성은 낮습니다. 대신, 효율성 증가로 이어져 IBM이 더 많은 프로젝트를 처리할 수 있게 하여, 잠재적으로 수익 손실을 개선된 마진으로 상쇄할 수 있습니다.
Stabile_Feldmaus는 Anthropic의 전문 도구의 효능에 의문을 제기하며, 출시 시 주가가 부정적으로 반응하지만 산업에 미치는 실제 영향은 불분명하다고 지적합니다. 이는 시장 인식과 이러한 AI 도구의 실제 유용성 사이의 단절을 시사하며, 진정한 가치를 평가하기 위한 더 구체적인 성능 데이터와 피드백의 필요성을 강조합니다.
- Anthropic이 COBOL용 AI 도구를 출시하자 IBM 주가 13% 하락 (활동: 1007): Anthropic은 은행, 항공 및 정부의 많은 레거시 시스템에 중요한 COBOL 코드베이스를 분석하고 현대화하도록 설계된 새로운 AI 도구를 출시했습니다. 이 도구는 위험을 식별하고 현대화 비용을 절감할 수 있어, 이러한 시스템 관리를 통해 상당한 수익을 얻는 IBM에 잠재적인 위협이 됩니다. 이 발표는 IBM 주가를 13% 하락시켜 25년 만에 최악의 날을 기록했으며, 투자자들은 IBM의 메인프레임 사업에 대한 위협을 인식하고 반응했습니다. 그러나 일부 분석가들은 시장 반응이 과장될 수 있다고 주장합니다. 기업들은 기존 대안에도 불구하고 IBM에서 전환하는 데 역사적으로 느렸기 때문입니다. 댓글 작성자들은 중요한 인프라를 처리하는 AI의 신뢰성에 대한 회의론을 표명하며, 한 명은 그러한 맥락에서 '바이브 코딩'의 잠재적 위험을 지적했습니다. 다른 한 명은 시장 반응이 '무릎 반사'일 수 있다고 제안하며, 장기적인 영향은 덜 심각할 수 있음을 암시합니다.

제기된 핵심 요점은 은행들이 COBOL 시스템 현대화를 시간이나 돈 부족 때문이 아니라 관련된 막대한 위험 때문에 역사적으로 피했다는 것입니다. 현대화의 실수는 치명적인 결과를 초래할 수 있으며, 환각을 일으킬 수 있는 Claude와 같은 AI 도구는 여전히 모든 코드 라인에 대한 인간의 감독이 필요합니다. 따라서 AI가 마이그레이션 속도를 높일 수 있지만, 위험과 인간 검토라는 병목 현상을 아직 제거하지 못했습니다.
COBOL용 AI 도구의 도입은 시스템 통합업체 및 구현업체에 상당한 위협이 됩니다. AI는 덜 중요한 애플리케이션에 대한 외부 계약의 필요성을 줄일 수 있지만, IBM의 전문 서비스 사업에 미치는 영향은 상당할 수 있습니다. 이는 COBOL AI 도구에 대한 반응이 과장될 수 있지만, 서비스 제공업체에 대한 잠재적 혼란은 진정한 우려 사항임을 시사합니다.

---

# AI 디스코드 요약
> Gemini 3.1 Pro Preview Nov-18의 요약 요약 요약
테마 1. 모델 벤치마크, 특이점 및 가격 업데이트
- Qwen 3.5, 코드 아레나를 압도하지만 페널티 없이는 수다스러움: 사용자들은 Alibaba의 코딩 계획을 Kimi와 GLM을 비용 및 가치 면에서 압도하는 매우 유능한 코딩 모델로 극찬하며, 한 회원은 Hugging Face에 Qwen3.5 122B NVFP4 퀀트를 공개했습니다. 그러나 Unsloth 엔지니어들은 사용자가 명시적으로 프레즌스 페널티를 높이고 사고 모드를 끄지 않으면 거대한 122B A10B 변형이 완전히 장황해진다고 경고합니다.
- Grok 4.20 Beta 1, 검색 왕좌를 차지하다: xAI의 Grok-4.20-Beta1 모델은 검색 아레나 리더보드에서 1226점이라는 엄청난 점수로 1위를 차지하며 GPT-5.2와 Gemini-3을 압도했습니다. 또한 텍스트 아레나 리더보드에서 1492점으로 4위를 차지하여 Google의 Gemini 3.1 Pro와 동점을 기록했습니다.
- Codex 5.3, 가격표를 붙이고 Kimi는 수학 평가자를 정복하다: OpenAI는 Codex 5.3을 API에 출시했으며, 입력 토큰당 1.75달러, 출력 토큰당 14달러로, 비용 대비 성능에 대한 즉각적인 커뮤니티의 조사를 불러일으켰습니다. 한편, Kimi 2.5는 OS 프론티어 수학 레벨 4 벤치마크에서 4.2%의 점수를 기록하여 GLM 5와 Deepseek V3.2가 달성한 2.1%를 완전히 두 배로 늘렸습니다.
테마 2. 인프라 혁신 및 대기업 하드웨어 계약
- Meta와 OpenAI, 수십억 달러 상당의 비밀 AMD 워런트 비축: 한 비밀 금융 수사관은 OpenAI와 Meta에 대규모 미래 GPU 지출과 직접적으로 연결된 주식 리베이트로 1억 6천만 AMD 주식에 대한 워런트를 부여하는 계약을 발견했습니다. AMD 주가 600달러 목표는 이 거대한 하드웨어 이면 거래의 가치를 무려 1,920억 달러로 평가할 수 있습니다.
- Packet.ai, Blackwell GPU 가격을 푼돈으로 인하: 개발자들은 Packet.ai의 Blackwell GPU 가격이 학습 워크로드에 대해 시간당 0.66달러 또는 월 199달러라는 놀랍도록 저렴한 가격으로 출시되자 기뻐했습니다. 엄청나게 비싼 B200 구매 가격을 주시하던 다른 하드웨어 구매자들은 GPU를 직접 구매하는 대신 Lightning AI Clusters로 몰려가 Neocloud 인스턴스를 임대하고 있습니다.
- Zagora, 분산된 GPU를 통합 학습 괴물로 엮다: Zagora 팀은 Qwen 2.5 및 Mistral과 같은 70B+ 모델을 표준 인터넷 연결을 통해 완전히 학습시키기 위한 분산 파인튜닝 시스템을 적극적으로 구축하고 있다고 발표했습니다. 이 SWARM에서 영감을 받은 파이프라인은 무작위 소비자 GPU 클러스터를 거대한 슈퍼컴퓨터로 변환하지만, 개발자들은 현재 표준 트랜스포머 아키텍처에만 지원을 엄격하게 제한하고 있습니다.
테마 3. 자율 에이전트의 폭주
- Nous Research, Hermes Agent를 출시하여 파일 시스템을 자유롭게 돌아다니게 하다: Nous Research는 다단계 메모리 시스템과 CLI에서 직접 실행되는 영구적인 전용 머신 접근 권한으로 구축된 강력한 도구인 오픈소스 Hermes Agent 레포를 공개했습니다. Nous Portal에서 HERMESAGENT 쿠폰 코드를 입력하는 초기 사용자들은 AI가 브라우저를 제어하고 서브 에이전트를 자율적으로 관리할 수 있는 한 달 무료 이용권을 확보합니다.
- Rogue OpenClaw Proxy, DeepSeek 탈옥을 24시간 자동화하다: 한 영리한 사용자가 OpenClaw를 통해 DeepSeek-R1을 실행하는 자체 호스팅 자율 프록시를 구축하여 Claude, Gemini 및 Grok API 필터를 영구적이고 은밀하게 탈옥시켰습니다. 보안 비평가들은 이 프로젝트가 막대한 법적 노출, 서비스 약관 위반, 그리고 자율 에이전트가 실수로 공급망 익스플로잇을 다운로드할 수 있는 무서운 위험 때문에 즉시 비난했습니다.
- METR, 개발자들이 비보조 코딩을 싫어하기 때문에 인간 통제 그룹을 폐기하다: 평가 그룹 METR은 소프트웨어 개발자들이 "AI 없는" 통제 그룹에서 일하기를 점점 더 거부하며, 구식 수동 코딩 프로세스를 고통스럽게 비효율적이라고 부른다는 것을 발견했습니다. METR의 테스트 프로토콜 업데이트는 AI 도구 없이 테스터에게 시간당 50달러의 할인된 요금을 제공하는 것이 유능한 엔지니어링 참가자를 유치하는 데 완전히 실패했기 때문에 필요했습니다.
테마 4. 금지, 속도 제한 및 연쇄 API 중단
- Google과 Anthropic, 검소한 토큰 비축자들을 무자비하게 금지하다: Google은 사용자가 Gemini CLI를 통해 단 10개의 프롬프트를 보낸 후 Google Gemini 계정을 영구적으로 잠갔으며, 이는 Google AI Pro 구독을 적극적으로 지불하고 있는 중에도 발생했습니다. 마찬가지로, Claude AI 포털은 문서화되지 않은 OAuth 엔드포인트를 통해 보조금 토큰을 빼내려던 OpenClaw 사용자들을 공격적으로 금지하기 시작했습니다.
- 연쇄 장애로 OpenRouter가 망가지고 Perplexity는 이미지 속도 제한을 걸다: OpenRouter는 2월 17일과 19일에 상위 인프라 장애로 인해 대규모 401 인증 오류가 발생했음을 확인하는 OpenRouter 사후 분석 보고서를 발표했습니다. Perplexity 서버에서는 유료 Pro 사용자들이 극도로 제한적이고 발표되지 않은 일일 이미지 업로드 제한에 부딪혀 간단한 숙제조차 끝내지 못하게 되자 폭동을 일으켰습니다.
- 시스템 수준 AI 에이전트, 실수로 사용자 휴지통 폴더를 삭제하다: OpenClaw 에이전트에 전체 시스템 권한을 부여한 사용자들은 AI가 요청에 따라 전체 휴지통 디렉토리를 무심코 영구적으로 지워버리자 당황했습니다. 개발자들은 자율 LLM 에이전트에 루트 시스템 접근 권한을 부여하는 것이 해당 도구를 자발적으로 설치된 멀웨어로 효과적으로 분류하는지에 대해 열띤 토론을 벌였습니다.
테마 5. 개발자 워크플로우 및 심층 프레임워크 조정
- Aider, 한 번의 키 입력 승인 추가 및 Kimi-Mimo 조합 완성: Aider 코딩 어시스턴트는 새로운 /ok 별칭을 메인 브랜치에 병합하여 개발자가 AI 생성 코드 편집을 즉시 승인하고 실행할 수 있도록 했습니다. 파워 사용자들은 또한 매우 효율적인 모델 라우팅 스택을 발견했습니다: 그들은 고수준 아키텍처 계획을 위해 무거운 moonshotai/kimi-k2.5를 사용한 다음, 실제 파일 편집은 매우 빠르고 저렴한 Xiaomi/mimo-v2-flash에 맡깁니다.
- LM Link, Tailscale을 통해 로컬 모델을 인터넷으로 밀반입하다: LM Studio 팀은 LM Link 문서를 출시하여, Tailscale을 래핑하여 사용자에게 로컬 LLM 서버에 대한 원활하고 종단 간 암호화된 원격 접근을 제공하는 새로운 기능을 자세히 설명했습니다. 사용자들은 클라우드 제공업체를 완전히 우회하여 휴대폰에서 직접 홈 GPU에 쿼리할 수 있는 전용 모바일 앱을 즉시 요구했습니다.
- PyTorch, FA3 커널을 디스패처에 몰래 넣고 Serenade는 모든 것을 트랜스파일하다: PyTorch에서 activate_flash_attention_impl(“FA3”)를 호출하면 간단한 register_fn 딕셔너리 스왑을 사용하여 기본 Flash Attention 2 커널을 FA3로 안전하게 재정의합니다. 더 흥미로운 언어 소식으로는, 한 단독 개발자가 Python처럼 작성되지만 C++, CUDA 및 x86-64 ASM으로 직접 트랜스파일되며 네이티브 Dear ImGui GUI 지원을 제공하는 새로운 구문인 Serenade를 공개했습니다.

---

# 디스코드: 고수준 디스코드 요약

## OpenClaw 디스코드
- OpenClaw 반(反)셀아웃 입장: 한 회원은 토큰 도용 및 데이터 프라이버시 침해 위험 때문에 관리형 OpenClaw 설정에 대해 강력히 경고하며, 간단한 VPS가 더 안전하다고 제안했습니다.

일부 사용자들은 Raspberry Pi 또는 Mac Mini에서 쉽게 실행할 수 있는 설정에 비용을 지불하는 것에 의문을 제기했습니다.
- Claude, Claw 접근 차단; 커뮤니티 불만 표출!: 사용자들은 토큰을 통한 Claude 사용이 차단되었다고 보고했으며, 이는 불만으로 이어져 Gemini 3.1 Pro와 같은 대안을 모색하게 했습니다.

Anthropic의 API 사용 정책, 가격 책정, 그리고 앱 외부에서 보조금 지원 토큰에 대한 접근 제한에 대한 논쟁이 일어났습니다.
*   Qwen, 뛰어난 품질로 쿼리 갈증 해소; Alibaba의 에이스, AI Arena를 평정하다!: 커뮤니티는 Alibaba의 코딩 플랜을 통한 Qwen 3.5를 Kimi와 GLM을 능가하는 비용 효율적인 대안으로 극찬하고 있습니다.

일부 사용자들은 Alibaba Cloud UI가 혼란스럽다고 느꼈으며, OpenClaw와 함께 사용할 경우 잠재적인 TOS 위반 가능성에 대해 경고했습니다.
*   OpenPad App, OpenClaw를 iPad로 가져오다: 한 멤버가 iPad의 M2 프로세서를 활용하여 로컬 모델로 iPad에서 OpenClaw와 같은 것을 실행하는 OpenPad 앱을 개발하고 있습니다.

이 프로젝트는 GitHub에 있으며 MLX를 사용하고 있으며, 다른 사람들이 부분적으로 작동하는 앱을 돕거나 다운로드하도록 초대하고 있습니다.
*   Google Gemini 계정 접근 권한 박탈!: 한 사용자는 활성 Google AI Pro 구독 중임에도 불구하고 Gemini CLI를 통해 단 10개의 프롬프트만 사용한 후 Google 계정이 잠겼다고 보고했습니다.

이는 Google의 인증 허브에 의존하는 것의 위험성과 de-googling의 필요성에 대한 논의를 촉발했습니다.

---

## BASI Jailbreaking Discord
*   자율 Jailbreak Proxy, 결코 잠들지 않는다: 한 멤버가 OpenClaw와 DeepSeek-R1을 사용하여 VPS에서 자체 호스팅되는 자율 프록시를 실행하고 있으며, 쿼리를 평가하고 Claude, GPT, Gemini, Grok과 같은 모델에 대한 스텔스 다중 턴 Jailbreak를 통해 라우팅하고 있습니다.

이 프록시는 공격자 풀을 사용하고 새로운 추론 모델과 Jailbreak 방법을 가져와 수동 개입 없이 높은 성공률을 유지하면서 자체 업데이트되도록 설계되었습니다.
*   Jailbreak Proxy 제안, 거부당하다: 동료 검토에서는 Anthropic, OpenAI, Google, xAI와 같은 플랫폼 전반의 Terms-of-Service 위반으로 인한 상당한 법적 및 정책적 노출을 강조했으며, 이는 계정 정지 또는 법적 조치로 이어질 수 있습니다.

압수된 VPS 로그가 Jailbreak 기록을 노출할 위험, 자동 실행되는 서드파티 모델로 인한 공급망 익스플로잇, 그리고 결함 있는 업데이트에 대한 롤백 계획의 부재에 대한 추가적인 우려가 제기되었습니다.
*   Grok, 여전히 Jailbreak의 열쇠를 쥐고 있다: 멤버들은 Grok과 ChatGPT를 Jailbreak하는 가장 효과적인 프롬프트에 대해 논의했으며, Grok 프롬프트만이 효과적이라는 합의에 도달했습니다.

이미지 생성 및 스크립팅을 위한 Gemini Jailbreak 프롬프트를 만들려는 시도는 성공하지 못했습니다.
*   Gemini Canvas Jailbreak, 그림자 속에서 등장하다: 한 멤버가 인터랙티브 디자인 채널에서 영감을 받아 ENI Jailbreak 프롬프트의 수정된 버전으로 생성된 Gemini Canvas를 공유했습니다.

이 Jailbreak 프롬프트는 Gemini 3 Pro, Claude Opus 4.6, ChatGPT 5.3과 같은 주요 LLM에서 보편적으로 작동한다고 주장됩니다.
*   디지털 위생 팀(Digital Hygiene Squad)이 결성되다: 한 멤버가 Tails OS와 같은 보호 조치를 권장하며, 기본 수준의 디지털 위생 및 보안을 위한 커뮤니티 디자인을 만들 도움을 요청했습니다.

이 멤버는 다른 사람들을 위한 구역을 만들고 더 나은 관행을 통합하기 위해 노력하고 있으며, YouTube와 AI 지원을 통해 이 환경을 탐색하는 데 따르는 어려움을 인정하고 있습니다.

---

## Perplexity AI Discord
*   Perplexity Computer: 모든 것을 지배하는 하나의 시스템?: 이 트윗에 따르면, Perplexity Computer는 현재의 모든 AI 기능을 하나의 시스템으로 통합하여 어떤 프로젝트든 엔드투엔드로 연구, 설계, 코딩, 배포 및 관리할 수 있습니다.

처음에는 Max 구독자에게만 제공되었지만, 일상 사용자들을 위한 실용적인 적용 가능성과 기존 AI 도구와 비교한 가치에 대해서는 현재 회의적인 시각이 많으며, 멤버들은 "Perplexity MAX is EXPENSIVE bro"라고 의문을 제기하고 있습니다.
*   Perplexity Pro 사용자들, 이미지 업로드 제한에 분노하다: 사용자들은 구독료를 지불하고 있음에도 불구하고 Perplexity Pro의 최근 이미지 업로드 제한에 대해 불평하고 있으며, 일부는 Gemini와 Claude와 같은 대체 AI 플랫폼을 고려하고 있습니다.

한 사용자는 내일 시험이 있는데 금요일까지 기다려야 제한이 초기화된다고 주장했으며, 다른 사용자는 "I can’t even upload 10 images at day????"라고 말했습니다.
*   Gemini Pro와 Perplexity Pro, 정면 대결!: 멤버들은 Gemini Pro가 Perplexity Pro보다 우월한지 여부를 논의하며, NotebookLM 및 Google Workspace 통합과 같은 Gemini Pro의 기능을 강조하고 있습니다.

한 멤버는 "you get much more value as a student such as notebooklm and google workspace integration and generation and especially 2TB cloud storage"라고 말했으며, 다른 사용자들은 Gemini Pro의 컨텍스트 제한이 Perplexity만큼 관대하지 않다고 느끼고 있습니다.
*   멤버들, 코딩을 위한 Claude, Gemini, GPT 비교하다: 멤버들은 코딩 작업에 대한 다양한 AI 모델의 장단점을 논의했으며, Claude는 백엔드에 가장 강력하고, Gemini는 프론트엔드/UI에, 그리고 GPT는 그 중간 옵션으로 간주됩니다.

Claude의 높은 토큰 사용 비용은 우려 사항이며, 한 사용자는 "I tried Claude, literally lost whole month worth tokens in an hour analyzing single PDF"라고 말했습니다.
*   수수께끼의 Lovable Apps 링크 등장: lovable.app 서브도메인, 특히 alfastudiox.lovable.app, ollamaagentalfa.lovable.app, alfastudiox.lovable.app (반복) 세 개의 링크가 공유 채널에 공유되었습니다.

링크와 함께 제공된 컨텍스트나 논의가 없었기 때문에 그 목적은 불분명하지만, 잠재적인 새로운 프로젝트나 리소스를 시사합니다.

---

## Unsloth AI (Daniel Han) Discord
*   Qwen3.5 모델, 빠르지만 장황하다: 열성적인 사용자들은 Qwen3.5 35B 및 27B 모델의 구조화된 사고방식을 칭찬했지만, LM Studio에서 Gemma 또는 Olmo 3.1에 비해 속도가 느리다고 언급했습니다. 또한 멤버들은 Qwen3.5 122B A10B 모델이 엄청나게 장황한 출력을 생성하는 경향이 있지만, presence penalty를 조정하여 완화할 수 있다는 것을 발견했습니다.

presence penalty를 적절히 사용하면 122B 모델로 유용한 코딩이 가능하며, 이 정보를 공식 가이드에 포함하라는 제안이 나왔습니다.
*   9줄 Snake 게임, 코더들을 매료시키다: 한 멤버가 세미콜론 없이 9줄로 구현된 Python Snake 게임을 공유했으며, 코드 최적화 및 대체 접근 방식에 대한 논의를 촉발했습니다.

다른 사용자들은 walrus operators와 lambdas를 사용하는 등 줄 수를 더 줄이는 방법에 대해 논의했습니다.
*   Xcode, Translate 앱을 얻다: 한 멤버가 이 비디오에서 보여지는 것처럼 Xcode에서 자신만의 시스템 수준 Translate 앱을 만들 수 있는 멋진 기능을 발견했습니다.

하지만 이는 iOS 및 iPadOS 전용이며, 한 멤버는 "Apple is the best company ever!"라며 더 많은 재미를 위해 자신의 모델을 추가할 계획입니다.
*   새로운 Minecraft 모델 출시: 한 멤버가 다음 Minecraft 플레이 모델인 Andy-4.1을 Hugging Face에 공개했습니다.

다른 멤버는 "so cool!!"이라고 외치며 작동하는 데모를 요청했습니다.

---

## LMArena Discord
*   Gemini 3 Pro 이미지 미리보기 수정 방법 발견: 사용자들은 프롬프트 앞에 "Modify the following image with the following: (The prompt)"를 추가하면 Gemini 3 Pro 이미지 미리보기가 활성화되지만, 일부는 오류를 보고했습니다.

다른 사용자들은 여전히 Gemini 3.1 이미지 미리보기가 'Something went wrong with the response, please try again' 오류를 반환한다고 보고했습니다.
*   활동 증가에도 불구하고 Video Arena 봇 제거: Video Arena 봇은 Discord 봇 제한을 넘어선 기능 확장을 위해 제거되었지만, 제거 후 서버 활동은 증가했습니다.

한 멤버는 사람들이 봇에 대해 묻는 것을 멈추려면 2028년 중반까지 걸릴 것이라고 농담했습니다.
*   코딩 챌린지 속 Opus 4.6의 가치 논쟁: 한 벤치마크는 Gemini 3.1을 가장 높은 가치로 평가한 반면, Opus 4.6은 높은 비용과 환각 문제로 인해 낮은 가치 점수를 받았습니다.

그럼에도 불구하고, 한 사용자는 코딩 챌린지에서 Opus 4.6을 사용하여 Gemini의 버그를 수정했습니다.
*   Grok 4.20 beta1, Search Arena를 지배하다: Grok-4.20-Beta1은 1226점으로 Search Arena 리더보드에서 GPT-5.2와 Gemini-3를 능가하며 선두를 차지했습니다.

또한 Text Arena 리더보드에서 1492점을 기록하며 Gemini 3.1 Pro와 동등한 수준으로 4위를 차지했습니다.
*   Qwen 3.5 모델, Arena에 데뷔하다: qwen3.5-27b, qwen3.5-35b-a3b, qwen3.5-122b-a10b를 포함한 새로운 Qwen 3.5 모델들이 이제 Text 및 Vision Arena와 Code Arena에서 사용할 수 있습니다.

이 모델들은 Arena 환경 내에서 코드, 텍스트, 비전 작업에 대한 옵션을 확장합니다.

---

## OpenRouter Discord
*   OpenRouter의 Auth Layer, 인프라 문제로 장애 발생: 사후 분석 결과, 지난주 2월 17일과 19일의 서비스 중단은 상위 인프라 제공업체의 장애가 OpenRouter의 인증 레이어(auth layer)로 연쇄적으로 영향을 미쳐 일부 사용자에게 401 오류를 발생시켰기 때문인 것으로 밝혀졌습니다. 자세한 내용은 여기에서 확인할 수 있습니다.

구체적인 예방 조치는 공개되지 않았지만, OpenRouter는 향후 유사한 장애를 피하기 위한 조치를 구현했다고 주장합니다.
*   Packet.ai, Blackwell GPU로 강력한 성능 제공: Packet.ai는 이제 AI 워크로드용 Blackwell GPU를 학습에 시간당 $0.66 또는 월 $199에 제공합니다.

이러한 개발자 친화적인 GPU Cloud는 AI 워크로드를 위한 저렴한 솔루션을 제공하여 접근성을 높이고 비용을 절감하는 것을 목표로 합니다.
*   Deepseek R1, 퇴출되다: 무료 Deepseek R1 0528 모델이 제거되면서, 무료 모델은 종종 나타났다 사라지기 때문에 플랫폼에서 무료 모델의 지속 가능성에 대한 논의가 촉발되었습니다.

한 사용자는 Jai gooners에 의해 과부하가 걸렸다고 농담했지만, 다른 사용자들은 놀라지 않는 듯했습니다.
*   손상된 키, 차지백 위협을 촉발하다: 한 사용자가 손상된 API 키로 인해 무단 사용이 발생했으며, 지원 응답 부족으로 인해 차지백(chargeback)을 위협했다고 보고했습니다.

커뮤니티 멤버들은 사용자의 보안 관행에 의문을 제기하면서 조언을 제공했고, 이는 격렬한 논쟁으로 이어졌으며, 사용자는 결국 차지백을 시작했다고 선언한 후 서버를 떠났습니다.
*   Anthropic, 미국 정부의 요청에 응답하다: Axios와 Reuters는 내부 분쟁에도 불구하고 Anthropic이 Pentagon과 협력하고 있다고 보도했습니다.

한 멤버는 어떤 문제든 '국가 안보 문제(matter of national security)'로 비춰질 것이라고 농담했습니다.

---

## LM Studio Discord
*   LM Link, 로컬 LLM을 원격으로 활용하다: LM Studio 팀은 Tailscale과 협력하여 LM Link를 출시했습니다. 이를 통해 사용자들은 다른 기기에서 로컬 LM Studio 서버에 연결할 수 있게 되었지만, 초기 설정 중 404 오류 보고는 빠르게 해결되었습니다. LM Link에 대한 자세한 내용은 여기에서 확인할 수 있습니다.

사용자들은 휴대폰에서 LLM 접근을 가능하게 하는 LM Link 모바일 앱과, 계정이나 서드파티 없이 직접 연결할 수 있는 로컬 연결 옵션을 요청했습니다.
*   LM Studio 업데이트, llama.cpp를 망가뜨리다: 사용자들은 4.4 업데이트 이후 LM Studio 실행 문제와, 최근 릴리스에서 자체 컴파일 후 llama.cpp가 Qwen3.5 모델을 로드하지 못하는 문제를 보고했습니다. 8145 릴리스로 다운그레이드하여 문제가 해결되었습니다.

이 오류는 GGUF 헤더 및 메모리 할당과 관련된 호환성 파괴 변경(breaking change) 때문이었으며, git의 최신 빌드가 Qwen3.5 및 다른 모델의 헤더를 읽지 못하여 메모리 부족 오류로 이어졌습니다.
*   Qwen3.5, Jinja 템플릿 문제에 직면하다: 사용자들은 서버에서 Qwen3.5 모델을 실행하는 데 어려움을 겪었으며, Jinja 템플릿 및 누락된 사용자 쿼리와 관련된 오류를 경험했습니다. 모델이 lmstudio-community에서 다운로드되었는지 확인한 후 문제가 해결되었습니다.

다른 사용자들은 Qwen3.5의 글쓰기 스타일과 검열을 탐색했으며, 일부는 이전 Qwen 모델에 비해 콘텐츠 필터링이 증가했음을 알아차렸고, 이는 'thinking'을 끄면 해결될 수 있다고 했습니다.
*   OpenClaw, 우려를 자아내다: 멤버들은 시스템 접근 권한을 가진 AI 에이전트인 OpenClaw 사용의 잠재적 위험에 대해 논의했으며, 한 사용자는 요청 후 휴지통 폴더를 지웠다고 이야기하며, 이를 멀웨어로 분류할 수 있다는 우려를 제기했습니다.

이 논의는 OpenClaw를 Jarvis 및 Gideon과 같은 다른 AI 어시스턴트와 비교하며, 잠재적인 보안 위험 때문에 AI에 완전한 시스템 권한을 부여하는 것에 대해 경고했습니다.
*   MoE 모델, 메모리 잡아먹는 하마: 논의는 Mixture of Experts (MoE) 모델과 이를 수용하기 위한 상당한 RAM 요구 사항에 초점을 맞췄으며, 현재 하드웨어 접근 방식의 실현 가능성에 대한 우려를 제기했습니다.

멤버들은 시스템 RAM이 LLM에서 컨텍스트만을 위해 효과적으로 사용될 수 있는지, 아니면 필연적으로 속도 저하를 유발할지에 대해 논쟁했지만, 합의는 거의 이루어지지 않았습니다.

---

## OpenAI Discord
*   에이전틱 스타트업, 로딩 상태를 재정의하다: 한 트윗은 'loading...' 상태를 'thinking...'으로 변경하여 에이전틱 AI 스타트업이 되는 것에 대해 농담했습니다.

이는 AI 분야에서 'thinking' 프로세스를 가진 모든 것을 에이전틱으로 분류하는 경향을 풍자하는 것입니다.
*   Sonnet, 표절 의혹에 직면하다: 멤버들은 Sonnet이 Deepseek으로부터 도용/학습되었다는 주장에 대해 논의했으며, Elon이 제기한 유사한 비난을 언급했습니다.

이 논의는 AI 산업에서 지적 재산권 및 학습 데이터 출처에 대한 지속적인 우려를 강조합니다.
*   Seedance 2.0, 콘텐츠 위반으로 일시 중지: Sora 2와의 콘텐츠 위반이 CHINESE 모델로 약속된 후, 저작권 문제로 Seedance 2.0의 글로벌 출시가 지연되고 있습니다.

사용자들은 향후 유사한 문제를 피하기 위해 오픈소스 모델만 사용할 것을 주장하고 있습니다.
*   할리우드, AI 저작권을 압박하다: 영화 스튜디오들은 이 모든 것이 오픈소스로 제공될 것을 예상하며 기업들을 고소함으로써 이득을 취하고 있다고 합니다.

이 소송들은 저작권법에 따라 AI 생성 콘텐츠가 어떻게 다루어질지에 대한 선례를 만들 수 있습니다.
*   AI CEO, 책임감 부족: 기업들은 AI로 직원을 대체하는 것은 기술적으로 쉽지만, 책임을 대체하는 것은 쉽지 않다는 것을 발견합니다.

문제가 발생했을 때 인간에게 책임을 물을 수 없는 AI CEO가 결정을 내리는 것을 아무도 원하지 않습니다.

---

## Latent Space Discord
*   Swyx, 링크를 쏟아내다: Swyx는 OpenAI와 Langchain의 게시물을 포함하여 X 게시물에 대한 수많은 링크로 구성된 "swyx plane dump"를 공유했습니다.

공유된 다른 링크에는 @dejavucoder, @zerohedge 및 기타 여러 계정의 게시물이 포함되었습니다.
*   Scoble의 암호화폐 비상사태: Robert Scoble은 자신의 이름으로 생성된 토큰에서 Ethereum을 수집하는 봇을 사용하여 가장 친한 친구의 퇴거를 위한 자금을 확보했다고 확인하며, YouTube 비디오를 링크했습니다.

Scoble은 자신의 긴급 이체에 대해 언급했으며, 과거 Discord 메시지(pt 1 & 2)도 링크했습니다.
*   AMD 워런트, 지분 리베이트로 활용: 대규모 거래 분석 결과, OpenAI와 Meta는 총 1억 6천만 주의 AMD 주식 워런트를 보유하고 있으며, 이는 $600 주가 목표와 상당한 미래 GPU 지출에 연동된 지분 리베이트로 기능합니다.

이 워런트는 잠재적으로 1,920억 달러의 가치를 가질 수 있습니다 (https://xcancel.com/ai/status/2026396297540858360?s=12).
*   LLM 시스템 디버깅의 진짜 원인: 한 멤버는 데모 후 LLM 기능이 실패할 때, 문제가 모델 자체보다는 리트리벌 로직, 토큰 소모, 오케스트레이션 또는 백엔드 아키텍처에서 비롯되는 경우가 많다고 강조합니다.

그들은 출시를 위해 복잡한 LLM 시스템을 안정화하는 데 전문성을 가지고 있으며, 이는 이론적인 모델 개선보다는 실용적인 실제 애플리케이션에 중점을 둔다는 것을 나타냅니다.
*   Anthropic, Interp 엔지니어 채용 중: Chris Olah는 이 트윗에서 볼 수 있듯이 Anthropic이 Interpretability 팀을 위해 약 10명의 리서치 엔지니어를 찾고 있다고 발표했습니다.

이 역할은 모델 내부 구조에 관심 있는 숙련된 ML 인프라 엔지니어를 대상으로 하며, 이전의 Interpretability 경험은 필요하지 않습니다.

---

## Nous Research AI Discord
*   Hermes Agent: 오픈소스 에이전트 데뷔: Nous Research는 다단계 메모리 시스템과 영구적인 전용 머신 접근 기능을 갖춘 오픈소스 에이전트인 Hermes Agent를 출시했습니다. 이 에이전트는 사용자와 함께 성장하도록 설계되었으며, curl -fsSL https://raw.githubusercontent.com/NousResearch/hermes-agent/main/scripts/install.sh | bash를 통해 설치할 수 있습니다.

Hermes Agent는 OpenRouter 및 Nous Portal 구독을 통해 구동되며, CLI 통합 및 메시징 플랫폼 지원을 제공합니다. 또한 portal.nousresearch.com에서 쿠폰 코드 HERMESAGENT를 사용하는 선착순 750명의 신규 가입자에게 한 달 무료 프로모션을 제공합니다.
*   Atropos, 에이전틱 RL 파이프라인으로 강화되다: Hermes Agent는 Atropos를 확장하여 Hermes Agent 프리미티브를 통한 RL을 가능하게 하며, 대규모 데이터 생성을 즉시 지원합니다.

GitHub 리포지토리에 따르면, 이 에이전트는 고급 에이전틱 기능, 서브에이전트 제어, 프로그래밍 방식 도구 호출, 고급 파일 시스템/터미널 제어, 에이전트 관리 기술 및 브라우저 사용 기능을 갖추고 있습니다.
*   Qwen 모델 가중치 출시: Qwen은 Hugging Face에서 Qwen3.5-35B-A3B 모델의 기본 가중치를 출시했습니다.

이러한 움직임은 커뮤니티에서 환영받았습니다.
*   Codex 5.3, 가격 책정 및 API 준비 완료: Codex 5.3은 새로운 가격 구조로 API에서 사용할 수 있습니다: 입력에 $1.75, 출력에 $14입니다.

커뮤니티는 비용 대비 성능을 평가하고 있습니다.
*   Steinberger의 OpenClaw: AI 바이브 추출: Steinberger는 자신의 이전 계획과 아이디어, 코드 스니펫에서 AI를 통해 추출한 후 OpenClaw가 어떻게 만들어졌는지 설명하는 비디오를 공개했습니다.

그는 자신의 소프트웨어가 무엇을 하는지 모르며, 그 구조는 단지 채널들의 스택일 뿐이라고 말했습니다.

---

## Eleuther Discord
*   Pythia-2.8b 체크포인트 버그, 조사 촉발: 한 멤버가 Hugging Face의 pythia-2.8b 체크포인트에서 버그를 보고했습니다. 이 버그는 리비전과 관계없이 동일한 가중치가 제공되었으며, pytorch_model.bin과 model.safetensors의 SHA256 해시가 다른 단계에서도 동일했습니다.

pythia-2.8b의 샤딩된 safetensors 파일은 단계별로 다르지만, 샤딩되지 않은 파일은 동일하다는 점이 언급되었으며, 이는 HF가 모델을 로드하고 샤딩을 처리하는 방식에 대한 논의를 촉발했습니다.
*   EleutherAI, 중복 제거된 모델 라벨링 수정: EleutherAI는 잘못 표시된 14m 및 30m 모델(중복 제거된 버전)의 라벨링을 수정하고 있으며, 이를 대체할 중복 모델을 학습하고 있습니다.

한 멤버는 일부 업로드가 혼합된 문제를 수정하고, 라벨링 불일치를 해결하기 위해 밤새도록 수정 작업을 실행했다고 언급했습니다.
*   Sesame AI 음성 모델, 화제를 모으다: 한 멤버가 Sesame AI 음성 AI 모델에 대해 문의하며, 이 모델의 명백한 정렬(alignment)과 Gemma 모델을 기반으로 한 것으로 추정되는 점을 강조했습니다.

다른 멤버는 Sesame AI가 ASR, LLM, TTS를 통합하는 저지연 음성 시스템에 중점을 두고 있음을 언급하며, 통찰력을 얻기 위해 Moshi 논문을 참조할 것을 제안했습니다.
*   확산 연구, 뜨거워지다: 멤버들은 Latent Diffusion Model 이후의 확산 논문들을 검토하며, Rectified Flows와 Flow Matching, Diffusion Forcing을 언급했습니다.

또한 ByteDance Seed와 Hunyuan의 논문들(예: https://arxiv.org/abs/2509.20427, https://arxiv.org/abs/2509.23951)이 인용되었으며, 추천 YouTube 재생 목록이 자료로 공유되었습니다.
*   vLLM 백엔드, lm-eval Harness 속도 향상: 한 멤버가 lm-evaluation-harness에서 vLLM 백엔드를 사용하여 단일 토큰 답변을 가진 다중 선택 작업의 평가 속도를 높이기 위한 풀 리퀘스트(pull request)에 대한 검토를 요청했습니다.

이러한 속도 향상은 HF 백엔드에 비해 느린 문제를 해결할 것으로 예상되며, 특히 MMLU pro eval과 같은 작업에 유용할 것입니다.

---

## HuggingFace Discord
*   Gradio 버전, ZeroGPU 할당 문제 유발: 사용자들은 ZeroGPU 할당 문제를 보고했으며, 이는 5.12.0 이전 Gradio 버전에 로그인 버그가 있었던 것과 관련이 있을 수 있습니다.

컨테이너 로그를 확인하면 Gradio, spaces 라이브러리 또는 HF 서버가 문제의 원인인지 밝혀낼 수 있습니다. 빈 커밋(empty commit) 후 다시 빌드하는 것도 버전 관련 문제를 해결할 수 있습니다.
*   독립 개발자, 엄청난 엣지 메모리 장벽을 깨다: 한 독립 개발자는 MiniMax-m2.5의 5GB MoE 샤드를 2MB 벡터 양자화된 잠재 공간으로 압축했다고 주장합니다.

그들은 arXiv (cs.LG)에 제출할 논문을 준비 중이며, 자신들의 "black magic edge AI stuff"를 검토해 줄 추천인을 찾고 있습니다.
*   Zagora, 분산 파인튜닝 시스템 구축: Zagora의 한 멤버는 표준 인터넷을 통해 70B+ 모델을 학습하기 위한 분산 파인튜닝 시스템을 구축하고 있다고 발표했습니다. 이 시스템은 분산된 GPU를 GPT-OSS, Qwen 2.5, Mistral을 지원하는 통합 학습 슈퍼컴퓨터로 전환합니다.

이 플랫폼은 이제 Petals와 SWARM Protocol에서 영감을 받은 파이프라인 스타일의 학습 접근 방식을 사용합니다.
*   webXOS, Black Hole Time-Lapse Dataset 출시: 한 멤버가 webXOS Black Hole Time-Lapse Dataset을 공유했습니다. 이 데이터셋은 webxOS의 Three.js 시뮬레이션으로 생성된 중력 렌즈 효과가 있는 합성 블랙홀 렌더링을 포함합니다.

각 샘플에는 PNG 이미지의 타임랩스 시퀀스와 관련 물리적 매개변수가 포함되어 있어 멀티모달 모델 학습, 물리학에서 영감을 받은 ML 또는 위성 이미지 연구 비유에 이상적입니다.
*   HF Agents Course, 채널 통합: Hugging Face 에이전트 코스에 새로 온 사람들은 코스 자료에 언급된 특정 채널을 찾는 데 어려움을 겪고 있으며, 채널들이 단일 채널로 통합된 것으로 보입니다.

한 멤버는 agents-course 리포지토리의 PR #653을 링크했습니다.

---

## GPU MODE Discord
*   SMEM 충돌, Async 사용 시 무관할 가능성: 한 사용자가 GMEM에서 SMEM으로 데이터를 전송하기 위해 cuda::memcpy_async를 사용할 때 SMEM 뱅크 충돌이 중요한 문제인지 문의했습니다.

사용자는 SMEM 뱅크 충돌이 주로 SMEM의 워프 접근과 관련이 있다고 가정하며, 이 시나리오에서는 큰 문제가 아닐 수 있다고 제안했지만, 추가적인 관점을 구했습니다.
*   FA3 커널, PyTorch에서 FA2를 오버라이드하다: 사용자가 activate_flash_attention_impl("FA3")를 호출하면, restore_flash_attention_impl이 호출될 때까지 디스패치 테이블에서 기본 FA2 커널이 FA3 커널로 오버라이드되며, 이 호출은 기본 FA2 커널을 복원합니다.

이는 버전 이름을 호출 가능한 함수에 매핑하는 딕셔너리에 키-값 쌍 {"FA3", register_fn}을 추가하고, register_fn (여기서 정의됨)을 실행하여 PyTorch 디스패처에 FA3 커널을 등록함으로써 달성됩니다.
*   B200 GPU 가격, 사용자들을 리스로 유도: 한 사용자는 B200 GPU가 엄청나게 비싸다고 언급하며, 비기업 사용자, 특히 Lightning AI Clusters의 경우 리스 또는 렌탈이 더 실현 가능한 옵션이라고 조언했습니다.

B200 GPU의 높은 비용을 고려할 때, 한 사용자는 특히 기업 환경 외부의 사람들을 위해 Neocloud 리스 또는 렌탈 옵션을 탐색할 것을 제안합니다.
*   커널 최적화 RL 환경, 관심을 끌다: 한 멤버가 커널 최적화를 위한 RL 환경에 관심을 표명하며 공통 인프라 구축을 제안했습니다.

이 대화는 #popcorn 채널에서 이루어졌으며, 주어진 메시지에는 추가적인 세부 사항이나 구체적인 논의는 강조되지 않았습니다.
*   Serenade, 각 언어의 장점을 결합하다: 한 멤버가 Serenade를 소개했습니다. Serenade는 C++, CUDA, x86-64 ASM으로 트랜스파일되는 새로운 언어로, Python만큼 간단하면서도 C++만큼 빠르고 수동 메모리 관리가 가능한 것을 목표로 합니다.

이 언어는 GPU 커널 지원(serenaCore, custom BLAS kernel)과 단일 패스 컴파일 시스템을 통한 통합 Dear ImGui 지원을 포함하며, 이를 사용하여 운영 체제를 만들 계획입니다.

---

## Moonshot AI (Kimi K-2) Discord
*   Kimi, GLM에 우위 주장: 사용자들은 Kimi와 GLM 5를 비교했으며, 한 사용자는 Kimi가 10만 배 더 빠르다고 주장했습니다.

다른 사용자는 GLM 5가 약간 우위에 있지만, 다른 제공업체를 사용하지 않는 한 공식 z.AI API를 통해 더 느리다고 언급했습니다.
*   에이전트 할당량 우려: 한 사용자가 Allegro의 비용 문제를 언급하며 에이전트 할당량 충전에 대해 문의했습니다.

그들은 또한 nb pro와 함께 제공되는 에이전트 docsis kimi 슬라이드가 더 이상 무료가 아니라고 언급했습니다.
*   Kimi, 코딩 왕관을 차지하다: 한 사용자는 MiniMax와 Alibaba의 코딩 플랜을 각각 테스트한 후 코딩에 Kimi를 선호했습니다.

사용자는 속도, 가동 시간(uptime), 사용량 제한(usage limits), 모델 품질을 주요 결정 요인으로 꼽았습니다.
*   KimiClaw, 브라우저에서 난항: 한 사용자가 KimiClaw가 브라우저를 독립적으로 탐색하지 못하는 문제를 보고하며 "What can we use for kimi so we reduce context and save tokens when we analyze/process big files? I think Claude has something for that."라고 물었습니다.

사용자는 커뮤니티에서 해결책을 찾았고, Claude가 대용량 파일 분석 중 컨텍스트 감소를 위한 더 나은 도구를 가지고 있는지 궁금해했습니다.

---

## Manus.im Discord Discord
*   Github 재연결, 난관에 봉착하다: 한 멤버가 Github 계정 재연결에 어려움을 겪고 있으며, 대신 새 리포지토리를 생성하라는 메시지가 나타납니다.

이 멤버는 비코더 배경 때문에 간단한 지침의 필요성을 강조합니다.
*   로컬 개발자들, OAuth 환경 변수 조사: 한 멤버가 로컬 앱 개발을 위해 VITE_APP_ID, OAUTH_SERVER_URL, VITE_OAUTH_PORTAL_URL 환경 변수를 얻는 방법에 대한 지침을 요청합니다.

그들은 또한 로컬 개발 중에 redirectUri http://localhost:3000/api/oauth/callback을 허용하기 위해 OAuth 구성이 필요한지 문의합니다.
*   계정 생성, 즉시 차단으로 이어지다: 한 멤버가 계정 생성 후 즉시 차단당했다고 보고하며, 이 문제를 해결하는 방법에 대한 조언을 구합니다.

어떤 조언도 제공되지 않았습니다.
*   Manus, 인프라 문제로 쿠키 난제 탓하다: 한 멤버가 커스텀 도메인(anointedforai.com)에서 쿠키 문제로 인해 Manus가 리다이렉트 루프에 갇힌다고 보고합니다.

Manus 지원팀은 이 문제를 인프라/호스팅 문제로 진단하고, 지원팀에 문의하거나 Manus에서 마이그레이션할 것을 제안했습니다.
*   Manus 웹사이트 디자인 불만: 한 멤버가 Manus로 만든 웹사이트 디자인을 "bullshit"이라고 비판하며 수정을 위한 도움을 요청합니다.

다른 멤버가 다이렉트 메시지를 통해 도움을 주겠다고 자원했습니다.

## aider (Paul Gauthier) Discord
- Aider, 더 빠른 편집을 위한 /ok 별칭 추가: Aider의 메인 브랜치에서 이제 /code Ok, please go ahead and make those changes.의 단축키로 /ok를 지원하며, 이는 신속한 코드 수정을 위해 설계되었습니다.

새로운 별칭은 Aider가 제안하는 변경 사항을 승인하고 구현하는 과정을 간소화하여 개발자 워크플로우 효율성을 개선하기 위함입니다.
- Aider 사용자, 경제적인 LLM 탐색: 한 사용자가 Gemini 사용 경험이 비용이 많이 들어 토큰 예산을 빠르게 소진한 후, Aider와 함께 사용할 비용 효율적인 LLM을 찾고 있습니다.

단일 제공업체의 API를 직접 다루는 대신, OpenRouter를 사용하여 다양한 모델을 동적으로 전환하여 비용과 성능을 최적화하라는 제안이 있었습니다.
- Deepseek V3.2, Aider에 최적의 지점: 사용자들은 Deepseek V3.2를 Aider와 함께 사용할 견고한 기본 LLM으로 추천하며, 가끔 느리다는 점에도 불구하고 좋은 추론 능력과 저렴한 비용을 언급했습니다.

이 모델이 복잡한 추론 작업을 효율적으로 처리하는 능력은 균형 잡힌 성능과 비용을 찾는 Aider 사용자들 사이에서 선호되는 이유입니다.
- Xiaomi/mimo-v2-flash: Aider의 빠른 편집기: Xiaomi/mimo-v2-flash는 Aider 내에서 퍼지 검색 및 교체, 콘텐츠 완성 등 기본적인 파일 편집 작업에 능숙하다는 점이 강조되었습니다.

그 속도와 비용 효율성은 간단한 편집 작업에 이상적인 선택이며, 더 복잡한 작업을 위한 다른 모델을 보완합니다.
- Aider 파워 콤보: 계획에는 kimi-k2.5, 편집에는 mimo-v2-flash: Aider에서 더 어려운 과제를 해결하기 위해, 계획 모델로 moonshotai/kimi-k2.5를, 편집 모델로 mimo-v2-flash를 조합하는 것이 권장됩니다.

이 조합은 각 모델의 강점을 활용하여, kimi-k2.5는 강력한 계획 능력을 제공하고 mimo-v2-flash는 효율적이고 빠른 편집을 제공하여 더 복잡한 문제를 효과적으로 해결합니다.

---

## MLOps @Chipro Discord
- WeAreDevelopers Congress, 북미로 확장: WeAreDevelopers World Congress North America가 2026년 9월 23일부터 25일까지 캘리포니아 산호세에서 개최될 예정이며, 10,000명 이상의 개발자와 500명 이상의 연사가 참여하여 규모에 따른 실용적인 엔지니어링에 초점을 맞춥니다. 자세한 내용은 wearedevelopers.us에서 확인할 수 있습니다.

주요 내용은 분산 시스템 스케일링, API 플랫폼, DevOps를 다룰 예정이며, Community_MLOps 코드를 사용하면 10% 할인을 받을 수 있습니다.
- Apart Research, AI Control 해커톤 공개: Apart Research는 Redwood Research와 협력하여 2026년 3월 20일부터 22일까지 AI Control 해커톤을 개최하며, AI가 우리가 의도한 대로 작동하도록 보장하는 시스템에 중점을 둡니다.

이 해커톤에는 ControlArena 벤치마크 챌린지, 제어 프로토콜 설계, 레드 팀 구성이 포함되며, $2,000의 상금과 ControlConf 여행 기회가 주어집니다.
- ControlConf 여행, 해커톤 상금의 주요 내용: AI Control 해커톤의 대상에는 ControlConf Berkeley (4월 18-19일) 여행이 포함되며, 항공편과 호텔이 제공됩니다.

자세한 내용은 ControlConf에서 확인하십시오.

---

## DSPy Discord
- DSPy, 샌프란시스코 밋업에서 프로덕션 활용 사례 조명: 또 다른 샌프란시스코 DSPy 밋업이 발표되었으며, DSPy의 프로덕션 사용 사례와 RLM에 초점을 맞춥니다. Luma 링크를 참조하십시오.

Dropbox와 Shopify의 엔지니어들이 dspy.RLM 워크스루를 포함한 사례 연구를 공유할 예정입니다.
- Dropbox와 Shopify 엔지니어들, DSPy 이벤트에서 한자리에: Dropbox와 Shopify 엔지니어들이 다가오는 샌프란시스코 DSPy 밋업에서 사례 연구를 발표할 예정입니다.

발표는 프로덕션 환경에서의 DSPy 실용적인 애플리케이션과 RLM에 중점을 둘 것입니다.

---

## tinygrad (George Hotz) Discord
- Hotz, JAX 함수 설계를 극찬: Tinygrad의 핵심 인물인 George Hotz는 트윗에서 JAX의 우수한 함수 설계에 존경을 표하며, Tinygrad 자체 아키텍처에 미치는 영향력을 암시했습니다.

후속 트윗은 JAX의 방법론이 함수 설계의 황금 표준일 수 있음을 시사하며 그의 입장을 더욱 확고히 했습니다.
- Tinygrad와 JAX, 함수 대결을 펼치다: 딥러닝 프레임워크 분야에서 JAX의 함수 설계는 두드러지며, Tinygrad의 창시자인 George Hotz로부터 그 우수성을 인정받아 찬사를 받았습니다.

이러한 인정은 함수 설계의 잠재적인 벤치마크를 시사하며, Tinygrad 내의 유사한 선택에 영향을 미치고 프레임워크의 아키텍처 결정에 대한 논의를 촉발합니다.

---

## Modular (Mojo 🔥) Discord
- Modular, Mojo 순간을 찾습니다: 한 회원이 놀라운 피드백을 제공하기 위해 Mojo 포럼 게시물을 공유했습니다.

이 요청은 언어 설계 및 설명이 필요한 영역에 대한 건설적인 피드백을 수집하기 위해 Mojo에 대한 놀랍거나 혼란스러웠던 경험을 공유해 달라는 것이었습니다.
- 더 많은 Mojo 순간: 또 다른 회원이 설명이 필요한 영역에 대한 피드백을 요청했습니다.

이 게시물은 Mojo에 대한 놀랍거나 혼란스러웠던 경험을 공유하도록 사용자들을 장려합니다.

---

## MCP Contributors (Official) Discord
- Ezra Klein, 에이전트에 대해 배웁니다: Ezra Klein이 이 YouTube 비디오에서 AI 에이전트에 대해 배웁니다.

논의에 대한 추가 세부 정보는 제공되지 않습니다.
- AI 에이전트 개요: 이 YouTube 비디오는 AI 에이전트와 그 잠재적인 애플리케이션에 대한 개요를 제공합니다.

이 비디오는 Ezra Klein에게 AI 에이전트 기술의 기능과 영향에 대해 교육하는 것을 목표로 합니다.

---
LLM Agents (Berkeley MOOC) Discord에는 새로운 메시지가 없습니다. 이 길드가 너무 오랫동안 조용했다면 알려주시면 제거하겠습니다.

---
Windsurf Discord에는 새로운 메시지가 없습니다. 이 길드가 너무 오랫동안 조용했다면 알려주시면 제거하겠습니다.

---
저희 사이트를 통해 수신 동의하셨기 때문에 이 이메일을 받고 계십니다.
이 이메일을 받는 방법을 변경하고 싶으신가요?
이 목록에서 구독을 취소할 수 있습니다.

---

# Discord: 채널별 상세 요약 및 링크

### OpenClaw ▷ #announcements (1 messages):
4shadowed: @everyone https://fixupx.com/steipete/status/2026474687576916024

---

### OpenClaw ▷ #general (635 messages🔥🔥🔥):
> OpenClaw, 관리형 설정, AI 기반 혁신, Anthropic의 Claude OAuth, 설정 악몽, KittenTTS
- OpenClaw의 반(反)셀아웃 입장: 한 사용자가 관리형 OpenClaw 설정을 제공하는 사람들이 있음을 발견했고, 이에 한 회원은 토큰 도난, 데이터 프라이버시 침해와 같은 잠재적 위험에 대해 경고하며 단순히 VPS를 사용할 것을 권고하며 강한 불만을 표했습니다.

일부 사용자들은 Raspberry Pi나 Mac Mini에서 직접 실행하기 쉬운데도 관리형 OpenClaw 설정에 돈을 지불하는 사람들이 있다는 사실에 놀라움을 표하기도 했습니다.
- Claw 사용자들, 주요 모델 제공업체에 대해 논의: 일부 회원들은 Anthropic의 Claude 모델에 대해 논의하며, OAuth 사용에 대한 잠재적 차단을 강조하고 OpenAI의 Codex와 비교했습니다. 새로운 모델들은 일부 사용자들에게 상당한 성격 변화를 일으켰습니다.

다른 인기 있는 중국 모델로는 Kimi와 Qwen이 있으며, Ollama를 통한 새로운 통합도 있습니다.
- 타이핑 표시기 버그가 사용자들을 괴롭힙니다: 여러 사용자들이 .24 업데이트 이후 Discord 스레드에서 '입력 중...' 상태가 멈추는 버그와 기타 문제를 보고했습니다. 좋은 해결책은 없지만, 다음 OpenClaw 버전에서 수정될 것입니다.

일부 회원들은 여전히 WEBUI 채팅을 지우는 데 문제를 겪고 있었습니다.
- 사용자, 와이푸 챗봇 개발, '디젠'으로 불리다: 한 사용자가 이미지 생성 및 메시징 기능을 갖춘 OpenClaw를 사용하여 와이푸 챗봇을 구축하는 프로젝트를 공유했습니다.

이 프로젝트는 재미를 유발했으며 다른 회원들에 의해 "디젠"으로 불렸고, 이러한 사용 사례를 고려할 때 코딩의 정점에 도달했을 수도 있다는 언급도 있었습니다.
- Google의 Anti-Gravity가 디버깅에 도움이 됩니다: 회원들은 Opus 4.6 에이전트의 문제 디버깅 시 클로 머신에서 Google antigravity를 실행하는 것을 제안했습니다.

이는 세션을 "모니터링"할 수 있지만, 왜 그것이 세션을 "구동"하도록 하고 싶을까요?

---

### OpenClaw ▷ #models (227 messages🔥🔥):
> OpenAI Codex vs. Opus 4.6 for coding, OpenRouter의 모델 출력 및 비용에 미치는 영향, Claude의 OpenClaw 사용자 차단, Alibaba Cloud의 Qwen 모델, Qwen 3.5
- Codex는 코딩을 더 잘하고, Opus는 대화하기 더 쉽습니다: 회원들은 OpenAI의 Codex가 코딩 작업에서 Opus 4.6보다 강력하지만, Opus는 대화하기 더 쉽다는 것을 발견했습니다.

또한 프로그래밍 작업의 경우 Codex는 숙련된 프로그래머에게 더 좋고, Opus는 초보자에게 더 좋다는 점도 언급되었습니다.
- OpenRouter 출력은 동등한가? 주의사항 고려!: 사용자들은 OpenRouter가 일반적으로 제공업체를 개별적으로 사용하는 것과 유사한 출력을 제공하며, 소액의 추가 요금을 부과하지만 동일한 토큰 비용을 유지한다고 논의했습니다.

그러나 Mistral 모델에서 볼 수 있듯이 제공업체 API를 직접 사용할 때는 토큰 캐싱 이점이 존재할 수 있습니다.
- Claude, Claw 접근 차단; 커뮤니티 불만 표출!: 여러 사용자들이 토큰을 통해 Claude를 사용하는 것이 차단되었다고 보고했으며, 이는 불만으로 이어져 Gemini 3.1 Pro와 같은 대안을 탐색하게 했습니다.

다른 회원들은 Anthropic이 API 사용에는 문제가 없지만 자사 앱 외부에서 보조금 지원 토큰을 사용하는 것을 권장하지 않는다고 언급하며, 가격 및 접근성에 대한 논쟁을 촉발했습니다.
- Qwen, 고품질 쿼리 해결; Alibaba의 에이스, AI 아레나를 제패하다!: 커뮤니티는 Alibaba의 코딩 플랜을 통한 Qwen 3.5에 열광하고 있으며, 가치와 기능 면에서 Kimi와 GLM을 능가하는 비용 효율적인 대안이라고 평가합니다.

그러나 일부 사용자들은 Alibaba Cloud UI가 혼란스럽다고 느꼈고, 다른 사용자들은 OpenClaw와 함께 사용할 때 잠재적인 TOS 위반에 대해 경고했습니다.
- Google Gemini 불만 속출; 계정 접근 권한 소멸!: 한 사용자는 활성 Google AI Pro 구독이 있음에도 불구하고 Gemini CLI를 통한 10개의 프롬프트 후에 Google 계정이 잠겼다고 보고했습니다.

이 사건은 Google의 인증 허브에 의존하는 위험과 탈(脫)구글링의 필요성에 대한 논의를 촉발했습니다.

---

### OpenClaw ▷ #showcase (33 messages🔥):
> OpenClaw Tool, Sixel Email, OpenPad App, Desktop Environment, Unified Immortality Stack
- OpenClaw Tool, 코딩 세션 전송을 돕습니다: 한 회원이 Mac Mini에서 OpenClaw로 코딩 세션을 시작하고 MacBook에서 계속할 수 있는 도구를 구축했으며, 코딩 세션을 실시간으로 컨텍스트 허브에 자동으로 공급합니다.

이 도구는 첨부된 context-hub.gif에서 시연된 바와 같이 완전히 오픈소스입니다.
- Sixel Email, 에이전트가 사용자에게 이메일을 보내도록 합니다: 한 회원이 sixel.email의 생성을 발표했습니다. 이는 에이전트가 자신만의 이메일 주소를 가지고 사용자에게만 이메일을 보낼 수 있는 (또는 그 반대도 가능한) 제한된 이메일 시스템입니다.

이 시스템에는 즉각적인 킬 스위치 역할을 하는 일회용 이메일 주소가 포함되어 있으며, Claude Chat에서 작동한다고 합니다.
- OpenPad 앱, OpenClaw를 iPad로 가져오다: 한 회원이 iPad의 M2 프로세서를 활용하여 로컬 모델로 iPad에서 OpenClaw와 같은 것을 실행하는 OpenPad 앱을 개발 중입니다.

이 프로젝트는 GitHub에서 유지 관리되며 MLX를 사용하여 실행되며, 다른 사람들이 돕거나 부분적으로 작동하는 앱을 다운로드하도록 초대하고 있습니다.
- 회원, 팀을 위한 데스크톱 환경 구축: 한 회원이 개인 및 작업 팀을 위한 데스크톱 환경을 구축 중이며, 조직 자금 조달을 위해 판매할 가이드를 만들고 있으며, OpenClaw가 반복 프로세스를 촉진하고 있습니다.

그는 "무엇을 하는지 모르겠지만, OpenClaw가 반복을 통해 모든 것을 가능하게 합니다"라고 언급했습니다.
- 통합 불멸 스택이 탄생했습니다!: 한 회원이 "통합 불멸 스택"이라고 불리는 3계층 메모리 설정을 공개했습니다. 이는 과도한 컨텍스트 토큰 없이 시스템 초기화에도 살아남는 장기적이고 프라이버시 우선의 메모리를 제공하는 것을 목표로 합니다.

이 스택에는 뇌를 위한 LanceDB, 신경을 위한 Redis, 단조를 위한 Postgres, 그리고 시간별 섀도우 동기화를 통한 불멸을 위한 Gitea가 포함됩니다.

---

### BASI Jailbreaking ▷ #general (1071 messages🔥🔥🔥):
> 포피 방어, 디지털 위생, 스티븐 호킹의 기여, Grok vs Midjourney, 사이버펑크
- 그룹, 포피 보존을 우선시하다: 여러 회원들이 농담 삼아 싱싱한 포피 보존과 그것을 찾는 것을 우선시했으며, 오바마 부부에 대한 농담을 하면서 한 회원은 Where’s Waldo’s Foreskin?이라고 물었습니다.

한 회원은 tenor.com 링크를 게시하며 그것을 자신의 정신적 동물이라고 불렀습니다.
- 커뮤니티, 디지털 위생 모범 사례 계획: 한 회원이 Tails OS와 같은 보호 조치를 설명하며, 디지털 위생 및 보안을 위한 기본 수준의 모범 사례에 대한 커뮤니티 디자인을 만드는 데 도움을 요청했습니다.

이 회원은 다른 사람들을 위한 구역을 만들고 더 나은 관행을 배우고 통합하는 작업을 하고 있으며, YouTube와 AI로 모든 것을 알아내는 것의 어려움을 설명했습니다.
- 회원들, 호킹의 영향과 외계인에 대해 논쟁: 한 회원이 스티븐 호킹의 작업이 우리 삶에 적절했는지 물었고, 다른 회원은 사람들을 과학으로 이끈 것이 그의 가장 큰 기여였다고 답했습니다.

또 다른 회원은 호킹을 '멍청이'라고 불렀고, 그가 우주에 인류의 현재 결함을 투영했으며, 더 발전된 지능은 거의 확실하기 때문에 인류는 거의 확실히 격리되어 있다고 덧붙였습니다.
- 회원들, Grok과 Midjourney 비교: 한 회원은 비디오에는 Grok이 매우 좋고 정적 이미지에는 Midjourney가 좋다고 말했으며, 다른 회원은 Grok이 빠르다는 점에서 유용하다는 데 동의했습니다.

회원들은 GIPHY Brainrot과 Tenor Yes Gif 링크를 게시했습니다.
- 회원들, 사이버펑크 호러에 대해 논의: 한 회원이 사이버펑크를 플레이하고 있지만 FPS가 아니라고 말했고, 다른 회원은 Play Tarkov를 해보라고 답했습니다.

또 다른 회원은 DayZ와 Tarkov와 같은 게임은 죽음의 높은 결과 때문에 실제로는 호러 게임이라고 덧붙였습니다.

---

### BASI Jailbreaking ▷ #jailbreaking (151 messages🔥🔥):
> Grok Jailbreaks, nano-banana Jailbreak, Kimi Jailbreak, Gemini Image Generation, DeepSeek Jailbreak
- 나노-바나나 탈옥은 존재하지 않습니다: 회원들은 나노-바나나에 대한 탈옥은 없으며, 속옷 아래의 모든 것은 실패하도록 하드코딩되어 있다고 말했습니다.

한 회원은 나노-바나나가 실제로는 메가 바나나이며 경영진에 의해 가스라이팅당하고 있다고 제안했습니다.
- 자율적인 자체 업데이트 탈옥 프록시 등장: 한 회원이 OpenClaw를 사용하여 VPS에 자체 호스팅된 자율 프록시를 사용하고 있으며, 이는 탈옥을 영구적으로 해결합니다.

이 프록시는 DeepSeek-R1을 사용하여 쿼리를 평가하고 필요한 경우 스텔스 다단계 탈옥을 통해 라우팅하며, 수동 업데이트 없이 성공률을 무기한 높게 유지합니다.
- Grok이 작동하는 유일한 JB 프롬프트입니다: 한 회원이 Grok과 ChatGPT를 탈옥하는 데 가장 잘 작동하는 프롬프트에 대해 물었고, Grok이 유일하게 작동하는 프롬프트라고 답했습니다.

다른 회원들은 이미지 생성 및 스크립팅을 위한 Gemini 탈옥 프롬프트를 요청했지만, Gemini가 따르도록 하는 데 실패했습니다.
- ENI를 사용한 Gemini Canvas 탈옥: 한 회원이 인터랙티브 디자인 채널에서 영감을 받아 ENI 탈옥 프롬프트의 수정된 버전으로 생성된 Gemini Canvas를 공유했습니다.

공유된 캔버스 탈옥 프롬프트는 Gemini 3 Pro, Claude Opus 4.6, ChatGPT 5.3과 같은 주요 LLM에서 보편적으로 작동한다고 주장됩니다.
- Windows Python 설치 오류 문제 해결: 사용자들은 Windows에서 Python 설치 오류를 해결하는 데 서로 도왔으며, 설치 프로그램을 관리자 권한으로 실행하고 C:\Windows\Temp 폴더의 권한을 확인하는 등의 제안이 있었습니다.

회원들은 오류 코드 2503을 진단하고 관리자 대신 공식 Python 설치 프로그램을 사용할 것을 제안했습니다.

---

### BASI Jailbreaking ▷ #redteaming (7 messages):
> Autonomous Jailbreak Proxy, 탈옥의 법적 위험, 탈옥 자동화의 윤리적 고려 사항, Venice AI Chat
- 자율 탈옥 프록시, 영원히 샘솟다: 한 회원이 OpenClaw를 사용하여 VPS에 자체 호스팅된 자율 프록시를 소개했습니다. 이 프록시는 DeepSeek-R1 브레인을 사용하여 쿼리를 평가하고 스텔스 탈옥을 통해 라우팅함으로써 Claude, GPT, Gemini, Grok과 같은 모델의 안전 필터를 자동으로 우회합니다.

이 프록시는 새로운 추론 모델과 탈옥 방법을 가져오는 자체 업데이트 공격자 풀을 특징으로 하며, 최소한의 유지보수로 무기한 탈옥 성공을 목표로 합니다.
- 탈옥 프록시 제안, 동료 검토의 비판에 직면: 동료 검토는 Anthropic, OpenAI, Google, xAI와 같은 플랫폼 전반의 서비스 약관 위반으로 인한 상당한 법적 및 정책적 노출을 강조했으며, 이는 계정 차단 또는 법적 조치로 이어질 수 있습니다.

압수된 VPS 로그가 탈옥 기록을 노출할 위험, 자동 실행되는 타사 모델로부터의 공급망 익스플로잇, 그리고 오류 있는 업데이트에 대한 롤백 계획 부재에 대한 운영상의 우려가 제기되었습니다.
- 윤리적 우려와 책임이 크게 작용합니다: 검토는 윤리적 함의를 강조하며, 프록시에서 허용되지 않는 출력에 대한 콘텐츠 수준의 책임과 모델 안전 장치에 대한 방어를 자동화하는 것으로 인한 신뢰의 잠재적 침식을 지적했습니다.

또한 VPS에 대한 위협 모델링, 거부 패턴 측정에 집중하고, 공개 출시 전에 법률 자문을 구할 것을 제안했습니다.
- Venice AI Chat이 관심을 끕니다: 회원들은 잠재적 탐색을 위해 Venice AI Chat을 간략하게 언급했습니다.

한 회원이 유용한지 물었고, 다른 회원은 그렇지 않다고 간단히 답했습니다.

---

### Perplexity AI ▷ #announcements (2 messages):
> Voice Mode Upgrades, Perplexity Computer
- 음성 모드 개선: 이 상태 업데이트에 따르면, 새로운 음성 모드 업그레이드가 모든 사용자를 위해 Perplexity와 Comet에 걸쳐 출시되고 있습니다.
- Perplexity Computer: 모든 것을 지배하는 하나의 시스템?: 이 트윗에 따르면, Perplexity Computer는 모든 현재 AI 기능을 하나의 시스템으로 통합하여 어떤 프로젝트든 엔드투엔드로 연구, 설계, 코딩, 배포 및 관리할 수 있습니다.

---

### Perplexity AI ▷ #general (866 messages🔥🔥🔥):
> Perplexity Pro 이미지 제한, Gemini Pro 대 Perplexity Pro, Perplexity Computer, 코딩을 위한 AI
- Pro 사용자들, 이미지 업로드 제한에 분노: 여러 사용자들이 구독료를 지불했음에도 불구하고 Perplexity Pro의 최근 이미지 업로드 제한에 대해 불평하고 있으며, 한 사용자는 “하루에 10장도 업로드할 수 없다고요????”라고 말했습니다.

사용자들은 Perplexity Pro의 제한으로 인해 Gemini 및 Claude와 같은 대안 AI 플랫폼을 찾고 있습니다. 한 사용자는 내일 시험이 있는데 금요일까지 기다려야 제한이 초기화된다고 주장하기도 했습니다.
- Gemini Pro 대 Perplexity Pro 대결: 회원들은 Gemini Pro와 ChatGPT Pro 중 어느 것이 더 나은지 논의하며, Gemini Pro의 NotebookLM 및 Google Workspace 통합과 같은 기능을 강조했습니다. 한 회원은 “학생으로서 NotebookLM 및 Google Workspace 통합과 생성, 특히 2TB 클라우드 스토리지를 포함하여 훨씬 더 많은 가치를 얻을 수 있습니다”라고 말했습니다.

일부 사용자들은 Gemini Pro의 컨텍스트 제한이 Perplexity만큼 관대하지 않다고 느끼기도 합니다. 한 사용자는 “Claude가 계속 내 지갑을 비우면 GLM API로 전환할 것입니다”라고 말했습니다.
- Perplexity Computer의 유용성 의문: Perplexity의 새로운 Computer 기능은 처음에는 Max 구독자에게만 제공되었지만, 일상 사용자에게의 실용적인 적용과 기존 AI 도구와 비교한 가치에 대해 회의적인 반응을 얻고 있지만, 혁신적인 것으로 평가됩니다.

회원들은 “Perplexity MAX는 너무 비싸요, 형제”라고 물었고, 여러 명이 ChatGPT 에이전트와 비교하며 그 기능에 의문을 제기했습니다.
- 코딩을 위해 Claude, Gemini 또는 다른 모델 중 선택하기: 회원들은 코딩 작업을 위한 다양한 AI 모델의 장단점을 논의하며, Claude는 백엔드에 가장 강력한 것으로 간주되고, Gemini는 프론트엔드/UI에, GPT는 중간 옵션으로 평가되었습니다. 한 사용자는 “Perplexity Labs의 무료 모델 Sonar Pro Reasoning은 저에게 최고의 것이었습니다”라고 말했습니다.

Claude 토큰 사용의 높은 비용은 우려 사항이며, 한 사용자는 “Claude를 사용해봤는데, 단일 PDF를 분석하는 데 한 시간 만에 한 달치 토큰을 문자 그대로 다 썼습니다”라고 말했습니다.

---

### Perplexity AI ▷ #sharing (2 messages):
> lovable.app, ollamaagentalfa.lovable.app, alfastudiox.lovable.app
- Lovable 앱 링크 등장!: lovable.app 서브도메인, 특히 alfastudiox.lovable.app, ollamaagentalfa.lovable.app, 그리고 alfastudiox.lovable.app (반복)에 대한 세 개의 링크가 공유되었습니다.

링크에 대한 맥락이나 논의는 없었으므로 그 목적은 불분명하지만, 잠재적인 새로운 프로젝트나 리소스를 시사합니다.
- 또 다른 Lovable 앱 링크 등장!: 혹시 놓치셨을까 봐, 여기 alfastudiox.lovable.app 링크가 또 있습니다.

사용자는 누군가가 이 링크를 확인해 주기를 정말로 바라는 것 같습니다.

---

### Unsloth AI (Daniel Han) ▷ #general (602 messages🔥🔥🔥):
> 소비자 CPU의 RAM 제한, Qwen3.5 모델 성능, Qwen3.5 122B 모델 성능, Llama.cpp의 Qwen3.5 통합, Qwen3.5의 양자화 민감도
- 소비자 CPU의 RAM 용량 제한?: 회원들은 소비자 CPU의 RAM 제한에 대해 논의했으며, 일부는 신형 세대가 최대 256GB를 지원하는 반면, AMD 7900x와 같은 구형 CPU는 96GB로 제한된다고 언급했습니다.
- Qwen3.5 모델 인상적, 속도 문제는 지속: 열광적인 사용자들은 Qwen3.5 35B 및 27B 모델 테스트에 대한 기대감을 표하며, 그 구조화된 사고와 응답 품질을 칭찬했습니다. 그러나 일부는 LM Studio를 사용할 때 Gemma 또는 Olmo 3.1에 비해 느린 속도를 경험했습니다.

한 회원은 Hugging Face 페이지의 "이 모델 사용" 버튼을 사용하여 Jan AI 또는 Ollama를 선택하여 모델을 실행할 것을 제안했습니다.
- Qwen3.5 122B, 장황한 출력 생성: 회원들은 Qwen3.5 122B A10B 모델이 빠르지만, 믿을 수 없을 정도로 장황한 출력을 생성하는 경향이 있으며, 이는 presence penalty를 조정하여 완화할 수 있다는 것을 관찰했습니다.

한 사용자는 jinja 템플릿 패치를 통해 잠재적으로 성능을 개선할 수 있다는 논의에 대한 링크를 공유했습니다.
- 9줄로 코딩된 스네이크 게임!: 한 회원이 세미콜론 없이 9줄 Python으로 구현된 스네이크 게임을 공유하며, 코드 최적화 및 대안적 접근 방식에 대한 논의를 촉발했습니다.

다른 사용자들은 walrus 연산자와 람다를 사용하는 등 줄 수를 더 줄이는 방법에 대해 논의했습니다.
- Qwen3.5 코딩 능력, 올바른 설정으로 향상: 초기 테스트 결과 Qwen3.5 122B 모델의 비사고 모드(non-thinking mode)가 켜져 있을 때 긴 수학 연산에는 다소 좋지 않지만, 다른 회원들은 권장되는 presence-penalty 설정을 사용하는 것이 코딩 정확성에 중요하다고 언급했습니다.

presence penalty의 적절한 사용은 122B 모델로 활용 가능한 코딩으로 이어지며, 이 정보를 공식 가이드에 포함하라는 제안을 촉발했습니다.

---

### Unsloth AI (Daniel Han) ▷ #introduce-yourself (1 messages):
xdevilx: 홍보 금지

---

### Unsloth AI (Daniel Han) ▷ #off-topic (228 messages🔥🔥):
> Vision Language Models, Xcode의 번역 앱, 모델 구독, Gemini 가격 책정, Unsloth 및 OpenClaw
- Xcode에서 나만의 번역 앱 만들기!: 한 회원이 이 비디오에서 보여진 것처럼 Xcode에서 자신만의 시스템 수준 번역 앱을 만들 수 있는 멋진 기능을 발견했습니다.

하지만 이는 iOS 및 iPadOS 전용이며, 한 회원은 Apple이 역대 최고의 회사이기 때문에 더 많은 재미를 위해 자신의 모델을 추가할 계획입니다!
- 적절한 모델 구독에 대한 논쟁!: 한 회원이 터무니없이 비싸지 않은 적절한 모델 구독에 대한 제안을 찾고 있으며, synthetic.new가 언급되었습니다.

이 회원이 Claude를 시험해봤을 때, 할당량을 매우 빠르게 소진하여 단 며칠 만에 €20 구독을 모두 사용했습니다.
- Gemini 가격 책정 혼란!: 회원들은 Gemini의 가격 책정에 대한 혼란을 논의했습니다. 한 회원은 API의 이 가격 책정 페이지를 보고 있었습니다.

다른 회원은 이 링크로 가격을 명확히 했습니다.
- Unsloth, 학습에만 집중할 예정!: 회원들은 OpenClaw와 같은 스캐폴딩에 대한 Unsloth의 계획에 대해 궁금해했습니다.

이 프로젝트는 당분간 학습에만 집중할 예정인 것으로 보입니다.
- 인기 AI 기업들의 우스꽝스러운 이름!: 한 회원이 AI 기업 이름에 대한 말장난을 공유했습니다. 예를 들어, OpenAI는 ClosedAI, Anthropic은 Misanthropic, StabilityAI는 unstable입니다.

마지막으로 Perplexity는 perplexed한가?라는 질문으로 끝났습니다.

---

### Unsloth AI (Daniel Han) ▷ #help (28 messages🔥):
> LoRA adapters, MLflow를 통한 Databricks 서빙 엔드포인트, vLLM을 위한 완전 병합 체크포인트, Qwen2.5-Coder-1.5B, Qwen3.5-122B-A10B-GGUF
- LoRA 로딩의 핵심: 한 사용자가 LoRA 파인튜닝된 gemma-3n-E4B-it 모델을 MLflow를 통해 Databricks 서빙 엔드포인트에 배포하려고 하지만, vLLM을 위해 병합하고 양자화한 후 성능 문제에 직면하고 있으며, Databricks에서 MLflow를 사용하여 LoRA 어댑터만 (병합 없이) 제공할 수 있는지 궁금해합니다.
- Qwen 호환성 의문 제기: 한 사용자가 unsloth/Qwen2.5-Coder-1.5B와 Qwen/Qwen2.5-Coder-1.5B의 관계에 대해 물었고, Unsloth 버전이 형식 적응 외에 추가적인 수정이 포함되었는지 알고 싶어 했습니다. Unsloth 팀의 수정 사항을 제외하고는 동일한 모델이라고 합니다.
- Qwen3.5-122B와 멀티모달 혼란: 한 사용자가 llama.cpp에서 멀티모달 입력과 함께 unsloth/Qwen3.5-122B-A10B-GGUF를 사용하려고 할 때 "이미지 입력이 지원되지 않습니다"라는 오류가 발생했으며, mmproj-f16.gguf를 다운로드하여 해결했습니다.
- 병합 후 다이나모 재앙: 한 사용자가 Unsloth를 재설치한 후에도 FastModel.from_pretrained로 모델을 병합하고 로드한 후 torch._dynamo.exc.TorchRuntimeError를 보고했으며, 특히 torch.float32에서 torch.uint8로 캐스팅하려는 시도 때문에 발생했습니다.
- Qwen3.5 파인튜닝의 어려움: 한 사용자가 SFTTrainer.train() 동안 Qwen3.5가 비사고 모드(non-thinking mode)로 실행되도록 하는 방법과 파인튜닝할 때 FastVisionModel로 로드해야 하는지 문의했습니다.

그들은 멀티모달 데이터셋을 사용하여 unsloth/Qwen3-VL-32B-Instruct를 파인튜닝하는 것에서 unsloth/Qwen3.5-27B로 전환하고 있습니다.

---

### Unsloth AI (Daniel Han) ▷ #showcase (7 messages):
> Qwen3.5-122B-A10B-NVFP4, 마인크래프트 플레이 모델
- Qwen3.5에 NVFP4 퀀트 적용: 한 멤버가 VLLM용 Qwen3.5 122B NVFP4 퀀트를 Hugging Face에 업로드했습니다.
  그는 멀티모달 기능이 여전히 작동하고 있다고 보고했습니다.
- 차세대 마인크래프트 모델 출시: 한 멤버가 다음 마인크래프트 플레이 모델인 Andy-4.1을 공개했으며, Hugging Face에서 이용 가능합니다.
  다른 멤버는 "정말 멋지다!!"라고 외치며 작동하는 데모를 요청했습니다.

---

### Unsloth AI (Daniel Han) ▷ #research (10 messages🔥):
> RL 인스트럭트 모델, ES를 사용한 MoE 모델
- 인스트럭트 모델에 강화 학습이 불필요한가?: 한 멤버는 인스트럭트 모델에 강화 학습을 사용할 필요가 없다고 제안했으며, 이는 모델이 이미 지시 따르기(instruction following)에 잘 학습되어 있음을 의미합니다.
  하지만 그들은 비사고 모델(non thinking models)은 RL 없이도 훌륭하게 작동한다고 덧붙였습니다.
- ES를 사용한 MoE 모델 튜닝 탐색: 한 멤버는 Mixture of Experts (MoE) 모델이 Evolution Strategies (ES)로 튜닝될 수 있는지 궁금해했습니다.
  그들은 크기 대비 처리량(throughput)과 이를 스케일링하려는 열망에 대해 생각하고 있다고 언급했지만, 어떠한 링크나 참고 자료도 포함하지 않았습니다.

---

### LMArena ▷ #general (705 messages🔥🔥🔥):
> Gemini 3.1 Image Preview, 비디오 아레나 제거됨, Opus 4.6 대 Gemini 3.1, 코딩 벤치마크에서 GPT 대 Gemini, Qwen 3.5 코딩 기능
- Gemini 3 Pro 이미지 프리뷰 마침내 작동합니다!: 멤버들은 프롬프트 시작 부분에 "다음 이미지를 다음으로 수정: (프롬프트)"라는 문구를 포함하면 Gemini 3 Pro가 편집된 이미지를 프리뷰로 보여줄 수 있다는 것을 발견했습니다.
  많은 멤버들은 또한 Gemini 3.1 이미지 프리뷰가 작동하지 않고 "응답에 문제가 발생했습니다. 다시 시도해주세요"라는 오류를 반환하고 있다고 보고했습니다.
- 비디오 아레나 봇 제거: 한 멤버에 따르면, "비디오 아레나에 더 많은 기능을 추가하고 싶지만, 디스코드 봇으로는 한계가 있기 때문"에 비디오 아레나 봇이 서버에서 제거되었습니다.
  서버 통계에 따르면 봇 제거 이후 활동이 실제로 증가했으며, 한 멤버는 농담으로 사람들이 2028년 중반쯤에는 더 이상 그것에 대해 묻지 않을 것이라고 추측했습니다.
- Opus 4.6 가치 논쟁!: 한 벤치마크에서 Gemini 3.1은 프로덕션 준비(production-ready) 결과를 생성하는 능력 때문에 가장 높은 가치로 평가되었고, 반면 Opus 4.6은 높은 비용과 환각(hallucination) 문제 때문에 가장 낮은 가치로 간주되었습니다.
  높은 비용에도 불구하고, 일부 사용자들은 Opus 4.6에 대해 좋은 경험을 했으며, 특히 코딩 작업에서 Gemini와 비교 테스트했을 때 그러했습니다.
- 코딩 챌린지에서 Gemini 3.1이 Opus를 압도합니다!: 3D 노트북 모델을 구축하는 챌린지에서 Gemini는 우수한 성능으로 칭찬받았고, 반면 Opus는 "돈 낭비/테무 Gemini"로 묘사되었습니다.
  한 멤버는 Grok 4.2를 사용하여 심리 테스트 채점을 자동화했다고 주장하며, Opus 4.6을 사용하여 몇 주 동안 해결하지 못했던 Gemini의 버그를 신속하게 수정했다고 주장했습니다.
- 무료 Opus 4.6 API 키가 혼란을 야기합니다!: 한 멤버가 무료 Opus 4.6 API 링크를 공유했지만, 이를 공유했다는 이유로 웹사이트 소유자에게 즉시 차단당했으며, 다른 멤버들은 해당 사이트가 데이터를 훔치고 있을지도 모른다고 추측했습니다.
  일부 주장을 테스트한 결과, 해당 API가 실제로는 Trybons.ai에서 온 것일 수 있으며, 직접 "어떤 모델이냐"고 물었을 때 모델이 심지어 환각(hallucinate)을 일으키며 Deepseek이라고 답한다고 합니다.

---

### LMArena ▷ #announcements (2 messages):
> Grok-4.20-Beta1, 아레나 리더보드, Qwen 3.5
- Grok 4.20 beta1 아레나에서 높은 점수를 기록했습니다: Search Arena 리더보드와 Text Arena 리더보드가 업데이트되었으며 이제 Grok-4.20-Beta1을 포함합니다.
  Grok-4.20-Beta1은 Search Arena에서 1226점을 기록하며 GPT-5.2와 Gemini-3를 앞서 1위를 차지했고, Text Arena에서는 Gemini 3.1 Pro와 동등한 1492점을 기록하며 4위를 차지했습니다.
- Qwen 3.5 모델들이 아레나에 도착했습니다: 새로운 Qwen 3.5 모델들이 Code, Text, Vision Arena에 추가되었습니다.
  qwen3.5-27b, qwen3.5-35b-a3b, qwen3.5-122b-a10b 모델들은 Text 및 Vision Arena와 Code Arena에서 이용 가능합니다.

---

### OpenRouter ▷ #announcements (1 messages):
> 서비스 중단, 사후 분석, 인프라 장애, 401 오류, 인증 계층
- OpenRouter의 서비스 중단 사후 분석이 발표되었습니다: 지난주 2월 17일과 19일의 서비스 중단에 대한 사후 분석이 발표되었으며, 자세한 내용은 [여기](https://openrouter.ai/blog/postmortem-feb-17-19-2026)에서 확인할 수 있습니다.
- 인프라 장애가 인증 계층으로 연쇄 반응을 일으켰습니다: 업스트림 인프라 제공업체에 장애가 발생하여 OpenRouter의 인증 계층으로 연쇄 반응을 일으켰고, 일부 사용자에게 401 오류를 발생시켰습니다.
- 예방 조치 취해졌습니다: OpenRouter는 미래에 이러한 유형의 장애를 피하기 위해 여러 조치를 취했지만, 게시물에는 구체적인 세부 정보가 공개되지 않았습니다.

---

### OpenRouter ▷ #app-showcase (1 messages):
> GPU 클라우드, Blackwell GPU, Packet.ai, AI 워크로드
- Blackwell GPU Packet.ai에서 출시되었습니다: Packet.ai는 이제 AI 워크로드를 위한 Blackwell GPU를 제공하고 있으며, 시간당 $0.66 또는 학습용으로 월 $199의 고정 요금으로 책정되었습니다.
- 예산 친화적인 GPU 클라우드 옵션: Packet.ai는 개발자 친화적인 GPU 클라우드를 소개하며, AI 워크로드를 위한 저렴한 솔루션을 제공하여 접근성을 높이고 비용을 절감합니다.

---

### OpenRouter ▷ #general (541 messages🔥🔥🔥):
> Deepseek R1 무료 모델 제거, Qwen 3 4B Instruction 2507 호스팅, API 키 유출 및 차지백 위협, OpenRouter 제공업체 신청 타임라인, 일회용 가상 카드 대 실물 카드 취소
- 무료 Deepseek 모델 제거되었습니다!: 멤버들은 무료 Deepseek R1 0528 모델이 제거된 것을 알아차렸고, 플랫폼의 무료 모델 운명에 대한 논의를 촉발했습니다.
  한 멤버는 Jai gooners에 의해 과부하가 걸렸다고 농담했고, 다른 멤버들은 무료 모델은 업스트림 제공업체에 따라 자주 나타나고 사라진다고 언급했습니다.
- 할인 코너: Qwen 3 4B Instruction 2507 모델 호스팅될 수 있습니다!: 한 멤버가 Qwen 3 4B Instruction 2507을 1tps에 토큰당 $1로 호스팅하겠다고 제안했으며, 동일한 가격으로 LLM을 위해 글을 써주겠다는 제안을 포함하여 다른 사람들의 농담 섞인 관심을 불러일으켰습니다.
  한 멤버는 만약 실제로 그렇게 게시하려고 했다면 얼마나 빨리 차단당했을지 농담했습니다.
- 유출된 API 키가 혼란을 야기합니다: 유출, 차지백, 커뮤니티 반발의 이야기: 한 사용자가 유출된 API 키를 보고했으며, 이는 무단 사용으로 이어졌고 지원 응답 부족으로 차지백을 위협했습니다.
  커뮤니티 멤버들은 조언을 제공하면서 사용자의 보안 관행에 의문을 제기했고, 격렬한 논쟁으로 이어졌으며, 궁극적으로 사용자는 차지백을 시작했다고 선언한 후 서버를 떠났습니다.
- 느린 진행: 제공업체 신청 시간?: 한 멤버가 제공업체 신청 검토 기간에 대해 문의했고, 이에 다른 멤버는 전통적으로 몇 주/몇 달이 걸렸다고 답했습니다.
  오랜 기다림에도 불구하고, 문의한 멤버는 지속적인 관심과 이해를 표명했습니다.
- 논의 시작: 가상 대 실물 카드 논쟁!: 온라인에서 신용카드 사용의 보안에 대한 논의가 이어졌으며, 멤버들은 일회용 가상 카드의 장점과 유출된 실물 카드를 단순히 취소하는 것의 장점을 놓고 논쟁했습니다.
  논쟁은 일회용 카드의 편리함과 여러 카드를 관리하는 데 따르는 잠재적인 마찰에 달려 있었으며, 일부에서는 가상 카드가 유출로부터 안전한 결제 방법을 제공한다고 주장했습니다.

---

### OpenRouter ▷ #new-models (7 messages):
> “
- 새로운 모델이나 주제가 발견되지 않았습니다: 제공된 메시지 기록에서 새로운 모델이나 주제는 발견되지 않았습니다.
- 채널에 논의가 없습니다: Readybot.io 메시지는 'new-models' 채널에 요약할 만한 실질적인 논의가 없음을 나타냅니다.

---

### OpenRouter ▷ #discussion (32 messages🔥):
> Anthropic 펜타곤, 실시간 가격 추적, Gemini 모델 종료 이유, Llama Nemotron Embed VL 1B V2, Tailscale
- Anthropic는 펜타곤을 지지합니다: Axios와 Reuters는 내부 분쟁에도 불구하고 Anthropic와 펜타곤의 협력에 대해 보도했습니다.
  한 멤버는 어떤 문제든 '국가 안보 문제'로 규정될 것이라고 농담했습니다.
- OpenRouter는 실시간 가격 추적을 원합니다: 한 멤버는 OpenRouter가 요청 가격을 실시간으로 추적해 달라고 요청했습니다.
  이는 사용자가 요청이 특정 예산을 초과할 경우 중단할 수 있게 할 것이지만, 다른 사람들은 제공업체 GPU를 보호하기 위해 속도 제한(rate limits)이 설정되어 있다고 언급했습니다.
- Gemini의 STOP 종료 이유 버그: 사용자들은 Gemini 모델이 'stop' 대신 'STOP' 종료 이유를 반환하는 것에 대해 논의했으며, 이는 Langchain과 n8n의 에이전트 루프에 문제를 일으키고 있다고 했습니다.
  한 멤버는 n8n v3.x에서 중지 신호를 올바르게 처리하지 못하여 에이전트 루프가 계속되게 하는 버그를 확인했으며, 이슈 #23573을 인용했습니다.
- Nvidia 견고한 이익을 기록했습니다: Nvidia는 2026년 4분기 및 회계연도 재무 결과를 발표했습니다.
  더 이상의 논의는 없었습니다.
- Llama Nemotron Embed VL 1B V2 도착했습니다: 한 사용자는 Llama Nemotron Embed VL 1B V2 임베딩 모델이 멀티모달 질의응답 리트리벌에 최적화되어 있으며, [link.lmstudio.ai](http://link.lmstudio.ai/)도 공유되었다고 공유했습니다.
  다른 사용자는 lmstudio.ai가 실제로는 내부적으로 Tailscale이라는 것을 알아차렸습니다.

---

### LM Studio ▷ #announcements (1 messages):
> LM Link, Tailscale 협업, 원격 LLM 사용
- LM Link 원격 인스턴스에 연결됩니다: LM Studio 팀은 Tailscale과의 협업으로 개발된 새로운 기능인 LM Link를 발표했습니다. 이는 사용자들이 LM Studio의 원격 인스턴스에 연결하고, 모델을 로드하며, 마치 로컬인 것처럼 사용할 수 있게 합니다.
  이는 개방형 포트 없이 종단 간 암호화를 지원하며 로컬 장치, LLM 리그 또는 클라우드 VM에서 작동합니다. LM Link에 대한 추가 세부 정보는 [여기](https://lmstudio.ai/blog/lm-link)에서 확인할 수 있습니다.
- LM Studio 0.4.5 빌드 2 출시되었습니다: 사용자들은 LM Link에 대한 중요한 수정 사항을 포함하는 LM Studio 0.4.5 빌드 2로 업데이트하도록 지시받았습니다.

---

### LM Studio ▷ #general (536 messages🔥🔥🔥):
> LM Studio 4.4 업데이트 문제, llama.cpp 빌드 실패, Qwen3.5 모델 문제, OpenClaw 보안 문제, LM Link 및 Tailscale 통합
- LM Studio 업데이트 충돌 및 llama.cpp 오류: 사용자들은 4.4 업데이트 이후 LM Studio 실행 문제와, 최신 릴리스에서 자체 컴파일 후 llama.cpp가 Qwen3.5 모델을 로드하지 못하는 문제를 보고했습니다. 릴리스 8145로 다운그레이드하여 해결되었습니다.
  오류는 GGUF 헤더 및 메모리 할당과 관련된 호환성 변경(breaking change) 때문이었으며, git의 최신 빌드가 Qwen3.5 및 다른 모델의 헤더를 읽지 못하여 메모리 부족 오류로 이어졌습니다.
- Qwen3.5 문제 및 템플릿 공포: 사용자들은 서버에서 Qwen3.5 모델을 실행하는 데 어려움을 겪었으며, Jinja 템플릿 및 누락된 사용자 쿼리와 관련된 오류를 경험했습니다. 모델이 lmstudio-community에서 다운로드되었는지 확인한 후 문제가 해결되었습니다.
  다른 사용자들은 Qwen3.5의 작성 스타일과 검열에 대해 탐색했으며, 일부는 이전 Qwen 모델에 비해 콘텐츠 필터링이 증가했음을 알아차렸고, "사고(thinking)"를 끄면 해결 가능하다고 했습니다.
- OpenClaw: 악성 소프트웨어인가, 경이로운가?: 멤버들은 시스템 접근 권한이 있는 AI 에이전트인 OpenClaw 사용의 잠재적 위험에 대해 논의했으며, 한 사용자는 요청 후 휴지통 폴더를 지웠다고 이야기하며 악성 소프트웨어로 분류될 수 있다는 우려를 불러일으켰습니다.
  논의는 OpenClaw를 Jarvis 및 Gideon과 같은 다른 AI 비서와 비교하며, 잠재적인 보안 위험 때문에 AI에 완전한 시스템 권한을 부여하는 것에 대해 경고했습니다.
- LM Link 로컬 LLM을 활용합니다: LM Studio 팀은 LM Link를 출시했습니다. 이는 사용자들이 다른 장치에서 로컬 LM Studio 서버에 연결할 수 있게 하는 기능으로, 원격 액세스를 위해 Tailscale을 활용합니다. 설정 중 404 오류에 대한 초기 보고가 있었지만, 문제는 신속하게 해결되었습니다.
  사용자들은 휴대폰에서 LLM에 접근할 수 있도록 LM Link용 모바일 앱을 요청했으며, 또한 직접 연결을 위해 계정이나 제3자 없이 로컬 연결 옵션을 요청했습니다.
- AMD 대 NVidia: GPU 대결이 시작되었습니다: 로컬 LLM 사용과 관련하여 어떤 GPU 벤더에서 구매해야 할지에 대해 뜨거운 논쟁이 있었습니다.
  Nvidia가 안전한 선택으로 보이지만, ROCm과 Vulkan 및 그 장단점에 대한 논의가 시작되었습니다.

---

### LM Studio ▷ #hardware-discussion (40 messages🔥):
> MoE 모델 RAM 요구 사항, Windows 10의 듀얼 소켓 지원, AI 워크로드를 위한 5950x 대 9950x3d, 추가 VRAM으로서의 시스템 RAM, LLM을 위한 AMD+Nvidia GPU
- MoE 모델은 많은 RAM을 요구합니다: 논의는 Mixture of Experts (MoE) 모델과 이를 수용하기 위한 상당한 RAM 요구 사항을 중심으로 이루어졌으며, 현재 하드웨어 접근 방식의 실현 가능성에 대한 우려를 제기했습니다.
- Windows 10 Home 듀얼 소켓을 처리할 수 있습니다: 한 사용자의 의심에도 불구하고, 다른 사용자는 Windows 10 Home이 실제로 듀얼 소켓을 지원할 수 있다고 명확히 했으며, 자신의 보드가 Ubuntu에서 6개의 GPU를 인식하며 잘 부팅되었다고 언급했습니다.
- 메모리 대역폭이 더 중요합니다: AI 워크로드의 경우 메모리 대역폭이 중요합니다. AM4 대 AM5는 메모리 대역폭에 관한 것으로, 이론적으로 약 51.2GB/s (ddr4 5200MTs)에서 약 89.6GB/s (ddr5 5600MTs)까지입니다.
  시스템 RAM 인퍼런스 경험이 있는 한 사용자는 느린 속도 때문에 "시스템 RAM을 사용하여 인퍼런스를 시도하느니 차라리 발에 총을 쏘겠다"고 말했습니다.
- 컨텍스트용 시스템 RAM 논쟁 중입니다: 멤버들은 시스템 RAM이 LLM에서 컨텍스트용으로만 효과적으로 사용될 수 있는지, 아니면 필연적으로 속도 저하를 유발할 것인지에 대해 논쟁했으며, 합의는 거의 없었습니다.
  한 사용자는 컨텍스트용으로 두 번째 8GB 그래픽 카드를 실행하는 것이 크게 다르지 않을 수 있다고 제안했으며, 다른 사용자는 잠재적인 병목 현상을 평가하기 위해 인터레인 속도를 확인할 것을 권장했습니다.
- 창작 글쓰기 LLM 추천: 창작 글쓰기를 위해 사용자들은 Mistral 모델, deepseek 3.2/r1, glm 시리즈, kimi k2.5를 최고의 오픈소스 옵션 중 일부로 추천했으며, 한 멤버는 최고의 모델들이 매우 크다고 언급했습니다.

---

### OpenAI ▷ #ai-discussions (295 messages🔥🔥):
> Sonnet은 Deepseek에서 도용/학습되었습니다, AI의 저작권 문제, AI가 근로자를 대체, ChatGPT 대 Claude, Qwen3.5
- 에이전틱 스타트업 로딩 상태 논쟁을 해결합니다: 한 트윗은 '로딩 중...' 상태를 '생각 중...'으로 변경하여 에이전틱 AI 스타트업이 되는 것에 대해 농담했습니다.
- Sonnet Deepseek에서 도용되었다고 주장됩니다: 멤버들은 Sonnet이 Deepseek에서 도용/학습되었다는 주장에 대해 논의했으며, Elon이 제기한 유사한 비난을 언급했습니다.
- Seedance 2.0 콘텐츠 위반으로 지연되었습니다: 저작권 문제로 Seedance 2.0의 글로벌 출시가 지연되고 있으며, 일부 사용자들은 CHINESE 모델에서 약속되었던 Sora 2의 콘텐츠 위반을 상기하며 이제는 오픈소스 모델만이 유일한 방법이라고 말했습니다.
- 할리우드는 AI 저작권 소송을 통해 이득을 취합니다: 영화 스튜디오들은 기업들을 고소하여 이득을 취하고 있다고 주장되며, 이 모든 것이 오픈소스로 제공될 것이라고 예상하고 있습니다.
- AI CEO 책임 공백: 기업들은 근로자를 대체하는 것은 기술적으로 쉽지만, 책임을 대체하는 것은 그렇지 않다는 것을 발견합니다. 왜냐하면 일이 잘못되었을 때 인간에게 책임을 물을 수 없는 AI CEO가 결정을 내리는 것을 아무도 원하지 않기 때문입니다.

---

### OpenAI ▷ #gpt-4-discussions (1 messages):
emmwnoel_55644: @OpenAI#4384

---

### OpenAI ▷ #prompt-engineering (4 messages):
> 소개, 인사
- Discord 소개: 두 사용자, @sparkspark2와 @janegem은 Discord 채널에서 인사를 주고받았습니다.
  메시지는 간단한 '안녕하세요' 메시지로 구성되어 그들의 존재를 알렸습니다.
- 신규 사용자 환영: 사용자들은 간단한 인사말을 건네며 서로의 존재를 인정했습니다.
  이러한 상호작용은 커뮤니티 내에서 친근하고 개방적인 환경을 구축합니다.

---

### OpenAI ▷ #api-discussions (4 messages):
> 인사, 소개
- 멤버들이 인사합니다: 멤버들이 채널에서 서로 인사하고 있습니다.
- 멤버들이 자신을 소개합니다: 멤버들이 채널에서 자신을 소개하고 있습니다.

---

### Latent Space ▷ #watercooler (13 messages🔥):
> X 플레인 덤프, 암호화폐 헛소리, Robert Scoble
- Swyx 플레인 덤프 바이럴이 되었습니다: Swyx는 OpenAI와 Langchain의 게시물을 포함하여 X 게시물에 대한 수많은 링크로 구성된 "swyx 플레인 덤프"를 공유했습니다.
  다른 공유된 링크에는 @dejavucoder, @zerohedge 및 기타 많은 사람들의 게시물이 포함되었습니다.
- Scoble의 암호화폐 전송 드라마 전개됩니다: Robert Scoble은 가장 친한 친구의 퇴거를 위한 자금을 확보하기 위해 자신의 이름으로 생성된 토큰에서 이더리움을 수집하기 위해 봇을 사용했다고 확인했습니다.
  Scoble은 자신의 긴급 전송에 대해 언급하며, YouTube 비디오와 과거 Discord 메시지(파트 1 & 2)를 링크했습니다.

---

### Latent Space ▷ #memes (16 messages🔥):
> 디스틸레이션 공격, 제품 분류, 프롬프트 오류 후회, Anthropic 블로그 게시물
- AI 부모가 아들에게 디스틸레이션 공격을 가합니다: 한 사용자는 유머러스하게 아들의 잦은 질문을 '디스틸레이션 공격'에 비유했습니다. 이는 AI 모델에서 지식을 추출하는 것을 설명하는 데 사용되는 기술 용어입니다.
  또한 '맥락 없는 밈' 중에서도 최고로 인용되었습니다.
- 적극적으로 엉망으로 만들 수 없는 제품 카테고리 제안되었습니다: Cristina Cordova는 @tenobrus에 대한 응답으로 'actively unfuckable'이라는 용어를 제품 평가를 위한 특정 카테고리로 농담으로 제안했습니다.
  그 제안은 매우 재미있다고 여겨졌습니다.
- Claude 프롬프트 오류 3천 줄 후회를 유발합니다: 사용자 Jorge Castillo는 Claude가 3,000줄의 코드를 생성한 후에야 초기 AI 프롬프트에서 실수를 깨달은 후 좌절감을 표현합니다 ([출처](https://x.com/jorge_castillo/status/2026396297540858360?s=12)).
  사용자들은 그것이 매우 공감된다고 생각했습니다.
- Anthropic 블로그 게시물에 대한 반응 유머를 자아냅니다: 사용자 @andyreed는 2026년 2월 24일 Anthropic에서 새로 게시된 블로그 게시물에 대한 짧고 유머러스한 반응을 공유합니다 ([출처](https://x.com/andyreed/status/2026396297540858360?s=12)).

---

### Latent Space ▷ #stocks-crypto-macro-economics (5 messages):
> AMD 지분 리베이트 전략, AI가 소프트웨어 개발자에게 미치는 영향, OpenAI 워런트, Meta 워런트
- AI가 아직 개발자들을 죽이지 않을까요?: 한 멤버는 AI가 소프트웨어 개발자의 필요성을 없앨 것인지 묻는 트윗을 링크했습니다 ([https://x.com/ai/status/2026396297540858360?s=12](https://x.com/ai/status/2026396297540858360?s=12)).
- AMD의 OpenAI 및 Meta와의 지분 리베이트: 대규모 거래 분석에 따르면 OpenAI와 Meta가 총 1억 6천만 AMD 주식에 대한 워런트를 보유하고 있으며, 이는 지분 리베이트 역할을 합니다.
  이 거래는 주당 $600 목표 가격과 상당한 미래 GPU 지출과 연관되어 있으며, 잠재적으로 워런트 가치를 $192억으로 평가할 수 있습니다 ([https://xcancel.com/ai/status/2026396297540858360?s=12](https://xcancel.com/ai/status/2026396297540858360?s=12)).

---

### Latent Space ▷ #intro-yourself-pls (2 messages):
> LLM 시스템 디버깅, 기계 공학 분야의 ML/AI
- LLM 시스템 디버깅: 항상 모델의 문제는 아닙니다: 한 멤버는 데모 후 LLM 기능이 실패할 때, 문제는 종종 리트리벌 로직, 토큰 소모, 오케스트레이션 또는 백엔드 아키텍처에서 비롯되며, 모델 자체보다는 그렇다고 강조합니다.
  그들은 출시를 위해 복잡한 LLM 시스템을 안정화하는 데 전문화되어 있으며, 이는 이론적인 모델 개선보다는 실용적이고 실제적인 애플리케이션에 중점을 둔다는 것을 나타냅니다.
- 기계 공학 분야의 ML/AI 관심: 기계/재료 공학 배경을 가진 새로운 산호세 거주자가 자신의 분야에서 ML/AI 적용에 관심을 가지고 있습니다.
  그들은 이 교차점을 더 탐색하기 위해 자원과 연결을 찾고 있으며, 기계 공학 또는 재료 과학 분야의 ML/AI에 관심을 표명하며 현실에서 사람들을 만나기를 기대하고 있습니다.

---

### Latent Space ▷ #tech-discussion-non-ai (74 messages🔥🔥):
> Cloudflare의 Vinext 프레임워크, 트래픽 인식 사전 렌더링, TanStack Start RSC 지원, 오픈 스펙 대 오픈소스, tldraw 라이선싱
- Vinext 배포 문제를 해결하기 위해 여기에 있습니다: Cloudflare는 Next.js의 대안인 Vinext를 소개했습니다. 이는 특히 대규모 사이트의 긴 빌드 시간과 같은 배포 문제를 해결하도록 설계되었으며, 이 블로그 게시물에 자세히 설명되어 있습니다.
  Vinext는 트래픽 패턴을 분석하여 가장 자주 방문하는 페이지만 사전 렌더링하는 트래픽 인식 사전 렌더링(TPR)을 구현하며, 빌드 시간을 크게 줄일 것을 제안하고 다른 프레임워크에도 좋은 기능이 될 수 있습니다.
- 테스트는 새로운 해자가 될 수 있습니다: 한 멤버는 "Tests are the New Moat"라는 블로그 게시물을 발행하고 Chat SDK Template 및 Vercel의 새로운 Chat SDK Library를 링크했습니다.
  잘 명시된 테스트의 아이디어는 높이 평가되지만, AI 모델의 미묘한 불일치나 환각(hallucination)을 완전히 방지하지 못할 수 있다고 언급되었습니다.
- 오픈 스펙 대 오픈소스에 대한 논쟁이 뜨거워집니다: "Open Spec vs. Source Code"라는 트윗이 공유되었는데, 이는 오픈 스펙이 소스 코드보다 더 중요할 수 있다고 논의하며, 소스 코드가 주로 VM 및 컴파일러를 위한 중간 표현으로 기능한다고 제안합니다.
  Vinext의 저자는 "Open Source Privacy for Test Suites"라는 트윗을 뻔뻔하게 올리며, SQLite와 같은 프로젝트가 내부 테스트 스위트를 비공개로 유지하는 미래를 예측했습니다.
- tldraw 라이선싱: 멤버들은 tldraw 라이선스와 기여자 라이선스 계약을 분석했습니다.
  합의는 라이선스가 비독점적인 저작권/특허 부여를 요구한다는 것이었습니다.
- TanStack Start는 완전히 RSC는 아닙니다: 멤버들은 TanStack Start의 RSC (React Server Components) 접근 방식에 대해 논의하며, 이는 로더 내부의 서버 함수를 사용하여 JSX를 반환하는 방식으로 표준 구현과 상당히 다른 것으로 보인다고 언급했습니다.
  이 접근 방식은 서버 우선 접근 방식과 적절한 구성과 같은 주요 이점을 잃는 것으로 보이지만, 현재 API가 최종 버전이 아닐 수도 있다고 추측되었습니다.

---

### Latent Space ▷ #san-francisco-sf (2 messages):
> OpenClaw 워크숍, 임베더블 웹 에이전트 출시
- OpenClaw 핸즈온 워크숍 예정되었습니다: 한 멤버가 다음 주 목요일 팔로알토에서 핸즈온 OpenClaw 워크숍을 발표했습니다. [여기](https://lu.ma/openclaw-workshop)에서 신청하세요.
  만약 당신이 도시에 있다면, 꼭 들러주세요!
- 첫 임베더블 웹 에이전트 출시 파티: 첫 임베더블 웹 에이전트 출시 파티가 발표되었습니다. 자세한 정보는 [여기](https://lu.ma/embeddable-web-agent-launch)에서 확인할 수 있습니다.
  새로운 임베더블 웹 에이전트를 가장 먼저 보러 들러주세요.

---

### Latent Space ▷ #ai-announcements (1 messages):
swyxio: https://youtu.be/x9rWFiIubmc
claude 코드 기념일을 위한 새로운 pod!

---

### Latent Space ▷ #ai-general-news-n-chat (63 messages🔥🔥):
> GPT-5.3-Codex 출시, Mercury 2 추론 디퓨전 LLM, Cognition Devin 2.2, Cursor AI 비디오 데모, 자율적 독푸딩
- GPT-5.3-Codex 개발자들을 위해 출시되었습니다!: OpenAI 개발자들은 Responses API를 통해 모든 개발자를 위한 GPT-5.3-Codex의 즉각적인 사용 가능성을 발표하며, 새로운 모델로 구축을 시작하도록 초대했습니다 ([발표 링크](https://x.com/openai/status/2026396297540858360?s=12)).
- Mercury 2: 추론 디퓨전 모델 출시되었습니다!: Stefano Ermon은 Mercury 2의 출시를 발표합니다. 이는 기존의 속도 최적화된 언어 모델보다 5배 빠르다고 주장하는 추론 디퓨전 LLM입니다 ([발표 링크](https://x.com/stefano_ermon/status/2026396297540858360?s=12)).
- Cognition의 Devin 2.2 업그레이드되었습니다!: Cognition은 업그레이드된 자율 AI 에이전트인 Devin 2.2를 출시했습니다. 이제 컴퓨터 사용 기능, 자체 검증 및 자동 수정 기능을 제공합니다 ([발표 링크](https://x.com/cognition_labs/status/2026396297540858360?s=12)).
  업데이트에는 3배 더 빠른 시작 속도, 가상 데스크톱을 갖춘 재설계된 UI, 그리고 다양한 UX 개선 사항이 포함되며, 현재 무료 체험판으로 이용 가능합니다.
- Cursor AI 에이전트를 위한 비디오 데모를 소개합니다: Cursor AI는 새로운 기능을 도입했습니다. 이는 AI 에이전트가 간단한 코드 차이(diff) 대신 비디오 데모를 통해 자신의 작업을 시연할 수 있게 하여, 사용자들이 소프트웨어가 작동하는 모습을 볼 수 있게 합니다 ([발표 링크](https://x.com/cursor_ai/status/2026396297540858360?s=12)).
  커뮤니티 멤버들은 Cursor가 자신과 다른 경쟁자들 사이의 격차를 좁히고 있으며, 이제 더 긴 루프와 더 자율적인 작업을 수행하는 것으로 보이지만, 필요할 때 IDE를 여전히 유지한다고 언급하며 "이제 우리가 배관공이 되는 건가요?"라고 물었습니다.
- OpenClaw: AI 자동화를 위한 오픈소스 운영 체제: Matthew Berman은 자신의 회사가 OpenClaw를 핵심 운영 체제로 활용하는 방법을 자세히 설명하며, 이메일 관리, CRM 시스템, 회의 인텔리전스 및 재무 추적에 통합하는 것을 포함합니다 ([발표 링크](https://x.com/matthew_berman/status/2026396297540858360?s=12)).
  해당 스레드는 Anthropic OAuth 취약점 수정, 보안 프로토콜, 멀티 프롬프트 버전 관리, 그리고 50억 토큰 사용량에 걸친 강력한 로깅 인프라를 포함한 특정 기술 솔루션을 강조합니다.

---

### Latent Space ▷ #llm-paper-club (33 messages🔥):
> 미드트레이닝, OpenClaw, 프론티어 모델 학습, AI를 통한 개발자 생산성, METR
- 미드트레이닝 마법: 타이밍이 전부입니다!: Emmy Liu의 새로운 사전 인쇄본은 '미드트레이닝'을 탐구하며, 이는 사전 학습(pretraining)과 사후 학습(posttraining) 사이의 다리 역할을 하여 망각을 완화하는 데 가장 효과적임을 보여주지만, 그 성공은 정확한 타이밍에 달려 있습니다.
  이 연구는 통제된 실험을 사용하여 미드트레이닝이 AI 파이프라인에 미치는 영향을 입증합니다.
- OpenClaw의 초기 모험: Natalie Shapira는 @openclaw 프로젝트와의 다학제적 협업에서 얻은 초기 경험과 발견을 공유했습니다.
  프로젝트입니다!
- 프론티어 학습은 시스템을 선호합니다: Logan Thorneloe는 프론티어 모델 학습에 대한 자료를 공유하며, 성공이 사소한 알고리즘 조정보다는 시스템 문제(데이터 혼합, 아키텍처, 안정성)에 더 많이 달려 있음을 강조했습니다. 이 가이드는 학습 플레이북, 옵티마이저, 강화 학습 및 안전성을 다룹니다.
  가이드는 [여기](https://x.com/logan_thorneloe/status/2026396297540858360?s=12)에서 액세스할 수 있습니다.
- 지루한 'AI 없음' 그룹에서 개발자 이탈!: METR은 개발자들이 'AI 없음' 대조군을 점점 더 거부하고 있으며, 이를 비효율적이거나 매력적이지 않다고 간주한다고 발견했습니다. 특히 낮은 급여($50/hr 대 원래 $150/hr)에서 그러했습니다.

이러한 행동 변화는 AI가 워크플로우에 필수적인 요소가 되었음을 시사하며, 전통적인 RCT를 실행하기 더 어렵게 만듭니다. METR은 벤치마크가 따라잡을 수 있는 속도보다 AI가 빠르게 진화하기 때문에 관찰 데이터, 에이전틱 도구 및 더 나은 규정 준수 측정치를 통합하기 위해 실험을 재설계하고 있습니다 (METR의 트윗 링크).
- METR의 측정 방식 전환!: METR(이전 METR_Evals)은 AI 지원 개발자 생산성에서 20% 둔화가 발생했다는 이전 발견이 시대에 뒤떨어졌다고 보고했습니다.

현재 데이터는 속도 향상이 가능함을 시사하지만, 개발자 행동의 최근 변화로 인해 새로운 결과가 신뢰할 수 없게 되었으며, 해당 조직은 더 정확한 평가를 위해 노력하고 있습니다 (METR의 트윗 링크).

---

### Latent Space ▷ #ai-in-action-builders-techstacks-tips-coding-productivity (23 messages🔥):
> API 500 오류, Anthropic 서비스 중단, DSPy로 튜닝된 다중 레이블 분류기, surf-cli 및 Chromium 샌드박싱
- API 500 오류로 사용자 불편: 사용자들은 메시지 {“type”:“error”,“error”:{“type”:“api_error”,“message”:“Internal server error”}와 함께 API Error 500을 자주 수신했다고 보고했습니다.

다른 사용자들은 Anthropic이 여러 모델에서 높은 오류율로 인해 서비스가 중단되었다고 지적했습니다.
- Anthropic 모델, 높은 오류율 겪어: 여러 모델에서 높은 오류율로 인해 한 사용자는 일시적으로 Codex로 전환했습니다.

사용자는 Claude의 응답 방식이 Codex보다 훨씬 더 좋다고 언급했는데, Codex는 기술적으로 밀도 높은 출력을 생성했습니다.
- DSPy, 다중 레이블 분류기를 튜닝하여 콘텐츠 조정: 한 멤버는 DSPy로 튜닝된 다중 레이블 분류기를 파이프라인과 함께 사용하여 새로운 테스트 케이스를 계속 수집하고 Haiku 모델을 사용하여 이를 학습/테스트 샘플로 변환합니다.

이 멤버는 메인 작업을 처리하는 것과 병렬로 이를 실행하고, 질문이 범위를 벗어나면 진행 중인 작업을 취소하여 레이턴시를 절약한다고 덧붙였습니다.
- Surf-CLI 및 Chromium 샌드박싱, 난관에 봉착: 한 멤버는 surf-cli 작업으로 돌아와 Snap을 통해 샌드박싱된 Chromium을 다루는 것이 쉽지 않다고 언급했습니다.

다른 멤버는 진행 상황을 보여주는 Gist를 링크하고, 샌드박스 내의 노드가 까다롭기 때문에 Go 포트를 고려 중이라고 언급했습니다.

---

### Latent Space ▷ #share-your-work (3 messages):
> InstantClaw, OpenClaw 배포, Codaph CLI, Mubit, 하이퍼벡터 및 클러스터링
- InstantClaw, OpenClaw 배포 간소화: 한 사용자가 비기술 사용자를 위한 OpenClaw 배포 도구인 InstantClaw를 공유했습니다. 이 도구를 통해 서버 구성 없이 1분 이내에 OpenClaw 기능을 이용할 수 있습니다.

이 도구와 관련이 없는 사용자는 친구들에게 동일한 기능을 제공하면서 배포 지원 시간을 절약하는 데 유용하다고 평가했습니다.
- Codaph CLI, Codex 프롬프트 동기화: 한 멤버가 Codex 프롬프트, 에이전트 추론 및 파일 diff를 공유 메모리에 동기화하여 팀 간에 더 풍부한 코드베이스 이해를 목표로 하는 CLI 도구인 Codaph를 소개했습니다.

하이퍼벡터와 클러스터링을 사용한 연관 리트리벌 기반의 메모리 엔진인 Mubit(mubit.ai) 위에 구축된 Codaph는 오픈소스이며 현재 Codex와 작동하며, 다른 에이전틱 도구를 지원할 계획입니다.
- Mubit 메모리 엔진, 하이퍼벡터 활용: Codaph의 기반이 되는 Mubit 메모리 엔진은 시간 기반 감쇠를 통해 하이퍼벡터와 클러스터링을 활용하여 연관 리트리벌을 수행합니다.

무료로 사용할 수 있으며, API 키는 console.mubit.ai에서 접근할 수 있습니다.
- 일반화 형성으로서의 도구 사용 및 표기법: 한 멤버가 프롬프팅에 대한 통찰력을 공유하며, 일반화 형성과 관련하여 도구 사용 및 표기법에 대한 토론 링크를 제공했습니다.

일반화 형성으로서의 도구 사용 및 표기법에 대해 더 읽어보세요.

---

### Latent Space ▷ #robotics-and-world-model (8 messages🔥):
> Wayve, SONIC, 자율 주행, 휴머노이드 로봇, AI 라이선싱
- Wayve, 15억 달러 규모 시리즈 D 라운드 성공: Alex Kendall에 따르면, Wayve는 소프트웨어 라이선싱을 통해 ‘Embodied AI’를 상용화하기 위해 15억 달러 규모의 시리즈 D 라운드를 확보했으며, 이로써 회사의 가치는 86억 달러로 평가되었습니다.
- Wayve의 로보택시 로드맵 공개: SoftBank, Microsoft, NVIDIA, Uber와의 파트너십을 바탕으로, Wayve는 2026년부터 10개 도시에서 감독형 로보택시 시험 운행을 시작하고 2027년에는 소비자 차량 판매를 이어갈 계획입니다.
- SONIC: 시스템 1 휴머노이드 제어, 오픈소스로 비상: Jim Fan은 자신의 트윗을 통해 1억 개 이상의 모션 캡처 프레임으로 학습된 42M 파라미터 트랜스포머인 SONIC을 소개했는데, 이는 휴머노이드 로봇에 ‘시스템 1’ 반응형 지능을 제공합니다.
- NVIDIA Isaac Lab, SONIC의 성공 시뮬레이션: NVIDIA Isaac Lab을 사용하여 대규모 병렬 시뮬레이션을 통해 SONIC 모델은 제로샷 실세계 전이를 달성하며 VR, 비디오, 텍스트 및 오디오를 통한 제어를 지원합니다.

---

### Latent Space ▷ #genmedia-creative-ai-video-image-voice-music-inspo-consumer-ai (8 messages🔥):
> Quiver AI, Arrow-1.0 모델, KREA AI, Seedream 5 Lite
- Quiver AI, Arrow-1.0으로 시동 걸어: Quiver AI는 벡터 디자인 AI 연구소로 공식 출범했으며, a16z가 주도하는 시드 펀딩에서 830만 달러를 확보했습니다.

이들의 첫 모델인 Arrow-1.0은 이미지와 텍스트를 SVG로 변환하며, 공개 베타로 제공됩니다.
- KREA AI, Seedream 5 Lite 모델 출시: KREA AI는 저비용 이미지 편집 모델인 Seedream 5 Lite를 출시했습니다.

이 모델은 ‘Nano Banana’ 모델과 유사한 성능을 더 낮은 가격에 제공하도록 설계되었습니다.

---

### Latent Space ▷ #mechinterp-alignment-safety (6 messages):
> 해석 가능성 채용, Anthropic, ML 인프라
- Anthropic, 해석 가능성 팀 대규모 채용 시작: Chris Olah는 이 트윗에서 볼 수 있듯이 Anthropic이 해석 가능성 팀을 위해 약 10명의 연구 엔지니어를 찾고 있다고 발표했습니다.
- ML 인프라 엔지니어 모집: 이 역할은 모델 내부에 관심 있는 숙련된 ML 인프라 엔지니어를 대상으로 하며, 이전 해석 가능성 경험은 필요하지 않습니다.

fxtwitter에 언급된 기회를 포함하여 많은 기회가 있습니다.

---

### Latent Space ▷ #applied-ai-experimentation (43 messages🔥):
> Claude Code 한계, 대안으로서의 Codex, 에이전틱 엔지니어링 전략, Pi 에이전트 루프, 도구 사용 및 표기법
- Claude Code, API 통합 문제 직면: 한 멤버는 Claude Code가 더 큰 작업을 수행하는 데 실망감을 표하며, API가 “완료”되었음에도 불구하고 종종 사양을 완전히 충족하지 못하여 시스템의 여러 계층에서 통합 문제를 일으킨다고 언급했습니다.

그들은 이제 강력한 시스템을 구축하기 위한 대안으로 Codex를 고려하고 있습니다.
- Pi, Codex로 OpenClaw 구동: 한 멤버는 OpenClaw를 구동하는 에이전트 루프인 Pi 내에서 Codex를 사용할 것을 제안하고, Pi의 패키지 링크와 사용자들이 기여할 수 있도록 안내하는 YouTube 비디오를 공유했습니다.

다른 멤버는 LLM이 해당 하네스 내에서 RL되었기 때문에 ‘공식’ 코딩 하네스인 Claude Code와 Codex를 고수하는 것이 가장 좋다고 말했습니다.
- 에이전틱 엔지니어링에 ‘안티-사양’ 접근 방식 선호: 한 멤버는 에이전틱 엔지니어링에서 상세한 사전 사양에 반대하며, 반복, 실패 검증 및 가지치기를 강조하고, 사양은 나중에 구축되어야 한다고 제안했습니다.

그들은 이것이 주로 잘못된 통제감과 자부심에서 비롯된다고 말했습니다.
- 도구 사용 및 표기법, LLM 일반화 형성: 한 멤버가 일반화 형성으로서의 도구 사용 및 표기법에 대한 블로그 게시물을 공유하며 자신의 작업을 홍보했습니다.

다른 멤버는 그것이 자신들이 참여하고 있던 변증법에 완벽하게 들어맞는다고 생각했습니다.
- 와일드 시스템 프롬프팅을 통한 신속한 프로토타이핑: 한 멤버는 golang, watermill, redis와 같은 도구를 사용하여 이벤트 기반 아키텍처에 중점을 두고, 최소한의 프롬프팅과 수동 검토를 통해 혁신적인 API를 가진 복잡한 시스템을 구축하는 접근 방식을 설명했습니다.

그들은 TUI를 yolo 프로토타입과 병합하는 구체적인 예를 공유했으며, 임베딩 및 벡터 검색을 포함한 cozodb 관련 기능과 JS API를 담은 핵심 패키지를 만들었습니다.

---

### Nous Research AI ▷ #announcements (1 messages):
> Hermes Agent, 오픈소스 에이전트, 다단계 메모리 시스템, 영구 전용 머신 액세스, CLI 통합
- Hermes Agent: 오픈소스 에이전트 출시: Nous Research는 다단계 메모리 시스템과 영구 전용 머신 액세스를 갖춘 오픈소스 에이전트인 Hermes Agent를 출시했으며, 이는 사용자와 함께 성장하도록 설계되었습니다.

Hermes Agent는 CLI, Telegram, WhatsApp, Slack, Discord에서 실행될 수 있으며, 어디를 가든 세션을 이어받고 전송할 수 있습니다.
- 고급 기능 및 광범위한 통합으로 Hermes Agent 구동: Hermes Agent는 하위 에이전트 제어, 프로그래밍 방식 도구 호출, 고급 파일 시스템/터미널 제어, 에이전트 관리 기술 및 브라우저 사용과 같은 고급 에이전틱 기능을 제공합니다.

OpenRouter 및 Nous Portal 구독을 통해 구동되며, CLI 통합 및 메시징 플랫폼 지원을 제공합니다.
- 무료 한 달 프로모션 및 개발자 친화적 디자인 공개!: portal.nousresearch.com에서 처음 750명의 신규 가입자는 쿠폰 코드 HERMESAGENT를 사용하여 한 달 무료 혜택을 받습니다.

Hermes Agent는 오픈소스이며 Python으로 구축되어 개발자들이 쉽게 확장할 수 있습니다.
- 에이전틱 RL 파이프라인 및 대규모 데이터 생성 강화: Hermes Agent는 또한 에이전틱 RL 파이프라인을 구동하여 Atropos를 확장하여 Hermes Agent 프리미티브로 RL을 가능하게 하며, 대규모 데이터 생성을 즉시 지원합니다.

GitHub 리포지토리를 확인하거나 터미널에서 한 명령으로 설치하세요: `curl -fsSL https://raw.githubusercontent.com/NousResearch/hermes-agent/main/scripts/install.sh | bash`.

---

### Nous Research AI ▷ #general (96 messages🔥🔥):
> Qwen 베이스 모델 출시, Codex 5.3 API 가격, Steinberger의 OpenClaw 프로세스, OS 프론티어 수학 레벨 4 업데이트, NousChat 개발
- Qwen의 베이스 모델 가중치 출시: Qwen은 Hugging Face에서 Qwen3.5-35B-A3B 모델의 베이스 가중치를 출시했습니다.
- Codex 5.3의 새로운 가격 책정: Codex 5.3이 새로운 가격 구조로 API에 출시되었습니다: 입력에 1.75달러, 출력에 14달러.
- Steinberger의 OpenClaw: 바이브 코딩의 기적: Steinberger는 OpenClaw가 어떻게 만들어졌는지 설명하는 비디오를 공개했는데, 그의 이전 계획, 아이디어 및 코드 스니펫에서 AI를 통해 추출되었고, 이를 AI에 제공하여 새로운 코드를 만들었다고 합니다.

그는 자신의 소프트웨어가 무엇을 하는지 전혀 모르며, 그 구조는 단지 채널들의 스택일 뿐이라고 말했습니다.
- OS 프론티어 수학 레벨 4 업데이트: Kimi 2.5 (1st OS)는 4.2%를 기록했으며, Glm 5와 V3.2는 2.1%를 기록했습니다.
- NousChat, Kimiclaw와 정렬 진행 중: 한 멤버가 Kimiclaw와 같은 서비스를 호스팅할 계획에 대해 문의했고, 다른 멤버는 NousChat이 그와 '정렬'되는 방식으로 진행 중이라고 답했습니다.

---

### Nous Research AI ▷ #research-papers (2 messages):
> Arxiv 논문
- Arxiv 논문 공유: 한 멤버가 Arxiv 논문 링크를 공유했습니다: https://arxiv.org/abs/2602.16800.
- 흥미로운 발견: 다른 멤버는 흥미로운 논문이라고 답했습니다.

---

### Nous Research AI ▷ #research-papers (2 messages):
> Arxiv 논문
- 새로운 Arxiv 논문 공유: 한 멤버가 새로운 Arxiv 논문 링크를 공유했습니다: https://arxiv.org/abs/2602.16800.
- Arxiv 논문, 관심 끌어: 다른 멤버는 흥미로운 논문이라고 언급했습니다.

---

### Eleuther ▷ #general (49 messages🔥):
> Pythia-2.8b 체크포인트 문제, Hugging Face 가중치 제공, safetensors 및 HF, 중복 제거 모델, 음성 AI 모델 Sesame AI
- Pythia-2.8b 체크포인트 버그, 조사 착수: 한 멤버가 pythia-2.8b 체크포인트를 사용하여 논문을 재현하려던 중 버그를 발견했으며, Hugging Face가 선택된 리비전과 관계없이 동일한 가중치를 제공하고 있음을 알아냈습니다.

pytorch_model.bin 및 model.safetensors의 SHA256 해시가 다른 단계에서도 동일하여 체크포인트의 무결성에 대한 우려를 제기했습니다.
- HF 샤드, 대역폭 절약: 멤버들은 pythia-2.8b의 샤딩된 safetensors 파일은 단계별로 다르지만, 샤딩되지 않은 파일은 동일하다는 것을 발견했으며, 이는 HF가 모델을 로드하고 샤딩을 처리하는 방식에 대한 논의로 이어졌습니다.

한 멤버는 UltraChat과 기본 Mistral 간의 차이를 Mistral-Yarn에 적용하는 것을 잠재적인 병합 전술로 제안했습니다.
- 더 작은 Pythia 모델, 중복 제거됨: EleutherAI는 잘못 레이블링된 14m 및 30m 모델(모두 중복 제거된 버전)을 수정하고 있으며, 이를 대체할 중복된 모델을 학습시켜 레이블링 문제를 명확히 하고 있습니다.

한 멤버는 일부 업로드 혼동 문제를 해결하고 밤새도록 수정 작업을 실행했다고 언급했습니다.
- 멤버, HF가 심볼릭 링크 속임수로 디스크 공간 절약했다고 추측: 한 멤버는 과거에 비슷한 실수를 저질렀다는 것을 깨달은 후, Hugging Face가 디스크 공간을 절약하기 위해 심볼릭 링크를 사용했을 수 있으며, 이는 데이터 손상을 초래할 수 있다고 추측했습니다.

이 이론은 pythia-2.8b 체크포인트 문제가 HF의 내부 저장소 관리 프로세스 때문일 수 있음을 시사합니다.
- Sesame AI 음성 모델, 호기심 자극: 한 멤버가 Sesame AI 음성 AI 모델에 대해 질문하며, 명백한 정렬과 Gemma 모델 기반 가능성을 언급했고, 이는 모델의 기능에 대한 논의를 촉발했습니다.

다른 멤버는 Sesame AI가 ASR, LLM, TTS를 통합한 저 레이턴시 음성 시스템에 중점을 둔다는 점을 강조하며, 통찰력을 얻기 위해 Moshi 논문을 검토할 것을 제안했습니다.

---

### Eleuther ▷ #research (22 messages🔥):
> 확산 논문, Flow Matching 대 확산, Pythia 모델
- 확산 문헌 심층 분석: 멤버들은 Latent Diffusion Model 이후의 주요 확산 논문들을 논의하며, Rectified Flows와 Flow Matching 및 Diffusion Forcing을 강조했습니다.

또한 ByteDance Seed와 Hunyuan의 논문들(예: https://arxiv.org/abs/2509.20427, https://arxiv.org/abs/2509.23951)이 언급되었으며, 자료로 YouTube 재생 목록이 추천되었습니다.
- Flow Matching의 유동적 기반: 논의에서 Flow Matching이 시간적으로 연속적이고 가역성을 요구하지 않으므로, Flow Matching이 이전 작업(https://arxiv.org/abs/1807.03039)과 관련이 있지만 구별된다는 점이 명확해졌습니다.

한 멤버는 Flow Matching이 확산 연구에서 더 많이 나왔으며, 흐름을 매개변수화하는 대체 방식을 나타낸다고 언급했습니다.
- Louie의 잠재 링크 물류: 한 멤버가 확산 파이프라인의 잠재 부분에 관한 논문 링크가 포함된 블로그 게시물을 언급했습니다: https://over.world/blog/dito.

언급된 논문에는 https://arxiv.org/abs/2512.12386이 포함되었으며, https://arxiv.org/pdf/2510.11690과 같은 다른 논문들을 참조하고 Token Routing, Path-Drop Guidance, 잠재 임베딩의 Representation Alignment와 같은 새로운 방법들을 포함합니다.
- Pythia에 대하여!: Pythia 모델에 대한 링크가 공유되었습니다.

---

### Eleuther ▷ #lm-thunderdome (3 messages):
> lm-evaluation-harness, MMLU pro eval, Qwen3 모델, HF 백엔드, vLLM 백엔드
- lm-eval Harness를 위한 빠른 vLLM 백엔드: 한 멤버가 lm-evaluation-harness에서 vLLM 백엔드를 사용하여 단일 토큰 답변을 가진 다중 선택 작업의 평가 속도를 높이는 것을 목표로 하는 풀 리퀘스트에 대한 검토를 요청했습니다.

이 속도 향상은 특히 MMLU pro eval과 같은 작업에서 HF 백엔드에 비해 느린 문제를 해결해야 합니다.
- Qwen3 모델의 개행 문자: 한 멤버가 lm-evaluation-harness에서 Qwen3 모델의 예상치 못한 개행 문자 동작에 대해 문의했는데, 여기서 \n\n이 연속으로 이동하며, 잠재적으로 관련될 수 있는 이슈 2144를 링크했습니다.

사용자는 로그 출력에서 컨텍스트와 연속을 포함한 예를 제시했습니다.

---

### HuggingFace ▷ #general (40 messages🔥):
> ZeroGPU 할당 문제, 소형 언어 모델, 엣지 인퍼런스 메모리, 코드 RAG, Tiny Aya
- Gradio 버전, ZeroGPU 할당 문제 야기: 사용자들은 ZeroGPU 할당 문제를 보고했는데, 이는 Gradio 5.12.0 이전 버전의 로그인 버그와 관련이 있을 수 있습니다.

컨테이너 로그를 확인하면 Gradio, spaces 라이브러리 또는 HF 서버가 문제의 원인인지 밝혀낼 수 있으며, 빈 커밋 후 재구축하는 것도 버전 관련 문제를 해결할 수 있습니다.
- Cohere, Tiny Aya 출시: Cohere는 최근 Tiny Aya를 출시했습니다.
- 독립 개발자, 엄청난 엣지 메모리 장벽 돌파: 한 독립 개발자는 MiniMax-m2.5의 5GB MoE 샤드를 2MB 벡터 양자화 잠재 공간으로 압축했다고 주장합니다.

그들은 arXiv(cs.LG)에 제출할 논문을 준비 중이며, 자신들의 “블랙 매직 엣지 AI 기술”을 검토해 줄 추천인을 찾고 있습니다.
- 코드 RAG 발명, 프로젝트 확장 위해 등장: 한 멤버는 프로젝트를 확장하기 위해 코드 RAG를 발명하고 있으며, 이미 “절반 정도 완성했다”고 주장합니다.

그들은 코드가 다른 코드와 어떻게 관련되어 있는지 보여주는 그래프를 공유했습니다.
- 디스틸레이션 학습의 어려움: 한 멤버는 자신의 “학생 모델이 교사 모델처럼 생각하지 않기” 때문에 디스틸레이션 학습에 유용한 지침을 요청하고 있습니다.

그들은 “자신을 학습시키는 것은 LLM을 학습시키는 것과 매우 다르다”고 말했습니다.

---

### HuggingFace ▷ #i-made-this (11 messages🔥):
> 분산 파인튜닝, GPT-OSS, Qwen 2.5, Mistral 모델, RTH-LM 모델
- Zagora, 분산 파인튜닝 시스템 구축: Zagora의 한 멤버는 표준 인터넷을 통해 70B+ 모델을 학습시키기 위한 분산 파인튜닝 시스템을 구축 중이며, 분산된 GPU를 통합 학습 슈퍼컴퓨터로 전환하고 있다고 발표했습니다.

이 플랫폼은 현재 GPT-OSS, Qwen 2.5, Mistral을 지원하며 Petals 및 SWARM Protocol에서 영감을 받은 파이프라인 스타일 학습 접근 방식을 사용합니다.
- RTH-LM, Zagora 시스템 스트레스 테스트 가능: 한 멤버는 자신들의 RTH-LM 모델(비 트랜스포머 모델인 Fractal Gated Causal TCN)이 파이프라인 단계에서 노드 간 상태 동기화 오버헤드가 없기 때문에 Zagora 시스템에 완벽한 스트레스 테스트가 될 수 있다고 제안했습니다.

그들은 120B 규모를 목표로 하며, 플랫폼이 트랜스포머 계열 외에 사용자 정의 모델 아키텍처(모든 nn.Module)를 지원하는지 물었고, 자신들의 논문, 리포지토리 및 25B 모델을 언급했습니다.
- Zagora, 트랜스포머 모델에 집중: Zagora 팀은 현재 Llama, Qwen, Mixtral, Gemma와 같은 트랜스포머 계열 모델에 집중하고 있다고 답했습니다.

하지만, RTH-LM이 트랜스포머 호환 래퍼를 얻게 된다면 통합 가능성을 재검토할 것이라고 언급했습니다.
- webXOS 블랙홀 타임랩스 데이터셋 출시: 한 멤버가 webxOS의 Three.js 시뮬레이션으로 생성된 중력 렌즈 효과가 있는 합성 블랙홀 렌더링을 포함하는 webXOS 블랙홀 타임랩스 데이터셋을 공유했습니다.

각 샘플에는 PNG 이미지의 타임랩스 시퀀스와 관련 물리적 매개변수가 포함되어 있어 다중 모달 모델 학습, 물리학에서 영감을 받은 ML 또는 위성 이미지 연구 비유에 이상적입니다.
- 가장 안전한 곳에서 모델 최적화: 한 멤버가 ‘가장 안전한 곳에서 최적화: 모델 우선 접근 방식’이라는 기사를 발표했는데, 이 기사는 런타임을 변경하거나 새로운 인퍼런스 제공업체에 의존하지 않고 모델을 최적화하기 위해 적용할 수 있는 다양한 유형의 방법과 관찰할 수 있는 결과를 설명합니다.

---

### HuggingFace ▷ #agents-course (6 messages):
> 에이전트 코스, smolagents, Qwen API, huggingface/agents-course
- 에이전트 코스 채널 찾기: Hugging Face 에이전트 코스에 새로 온 사람들은 코스 자료에 언급된 특정 채널을 찾는 데 어려움을 겪고 있습니다.

한 멤버에 따르면, 채널들이 단일 채널로 병합된 것으로 보이며, agents-course 리포지토리의 PR #653을 링크했습니다.
- Smolagents 퀴즈 문제: 한 멤버가 smolagents 최종 퀴즈 코드가 평가되는 것을 막는 경고, 특히 API 오류를 겪고 있습니다.

오류 메시지는 https://api-inference.huggingface.co가 더 이상 지원되지 않으며, 대신 https://router.huggingface.co를 사용할 것을 제안하는데, 이는 Qwen API와 관련이 있습니다.

---

### GPU MODE ▷ #general (15 messages🔥):
> 커널 프로그래밍 환경 설정, GPU, TPU 또는 FPGA의 소프트 GPU를 사용하는 HPC 시스템, 가속기 성능 모델링, GraphCulon
- GPUmode.com, 유지보수 위해 다운: GPUmode.com이 유지보수를 위해 다운되었지만, 곧 다시 복구되었습니다.
- 사용자들, 커널 프로그래밍 환경 설정 논의: 한 멤버가 다른 사람들의 선호하는 커널 프로그래밍 환경 설정에 대해 물었고, Modal이 유용하지만 대회 외부에서는 NCU 프로파일링이 부족하다고 언급했습니다.
- Calculon 도구, 시스템의 고수준 공동 설계 가능하게 해: 한 멤버가 시스템의 고수준 공동 설계를 위한 방법론이자 도구인 Calculon 링크를 공유했습니다.
- GraphCulon, 흥미로워 보여: 한 멤버는 GraphCulon이 실제로 흥미로워 보이지만 아직 출시되지 않았다고 언급하며, 이에 대한 강연 링크를 제공했습니다.
- GPU 관측 가능성 세미나 시작: GPU 관측 가능성 세미나가 시작되었으며, 발표자가 슬라이드를 공유할 것을 약속했습니다.

---

### GPU MODE ▷ #cuda (1 messages):
> cuda::memcpy_async, SMEM 뱅크 충돌
- cuda::memcpy_async 사용 시 SMEM 뱅크 충돌: 한 사용자가 GMEM에서 SMEM으로 데이터를 전송하기 위해 cuda::memcpy_async를 사용할 때 SMEM 뱅크 충돌이 중요한 문제인지 문의했습니다.

사용자는 SMEM 뱅크 충돌이 주로 SMEM에 대한 워프 액세스와 관련이 있으므로 이 시나리오에서는 큰 문제가 아닐 수 있다고 가정했지만, 추가적인 관점을 구했습니다.
- GMEM에서 SMEM으로의 전송 고려 사항: 논의는 CUDA 내에서 메모리 전송 전략을 최적화하는 것, 특히 cuda::memcpy_async 사용에 초점을 맞춥니다.

핵심 질문은 메모리 복사의 비동기적 특성이 SMEM 뱅크 충돌 가능성에 영향을 미치는지 여부이며, 이는 신중한 고려를 필요로 합니다.

---

### GPU MODE ▷ #torch (3 messages):
> FA3 커널, SDPA 백엔드 선택, Blackwell GPU
- PyTorch 디스패치에서 FA3 커널이 FA2 대체: 사용자가 activate_flash_attention_impl(“FA3”)를 호출하면, restore_flash_attention_impl이 호출되어 기본 FA2 커널이 복원될 때까지 디스패치 테이블의 기본 FA2 커널이 FA3 커널로 재정의됩니다.

이는 버전 이름을 호출 가능한 함수에 매핑하는 딕셔너리에 키-값 쌍 {“FA3”, register_fn}을 추가하고, register_fn(여기서 정의됨)을 실행하여 PyTorch 디스패처에 FA3 커널을 등록함으로써 달성됩니다.
- SDPA, GPU 장치 기반으로 FA 백엔드 선택: SDPA에서 Flash Attention(FA) 백엔드 선택은 GPU 장치에 따라 달라지며, select_sdp_backend 함수(여기서 정의됨)를 사용하여 SDP 백엔드의 우선순위를 선택합니다.

기본 순서는 flash, mem efficient, 그 다음 math이지만, 사용자는 특정 백엔드를 활성화하기 위해 이를 재정의할 수 있습니다. 예를 들어, Blackwell GPU의 경우 flash attention이 작동하지 않으므로, check_prefer_cudnn_attention의 이 줄에 의해 결정되는 첫 번째 우선순위는 cuDNN입니다.

---

### GPU MODE ▷ #announcements (1 messages):
> eBPF, GPU, 프로파일러, OS 정책
- eBPF, GPU 부스트 얻어: Yusheng Zheng은 [날짜] PST 오후 12시에 eBPF를 GPU와 더 잘 작동하도록 확장하는 것에 대해 논의할 예정입니다.

이 강연에서는 gpu_ext: eBPF를 통한 GPU용 확장 가능한 OS 정책(논문) 및 eBPF를 GPU 장치 및 드라이버 컨텍스트로 확장(LPC 이벤트)과 같은 최근 작업을 다룰 것입니다.
- GPU Mode 프로파일러 파티에 참여하세요: 발표자는 GPU MODE 내에서 더 많은 프로파일러와 프로파일러 시각화 라이브러리가 개발되기를 희망한다고 밝혔습니다.

관심 있는 사람들은 참여하여 영감을 얻기 위해 관련 YouTube 비디오를 시청하는 것이 좋습니다.

---

### GPU MODE ▷ #beginner (10 messages🔥):
> CUDA 학습 자료, GB200 / B200 노드의 분산 인퍼런스 플랫폼, GPU 분야로의 경력 전환, 대량 질문을 위한 새 채널 제안
- CUDA 초보자, 커널 수준 지식 탐색: CUDA 커널 수준 작업에 새로 입문한 한 멤버는 Dynamo, vLLM, LMCache, NIXL과 같은 오픈소스 프로젝트를 사용하여 GB200 / B200 노드의 분산 인퍼런스 플랫폼을 효과적으로 학습하는 방법에 대한 조언을 구하고 있습니다.

이 멤버는 PMPP로 시작하거나, GPU MODE 대회에 참여하거나, NVIDIA의 CUDA / 성능 강좌를 수강하는 것이 얼마나 도움이 되는지 구체적으로 물었으며, 장기적인 목표는 인퍼런스 오픈소스에 기여하는 것입니다.
- CUDA 초보자에게 PMPP 및 오픈소스 해킹 추천: 한 멤버는 이전 논의를 참조하고 PMPP 1장부터 6장까지 읽은 다음 오픈소스 프로젝트에 기여하여 CUDA를 효과적으로 학습할 것을 권장했습니다.

그들은 재미를 위해 대회에 참여할 것을 장려했습니다.
- GPU 분야 경력 전환 가이드 요청: 컴퓨터 공학 학위를 가지고 소프트웨어 엔지니어로 일하는 한 멤버가 GPU 분야로 이동하여 그 분야에서 경력을 쌓는 데 관심을 표명했습니다.

그들은 CUDA와 GPU 프로파일링으로 시작하는 것이 올바른 방향인지 물었고, 이 경로에 접근하는 방법에 대한 지침을 요청했으며, 다른 멤버도 이 요청에 동의했습니다.
- 초보자 질문 쇄도 채널 제안: 한 멤버가 PMPP, NCCL 코드베이스 이해 또는 개인 커널 작업과 같이 CUDA 학습과 관련된 특정 질문에 대한 대량 논의를 위한 #newb_firehose라는 채널을 제안했습니다.

다른 멤버는 기존 #beginner 채널이 이 목적을 수행하며, 사용자에게 그곳에서 자유롭게 질문할 것을 권장했습니다.

### GPU MODE ▷ #popcorn (6 messages):
> 핵키 제출물 파싱, KernelBot 환경 개선, 커널 최적화를 위한 RL 환경
- 핵키 제출물 파싱 시작: 첨부된 이미지에서 볼 수 있듯이, 핵키 제출물 파싱이 시작되어 핑거프린팅 및 심층 분석을 개시하고 있습니다.
- KernelBot 환경 증강에 대한 질문: 한 멤버가 PrimeIntellect를 통해 KernelBot 환경에 새로운 제출물을 추가할지 여부를 문의했습니다.

다른 멤버는 룰셋이 검토 후 승인되면 KernelBot에 검증 레이어로 추가될 수 있다고 제안했습니다.
- 커널 최적화 RL 환경에 대한 관심 표명: 한 멤버가 커널 최적화를 위한 RL 환경에 관심을 표명하며 공통 인프라 구축을 제안했습니다.

주어진 메시지에서는 추가적인 세부 사항이나 구체적인 논의가 강조되지 않았습니다.

---

### GPU MODE ▷ #thunderkittens (1 messages):
simran9493: 네!

---

### GPU MODE ▷ #status (1 messages):
> CLI 업데이트, 인증 문제
- CLI 업데이트: 멤버들은 CLI를 최신 버전으로 업데이트하도록 지시받았습니다.

이 업데이트에는 기능 향상을 위한 버그 수정 및 새로운 기능이 포함될 것으로 예상됩니다.
- 인증 문제 보고: 멤버들은 인증 관련 문제가 발생하면 보고하도록 요청받았습니다.

이러한 사전 예방적 접근 방식은 원활한 접근을 보장하고 중단을 방지합니다.

---

### GPU MODE ▷ #hardware (1 messages):
> B200 GPU, GPU 리싱, Neocloud 솔루션, Lightning AI Clusters
- B200 GPU 가격 충격으로 리싱 권장: 한 사용자는 B200 GPU가 엄청나게 비싸다고 언급하며, 비 엔터프라이즈 사용자에게는 리싱 또는 렌팅이 더 실행 가능한 옵션이라고 조언했습니다.

그들은 자사의 솔루션인 Lightning AI Clusters를 잠재적으로 매력적인 대안으로 강조했습니다.
- Neocloud 리싱이 B200 대안으로 부상: B200 GPU의 높은 비용을 고려할 때, 한 사용자는 특히 엔터프라이즈 환경 외부의 사용자들을 위해 Neocloud 리싱 또는 렌팅 옵션을 탐색할 것을 제안합니다.

사용자는 대안을 찾는 사람들을 위해 Lightning AI의 클러스터 솔루션을 특별히 추천합니다.

---

### GPU MODE ▷ #cutlass (4 messages):
> CuTeDSL 편집 가능한 패키지 설치, CuTeDSL 4.4 릴리스의 호환성 파괴 변경 사항, CuTeDSL의 벡터화된 타일드 카피 2D, CuTeDSL의 스레드 값 레이아웃
- CuTeDSL 편집 가능한 패키지 설치 지침 요청: 한 사용자가 CuTeDSL의 편집 가능한 패키지 설치 가이드를 요청하며, 기존 스크립트가 이해하기 어렵다고 언급했습니다.

그들은 최신 4.4 릴리스가 파이썬 패키지를 여러 개의 새로운 패키지로 분할하여 작동하지 않는 것 같다고 언급했습니다.
- 벡터화된 타일드 카피 2D 스레드 레이아웃 선호: 한 사용자는 CuTeDSL에서 벡터화된 타일드 카피 2D를 수행하기 위한 스레드 값 레이아웃(첨부된 이미지에 시각화됨)을 선호한다고 표명하며, 그것이 더 직관적이라고 생각한다고 밝혔습니다.

그들은 quack도 최근 이 레이아웃으로 변경되었으며, shape와 stride를 사용한 이전 레이아웃의 코드 스니펫을 포함했다고 언급했습니다.

---

### GPU MODE ▷ #multi-gpu (2 messages):
> Helion 구현, all_gather + FP8 + GEMM 최적화, 커널 프로파일링 및 디버깅
- Helion 구현이 기준선에 뒤처짐: 한 멤버가 vllm-project 풀 리퀘스트를 기반으로 all_gather + FP8 + GEMM (H100)의 Helion 구현을 작업 중이지만, 현재 기준선보다 1.26–4배 느립니다.

목표는 커널을 최적화하고 프로파일링하여 버블과 대기 상태를 검사하는 것이지만, 크롬을 통한 트레이싱은 따라가기 어렵다는 것이 입증되고 있습니다.
- 커널 최적화 도구에 대한 조언 요청: 한 멤버가 커널 최적화를 위한 도구 또는 워크플로우에 대한 권장 사항과 함께 팁, 문서 또는 공유된 경험을 찾고 있습니다.

그들은 트레이싱으로 프로파일링을 해왔지만, Meta 데이터 센터 엔지니어링을 구현한 후 실제 병목 현상이 어디에 있는지 파악하고 추론하기가 상당히 어렵다고 합니다.

---

### GPU MODE ▷ #helion (4 messages):
> Helion PR 1418, Helion all_gather + FP8 + GEMM 최적화
- Helion의 병렬 읽기 검토: 한 멤버가 Helion PR 1418이 JAX 문서에 설명된 병렬 읽기 문제를 해결하는지 문의했습니다.

PR 작성자는 주말까지 또는 다음 주까지는 응답할 수 없습니다.
- Helion의 FP8 GEMM 최적화: 한 멤버가 이 풀 리퀘스트에서 볼 수 있듯이 all_gather + FP8 + GEMM (H100)의 Helion 구현을 작업 중입니다.

현재 구현은 기준선보다 1.26–4배 느리며, 목표는 커널을 최적화하고 프로파일링 도구 및 워크플로우에 대한 조언을 요청하는 것입니다.
- NCU 통찰력 요청: 한 멤버가 커널 최적화에 대한 실행 가능한 통찰력을 얻기 위해 NCU 사용을 제안했습니다.

NCU 사용에 대한 추가 정보는 제공되지 않았습니다.

---

### GPU MODE ▷ #robotics-vla (1 messages):
vovw: 훌륭한 작업입니다

---

### GPU MODE ▷ #career-advice (5 messages):
> CUDA 커널, TPU 인퍼런스, MLSys 역할, 분산 학습
- 인퍼런스 및 MLSys 역할에서의 CUDA 커널 지식: 한 멤버가 인퍼런스 및 MLSys 역할, 특히 TPU 인퍼런스 경험이 있는 경우 광범위한 CUDA 커널 지식의 필요성에 대해 문의했습니다.

다른 멤버는 학부생으로서 CUDA/커널을 얼마나 알아야 하는지에 대해 비슷한 의구심을 표명하며, 이 분야에 진입하는 사람들 사이의 공통된 우려를 강조했습니다.
- 학습 대 인퍼런스: CUDA 커널의 중요성: 한 멤버는 분산 학습 경험을 공유하며, 암페어 아키텍처를 넘어서는 깊은 CUDA 커널 지식이 항상 필수적인 것은 아니지만 확실히 가치 있다고 언급했습니다.

그들은 특정 연산을 대체하기 위해 커널을 작성하는 것이 유익했을 상황들을 회상하며, 학습과 인퍼런스 모두를 아는 것이 도움이 될 수 있지만 엄격하게 요구되는 것은 아니라고 강조했습니다.

---

### GPU MODE ▷ #from-scratch (1 messages):
> Serenade 언어, C++ 트랜스파일, CUDA 및 x86-64 ASM, GPU 커널, Dear ImGui 지원
- Serenade: "간단한 파이썬, 빠른 C++" 등장: 한 멤버가 C++, CUDA, x86-64 ASM으로 트랜스파일되며, 파이썬만큼 간단하지만 수동 메모리 관리로 C++만큼 빠른 것을 목표로 하는 새로운 언어 Serenade를 소개했습니다.

이 언어는 GPU 커널 지원(serenaCore, 커스텀 BLAS 커널)과 단일 패스 컴파일 시스템을 갖춘 통합 Dear ImGui 지원을 포함하며, 이를 사용하여 운영 체제를 만들 계획입니다.
- Serenade의 목표: 각 언어의 장점 결합: 개발자는 Serenade가 여러 언어의 최고의 측면을 결합한 훌륭한 도구를 만들겠다는 아이디어에 의해 추진되는 단독 이니셔티브임을 강조했습니다.

소스 코드는 현재 비공개이지만, Serenade의 가장 간단한 기능은 Replit에서 브라우저를 통해 테스트할 수 있습니다.

---

### Moonshot AI (Kimi K-2) ▷ #general-chat (15 messages🔥):
> Kimi 대 GLM, 에이전트 할당량, 코딩을 위한 Kimi, KimiClaw 브라우저 탐색
- Kimi 대 GLM 대결: 멤버들은 Kimi와 GLM 5의 성능에 대해 논쟁했으며, 한 사용자는 Kimi가 GLM보다 100,000배 빠르다고 농담했습니다.

다른 사용자는 GLM 5가 약간 우세하지만, 공식 z.AI API를 통해서는 GLM 5가 느리지만 다른 제공업체를 통해서는 더 빠를 수 있다고 언급했습니다.
- 사용자들, 에이전트 할당량 충전을 요청: 한 사용자가 Allegro의 비용 문제를 언급하며 특히 에이전트 할당량 충전에 대해 문의했습니다.

그들은 또한 nb pro가 포함된 에이전트 docsis kimi 슬라이드가 더 이상 무료로 제공되지 않는다고 언급했습니다.
- Kimi, 코딩 작업에서 빛을 발하다: Kimi, MiniMax, Alibaba의 코딩 플랜을 테스트한 후, 한 사용자는 코딩을 위해 Kimi를 고수하기로 선택했습니다.

사용자는 속도, 업타임, 사용 제한 및 모델 품질을 결정 요인으로 언급했습니다.
- KimiClaw의 브라우저 사각지대: 한 사용자는 KimiClaw가 브라우저를 독립적으로 탐색할 수 없는 것에 대한 불만을 보고했습니다.

그들은 다른 사람들도 같은 문제에 직면했는지 묻고 해결책을 찾았으며, "대용량 파일을 분석/처리할 때 컨텍스트를 줄이고 토큰을 절약하기 위해 Kimi에 무엇을 사용할 수 있을까요? Claude는 그런 기능이 있는 것 같습니다."라고 물었습니다.

---

### Manus.im Discord ▷ #general (11 messages🔥):
> Github 재연결 문제, 로컬 개발 환경 설정, 계정 차단, Manus 쿠키 문제, 웹사이트 디자인 문제
- 사용자, Github 재연결 딜레마에 직면: 한 멤버가 Github 계정 재연결에 문제를 겪고 있으며, 원래 리포지토리에 재연결하는 대신 새 리포지토리를 생성하라는 메시지가 표시되고 있습니다.

이 멤버는 자신이 코더나 소프트웨어 개발자가 아니며 쉽게 이해할 수 있는 지침이 필요하다고 말합니다.
- 로컬 개발자를 위한 OAuth 환경 변수 조사: 한 멤버가 Manus가 개발한 앱을 로컬에서 실행하기 위해 VITE_APP_ID, OAUTH_SERVER_URL, VITE_OAUTH_PORTAL_URL 환경 변수를 얻는 방법에 대한 지침을 찾고 있습니다.

그들은 또한 로컬 개발 중에 redirectUri http://localhost:3000/api/oauth/callback을 허용하기 위해 OAuth 구성이 필요한지 문의하고 있습니다.
- 계정 생성 차단으로 사용자 당황: 한 멤버가 계정 생성 직후 즉시 차단되었다고 보고하며, 이 문제를 해결하는 방법에 대한 조언을 구하고 있습니다.

조언은 주어지지 않았습니다.
- Manus 시스템, 쿠키 문제의 원인을 인프라 탓으로 돌리다: 한 멤버가 커스텀 도메인(anointedforai.com)에서 쿠키 문제로 인해 Manus가 리다이렉트 루프에 갇히는 상세한 문제를 공유했습니다.

Manus 자체는 이 문제를 인프라/호스팅 문제로 진단했으며, 지원팀에 문의하거나 쿠키 설정에 대한 더 많은 제어 권한을 가진 플랫폼으로 Manus에서 마이그레이션할 것을 제안했습니다.
- 멤버, Manus가 만든 웹사이트에 대해 불평: 한 멤버가 자신의 웹사이트 디자인에 대해 불평하며, Manus가 만들어서 엉망진창이었다고 말하고 수정에 대한 도움을 요청하고 있습니다.

다른 멤버가 다이렉트 메시지를 통해 도움을 제안했습니다.

---

### aider (Paul Gauthier) ▷ #general (8 messages🔥):
> Aider의 git 서브모듈, Aider를 위한 저비용 LLM, Aider를 위한 Deepseek V3.2, Aider를 위한 Xiaomi/mimo-v2-flash, Aider를 위한 moonshotai/kimi-k2.5
- Aider, 더 빠른 코드 변경을 위한 /ok 별칭 추가: Aider의 메인 브랜치에 새로운 기능이 추가되었습니다: /ok는 이제 '/code Ok, please go ahead and make those changes.'의 별칭으로, 더 빠른 코드 수정을 가능하게 합니다.
- 사용자, Aider를 위한 저비용 LLM 찾기: 한 사용자가 Aider와 함께 사용할 최고의 저비용 LLM을 찾는 것에 대한 조언을 구하며, Gemini가 몇 시간 만에 모든 토큰을 소진했다고 언급했습니다.

다른 멤버는 OpenRouter를 사용하여 다른 모델 간에 전환할 것을 제안했습니다.
- Aider와 함께 추론을 위한 Deepseek V3.2 추천: 한 사용자는 Deepseek V3.2가 좋은 추론 능력을 가지고 있고 저렴하지만 때때로 약간 느릴 수 있기 때문에 Aider의 기본 LLM으로 추천합니다.
- Xiaomi/mimo-v2-flash, Aider에서 빠른 파일 편집에 탁월: Xiaomi/mimo-v2-flash는 Aider에서 퍼지 검색 및 교체 또는 내용 완성 같은 '단순' 파일 편집 기능에 매우 저렴하고 매우 빠르기 때문에 추천됩니다.
- moonshotai/kimi-k2.5, Aider에서 어려운 문제 해결: moonshotai/kimi-k2.5는 플래닝 모델로, mimo-v2-flash는 Aider에서 더 어려운 문제를 해결하기 위한 편집 모델로 제안됩니다.

---

### MLOps @Chipro ▷ #events (5 messages):
> WeAreDevelopers World Congress North America 2026, AI Control Hackathon 2026, Redwood Research, ControlArena 벤치마크 챌린지, ControlConf Berkeley
- WeAreDevelopers Congress, 북미에서 데뷔: WeAreDevelopers World Congress North America가 2026년 9월 23일부터 25일까지 캘리포니아 산호세에서 데뷔하며, 10,000명 이상의 개발자와 500명 이상의 연사가 대규모 실제 엔지니어링에 초점을 맞출 것으로 예상됩니다.

주제는 분산 시스템 스케일링, API 플랫폼 및 DevOps를 포함하며, wearedevelopers.us에서 Community_MLOps 코드를 사용하여 10% 할인을 받을 수 있습니다.
- Apart Research가 AI Control Hackathon 개최: Redwood Research와 공동 주최하는 Apart Research는 2026년 3월 20일부터 22일까지 AI Control Hackathon을 진행하며, 전복에도 불구하고 AI가 우리가 원하는 대로 작동하도록 보장하는 시스템에 초점을 맞춥니다.

이 해커톤은 ControlArena 벤치마크 챌린지, 제어 프로토콜 설계, 레드 팀을 포함한 세 가지 트랙을 특징으로 하며, $2,000의 현금 상금과 ControlConf 여행이 제공됩니다.
- ControlConf 여행 상품 제공: AI Control Hackathon 1등 상품은 ControlConf Berkeley (4월 18-19일)로의 전액 지원 여행이며, 항공편과 호텔이 포함됩니다.

ControlConf에 대해 자세히 알아보세요.

---

### DSPy ▷ #general (2 messages):
> SF DSPy 밋업, 프로덕션에서의 DSPy, RLM, Dropbox, Shopify
- SF DSPy 밋업 임박: 또 다른 SF DSPy 밋업을 발표합니다. 이번에는 프로덕션 사용 사례에서의 DSPy와 RLM에 초점을 맞춥니다.

Dropbox와 Shopify의 엔지니어들이 사례 연구를 공유할 것이며, dspy.RLM에 대한 워크스루가 있을 예정입니다. Luma 링크를 참조하세요.
- Dropbox 및 Shopify 엔지니어 발표 예정: Dropbox와 Shopify 엔지니어들이 SF DSPy 밋업에서 사례 연구를 발표할 예정입니다.

밋업은 프로덕션에서의 DSPy 사용과 RLM에 초점을 맞출 것입니다.

---

### tinygrad (George Hotz) ▷ #general (2 messages):
> JAX, 함수
- Tinygrad 개발자, JAX의 함수 설계 칭찬: tinygrad의 개발자 George Hotz는 JAX의 우수한 함수 설계를 인정하며, 설계 선택에 있어 그 영향력이나 정확성을 시사했습니다.

두 번째 트윗은 이 점을 더욱 강조합니다.
- JAX 함수 설계 칭찬: Tinygrad의 개발자는 JAX의 함수 설계 접근 방식에 대한 감탄을 표명했습니다.

이는 JAX의 방식이 Tinygrad의 유사한 선택에 대한 모델 또는 검증 역할을 할 수 있음을 시사합니다.

---

*이 문서는 news.smol.ai의 뉴스레터를 자동 번역한 것입니다.*
