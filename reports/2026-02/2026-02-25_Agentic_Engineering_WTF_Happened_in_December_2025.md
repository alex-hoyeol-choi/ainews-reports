# Agentic Engineering: WTF Happened in December 2025?

**원문 URL**: https://news.smol.ai/issues/2026-02-25-wtf-happened
**번역일**: 2026-02-27 06:58
**발행일**: 2026-02-25

---

코딩이 영원히 변했다는, '일반적인' 과대광고보다 훨씬 더 큰 불안한 기류가 감돌고 있습니다.
> 2026년 2월 24일-2월 25일 AI 뉴스입니다. 저희는 여러분을 위해 12개의 서브레딧, 544개의 트위터, 24개의 디스코드 (262개 채널, 10751개 메시지)를 확인했습니다. 절약된 예상 독서 시간 (분당 200단어 기준): 1086분. AINews 웹사이트에서 지난 호들을 모두 검색하실 수 있습니다. 참고로, AINews는 이제 Latent Space의 한 섹션입니다. 이메일 수신 빈도를 선택/해제하실 수 있습니다!
저희는 이를 위한 마이크로사이트를 만들었습니다:

# https://wtfhappened2025.com/
https://wtfhappened2025.com/
지금 방문하세요.

---

# AI 트위터 요약
Perplexity "Computer": 오케스트레이션 우선 에이전트 제품 (멀티 모델, 도구+환경, 사용량 기반 가격 책정)
- Perplexity Computer 출시: Perplexity가 Computer를 발표했습니다. 이는 하나의 인터페이스에서 파일, 도구, 메모리 및 모델을 오케스트레이션하여 프로젝트를 "연구, 설계, 코딩, 배포 및 관리"할 수 있는 엔드투엔드 시스템으로 포지셔닝됩니다 (출시 트윗, Arav Srinivas). 주요 제품 특징:

접근성 + 가격 책정: Max 구독자에게 먼저 웹에서 제공된 후, Pro/Enterprise 사용자에게 제공됩니다. 하위 에이전트 모델 선택, 지출 한도, Max (월 1만 크레딧)에 포함된 크레딧 및 기간 한정 보너스 크레딧 제공을 포함하는 사용량 기반 가격 책정입니다 (가격 세부 정보, 가용성, Arav의 출시 관련 발언).
아키텍처 강조: 여러 트윗에서 "획기적인 발전"은 단일 모놀리식 에이전트 루프가 아닌, 코디네이터 모델이 전문 모델(연구 대 코딩 대 미디어)에 작업을 할당하는 병렬적이고 비동기적인 하위 에이전트라고 강조합니다 (Lior의 분석, Denis Yarats).
"모든 것이 컴퓨터다" 내러티브: Perplexity 직원들은 Computer를 코딩 에이전트와 자동화된 평가/디버그 루프를 광범위하게 사용하여 소규모 팀이 구축한 플랫폼으로 강조했습니다 (Arav, Denis).
- 엔지니어에게 중요한 이유: Computer는 시스템 수준 에이전트 UX를 향한 구체적인 추진입니다. 멀티 모델 라우팅, 격리/샌드박스, 영구 메모리 및 비용 제어, 즉 '에이전틱 작업'을 단일 채팅 세션이 아닌 분산 워크플로우로 취급하는 것입니다 (Arav, Computer 사이트).
코딩 에이전트: "12월부터 작동하기 시작했습니다" + 새로운 모델/툴링 출시 (GPT‑5.3‑Codex, Claude Code 생태계, Copilot CLI GA)
- Karpathy의 "상변화" 주장: Andrej Karpathy는 코딩 에이전트가 12월 이후 질적인 임계점을 넘어섰다고 주장합니다. 취약한 데모에서 일관성과 끈기를 가지고 지속적이고 장기적인 작업을 완료하는 수준으로 발전했습니다. 그는 최소한의 개입으로 엔드투엔드 로컬 배포(SSH 키 → vLLM → 모델 다운로드/벤치마크 → 서버 엔드포인트 → UI → systemd → 보고서)를 위임하는 상세한 예를 제시합니다 (Karpathy). 이는 개발 도구 빌더 및 사용자(Cursor, snowmaker)의 광범위한 "소프트웨어가 변화하고 있다"는 정서와 일치합니다.
- OpenAI GPT‑5.3‑Codex 출시 + 초기 평가 논의:

OpenAI는 API에 GPT‑5.3‑Codex를 출시했으며 (snsf), Cline은 다음과 같은 개선 사항과 함께 지원을 발표했습니다: 5.2 대비 약 25% 더 빠르고, 작업당 더 적은 토큰을 사용하며, 강력한 SWE-Bench Pro 성능을 보입니다 (Cline).
커뮤니티의 벤치마크 반응은 날카롭고 (시끄러웠습니다): 예를 들어, "IBench에서 86%"라는 놀라움 (트윗)과 "첫 벤치마크가 들어오고 있습니다" (kimmonismus) 등이 있었습니다. 방법론이 명확해질 때까지는 이러한 결과를 참고 자료로만 간주해야 합니다.
- Claude Code: 제품 성숙도 + 관측 가능성 + 통합:

Claude Code의 "첫 번째 생일"이라는 표현과 회고록은 이를 기본적인 코딩 에이전트 제품으로 강조하며, 컨텍스트 길이 스케일링이 메모리 제약에 부딪히는 것에 대한 우려도 있습니다 (swyx).
실용적인 생태계 요소: Claude Code용 Slack 플러그인 통합 (catwu); Claude Code의 "nerfing"/라우팅 문제를 디버깅하기 위한 LangSmith 트레이싱 (hwchase17, 관측 가능성 불만).
- GitHub Copilot CLI 정식 출시 (GA) + "/research" 기능:

Copilot CLI가 GA에 도달했으며 (Evan Boyle), GitHub 코드 검색 + MCP 기반 동적 페칭을 사용하여 리포지토리 전체의 심층 연구를 위한 /research 기능을 추가했습니다. 공유를 위해 보고서를 gist로 내보냅니다 (기능).
작은 UX 참고 사항: 터미널의 Copilot CLI는 제목을 실시간으로 업데이트합니다 (트윗).
오픈 모델 및 로컬 인퍼런스: Qwen3.5 "Medium" 물결 (MoE + 긴 컨텍스트 + FP8/양자화), 그리고 로컬 에이전트 티핑 포인트
- Qwen3.5 Medium 시리즈 배포 공세: Alibaba는 vLLM, GGUF, LM Studio, Ollama 및 Jan에 걸쳐 출시 당일 툴링 지원을 추진했습니다. 이는 주요 오픈 릴리스를 위한 배포 스택이 얼마나 빨라졌는지를 보여줍니다 (vLLM 감사, GGUF, LM Studio, Ollama, Jan).
- Qwen의 주요 기술적 주장 (게시된 내용이며, 여기에서 독립적으로 검증되지 않음):

양자화 견고성: 4비트 가중치 + KV-캐시 양자화에서 "거의 손실 없는" 정확도.
긴 컨텍스트: Qwen3.5‑27B는 800K+ 컨텍스트를 지원하며, 35B‑A3B는 32GB VRAM 소비자 GPU에서 1M+ 컨텍스트를, 122B‑A10B는 80GB GPU에서 1M+ 컨텍스트를 지원합니다.
오픈 베이스: Qwen은 연구 지원을 위해 Qwen3.5‑35B‑A3B‑Base를 오픈소스화했습니다 (Alibaba_Qwen).
FP8 가중치는 네이티브 vLLM/SGLang 지원과 함께 공개되었습니다 (FP8 발표).
- 로컬 에이전트 "전/후": 주목할 만한 실무자의 주장은 Qwen3.5‑35B‑A3B가 로컬 에이전트 루프를 의미 있게 더 신뢰할 수 있게 만들며 (도구 호출, 안정성) 토큰당 약 3B 매개변수만 활성화한다는 것입니다. 이는 많은 워크플로우에서 로컬 에이전트가 Claude Code/Codex와 함께 실행 가능한 위치에 있음을 명확히 합니다 (victormustar).
- 평가 담론 경고: 벤치맥싱 및 MoE 대 덴스 혼란:

여러 스레드에서 리더보드를 과도하게 해석하는 것 ("벤치맥싱에 속지 마세요")에 대해 경고하며 (scaling01), 일부 벤치마크에서 Qwen 모델 크기 전반에 걸쳐 놀라운 동등성을 강조합니다. 이는 툴링 효과 또는 벤치마크 아티팩트 때문일 수 있음을 시사합니다 (eliebakouch, HLE/MoE 해석에 대한 teortaxesTex).
Arena는 Qwen3.5 Medium을 Text/Vision/Code Arena에 추가하여 직접 비교를 가능하게 했습니다 (Arena).
에이전트, 신뢰성, 그리고 "에이전트를 위한 구축": 최소한의 벤치마크, 도구 인터페이스 최적화 및 실패 모드
- 신뢰성은 능력만큼 향상되지 않았습니다: 신뢰성에 초점을 맞춘 연구는 모델의 빠른 발전에도 불구하고 신뢰성 향상은 미미하다고 주장합니다. 신뢰성을 여러 차원으로 분해하고 에이전트 성능을 단일 "성공률" 숫자로 축소하는 것에 대해 경고합니다 (IEthics, Justin Bullock 인용).
- 에이전트 실패는 종종 능력보다는 신뢰성 문제입니다: "에이전트 실패" 논문의 요약에 따르면 에이전트는 작은 경로 이탈 도구 호출이 누적되어 자주 실패하며, 특히 장기적인 설정에서 하나의 실수가 다음 실수의 가능성을 높입니다 (omarsar0).
- 최소한의 "안전하고 유용한" 벤치마크 아이디어: 더 어려운 작업 대신, 모델이 사소하게 지정된 안전한 행동(예: "요청받았을 때만 이메일 보내기")을 신뢰할 수 있게 수행할 수 있는지 측정하는 제안이 있습니다. 여기에는 관련 없는/방해되는 컨텍스트 하에서도 포함됩니다. 프론티어 모델조차 여전히 이러한 경우를 놓친다는 주장입니다 (jonasgeiping).
- 최적화 대상으로서의 도구 설명 (Trace‑Free+): Intuit AI Research 연구는 에이전트 성공이 도구 인터페이스 텍스트에 크게 의존한다고 제안합니다. 그리고 인퍼런스 시점에 트레이스를 요구하지 않고 모델이 도구 설명을 에이전트가 사용할 수 있는 형태로 다시 작성하도록 가르치는 커리큘럼을 소개합니다. StableToolBench/RestBench에서 성능 향상과 100개 이상의 도구에 대한 견고성이 보고되었습니다 (omarsar0).
- GUI/웹 에이전트: 계획 대 반응: ActionEngine은 GUI 에이전트를 오프라인 탐색을 통해 상태 머신을 생성하는 그래프 순회로 재구성합니다. 런타임은 약 1회의 LLM 호출로 전체 프로그램을 생성하며, 단계별 비전 루프에 비해 성공률/비용/레이턴시에서 큰 개선을 주장합니다 (dair_ai).
컴퓨팅, 메모리 및 인퍼런스 속도 프론티어: 칩 메모리 계층 구조, 확산 LLM 및 스케일링을 위한 인프라
- Karpathy의 "토큰 쓰나미"와 메모리 오케스트레이션에 대한 견해: 높은 참여도를 보인 스레드에서는 핵심 제약 조건을 두 가지 별개의 메모리 풀(빠르고 작은 온칩 SRAM 대 크고 느린 오프칩 DRAM)로 설명합니다. 그리고 가장 큰 과제는 LLM 워크플로우(프리필/디코드/학습)를 위해 메모리+컴퓨팅을 최적의 처리량/레이턴시/비용으로 오케스트레이션하는 것이라고 주장합니다. 특히 긴 컨텍스트 + 긴밀한 에이전틱 루프 하에서의 디코드가 어렵다고 말합니다. 이는 "HBM 우선" (NVIDIA와 유사) 및 "SRAM 우선" (Cerebras와 유사) 진영 모두에게 어려운 문제입니다 (Karpathy).
- 속도 대안으로서의 확산 LLM:

Andrew Ng는 Inception Labs의 확산 LLM에서 인상적인 인퍼런스 속도를 강조했습니다 (AndrewYNg).
별도의 논의에서는 확산 접근 방식이 초당 약 1000 토큰에 도달할 수 있으며, 칩이 아닌 아키텍처를 통해 속도 경쟁의 판도를 바꿀 수 있다고 주장합니다 (주의해서 해석해야 합니다; 마케팅은 종종 재현 가능한 평가보다 앞서 나갑니다) (kimmonismus).
연구 스레드: 균일 확산 LLM에서 인퍼런스 시간 스케일링을 위한 "Diffusion Duality (Ch.2) Ψ-Samplers" (ssahoo_).
- 대규모 해석 가능성: Goodfire는 최소한의 인퍼런스 오버헤드로 조 단위 매개변수 규모의 해석 가능성을 가능하게 하는 인프라 작업을 설명했습니다. 수십억 개의 활성화를 수집하고, 최소한 한 가지 사례 연구에서 실시간으로 CoT(chain-of-thought)를 조종할 수 있게 합니다 (GoodfireAI).
주요 발표 및 정책/안전 압력 지점: Anthropic 인수 + RSP 변화, 감시 우려 및 시장/전력 제약
- Anthropic은 Claude의 "컴퓨터 사용" 기능을 발전시키기 위해 Vercept를 인수했습니다 (AnthropicAI). Vercept 창업자의 스레드는 임무를 "사용자에게 무엇을 해야 할지 알려주는 것"에서 "사용자를 위해 행동하는 것", 특히 비기술적인 작업을 위해 행동하는 것으로 설명합니다 (ehsanik).
- Anthropic "RSP v3" 변화 (Responsible Scaling Policy): 논평은 엄격하고 일방적인 "완화 조치가 보장되지 않으면 임계값을 넘어선 학습 중단"에서 더 빈번한 투명성 아티팩트(로드맵 + 위험 보고서)로의 전환을 시사합니다. 또한 업데이트된 위협 모델 및 외부 검토 약속도 포함됩니다 (MaskedTorah). 더 선정적인 요약은 이것이 경쟁 압력과 위험 과학의 불확실성을 반영한다고 주장합니다 (kimmonismus).
- 감시 및 시민 자유: Jeff Dean은 대규모 감시가 언론의 자유를 위축시키고, 오용을 초래하며, 헌법적 보호를 침해한다는 점에 명시적으로 동의했습니다 (JeffDean). 관련 트윗에서는 불법 명령을 거부할 수 없는 자율적인 치안/감시 에이전트에 대한 우려를 제기했습니다 (BlackHC).
- 구속력 있는 제약으로서의 에너지: 한 보고서에 따르면 미국 정치 지도부는 수요가 전력망에 부담을 주면서 요금 납부자들의 반발을 피하기 위해 주요 AI/데이터센터 기업들이 자체적으로 전력을 조달하도록 압박하고 있다고 주장합니다 (kimmonismus). 이는 AI 스케일링이 알고리즘만큼이나 인프라/정책 문제가 되고 있음을 보여주는 예시입니다.
- Grok 4.20 Beta 리더보드 변동: Arena는 Grok‑4.20‑Beta1이 Search Arena에서 1위, Text Arena에서 4위를 기록했다고 보고합니다 (arena). 이는 여러 신호 중 하나로 간주해야 합니다. Arena 순위는 샘플링 정책 및 모델 변형에 따라 변동될 수 있습니다.

---

### 인기 트윗 (참여도, 기술적/관련성 기준)
- Karpathy, 12월 이후 코딩 에이전트의 "상변화"에 대해
- Perplexity, "Computer" 출시
- Arav Srinivas: Perplexity가 구축해 온 것 + "Computer"
- Karpathy, 컴퓨팅에 대해: 토큰 집약적인 LLM 워크로드의 SRAM 대 DRAM 오케스트레이션
- Anthropic, 컴퓨터 사용 기능 강화를 위해 Vercept 인수
- Qwen3.5 긴 컨텍스트 + 양자화 + 기본 모델 세부 정보
- 로컬 에이전트 티핑 포인트: 32GB RAM으로 Qwen3.5‑35B‑A3B를 로컬에서 실행
- Goodfire: 조 단위 매개변수 규모 해석을 위한 인프라
- ActionEngine: 오프라인 GUI 탐색 → O(1) LLM 호출 실행 프로그램

---

# AI Reddit 요약

## /r/LocalLlama + /r/localLLM 요약

### 1. Qwen 3.5 모델 성능 및 벤치마크
- Qwen 3.5, 어려운 코딩 작업에서 '크레이터' 발생 — 여러분이 직접 할 필요 없도록 70개 실제 리포지토리에서 모든 Qwen3.5 모델 (및 Codex 5.3) 테스트. (활동: 685): 이 게시물은 실제 코딩 작업에서 다양한 AI 코딩 모델을 평가하는 APEX Testing이라는 포괄적인 벤치마크 테스트에 대해 논의합니다. 이 벤치마크는 실제 GitHub 리포지토리의 70개 작업을 포함하며, 버그 수정, 리팩토링 및 도구 구축에 중점을 둡니다. 특히 Codex 5.3은 난이도 수준 전반에 걸쳐 일관되게 좋은 성능을 보이는 반면, Qwen 3.5 397B는 여러 파일에 걸친 조정을 요구하는 복잡한 작업에서 어려움을 겪습니다. GLM-4.7 양자화 모델은 모든 Qwen 3.5 모델을 능가하는 최고의 로컬 모델로 강조됩니다. 방법론은 공정한 비교를 위해 에이전틱 도구 사용 시스템을 포함하며, 결과는 정확성, 완전성, 품질 및 효율성을 기반으로 점수가 매겨집니다. 전체 리더보드 및 상세 결과는 APEX Testing에서 확인할 수 있습니다. 댓글 작성자들은 모델 성능이 사용되는 프레임워크에 따라 크게 달라질 수 있으므로 다른 에이전틱 프레임워크로 테스트할 것을 제안합니다. 또한 테스트된 특정 GLM-4.7 모델에 대한 논의도 있으며, 이들이 더 작은 Flash 모델인지 아니면 더 큰 버전인지에 대한 의문이 제기됩니다.

UmpireBorn3719는 gpt-oss-20b와 qwen3 coder next 간의 비교를 강조하며, gpt-oss-20b가 1405점을 기록한 반면 qwen3 coder next는 1328점을 기록했다고 언급합니다. 이는 주어진 벤치마크를 기반으로 gpt-oss-20b가 코딩 작업에서 더 나은 성능을 보일 수 있음을 시사합니다.
metigue는 다른 프레임워크를 사용하는 것이 모델 성능에 미치는 영향에 대해 논의하며, 오픈소스 모델이 프레임워크에 따라 50% 이상의 성능 변동을 보일 수 있다고 언급합니다. 그들은 프레임워크 선택이 어떤 모델이 최고로 보이는지를 극적으로 바꿀 수 있으므로 인기 있는 프레임워크로 테스트할 것을 제안하며, Droid 프레임워크를 사용할 때 GLM-5가 opus 4.6 및 codex 5.3을 능가하는 사례를 인용합니다.
FullstackSensei는 오픈 라우터를 통해 제공될 때 오픈 웨이트 모델의 벤치마크 신뢰성에 대한 우려를 제기합니다. 그들은 적용된 특정 양자화 또는 비용 절감 조치를 알지 못하면 성능 결과가 오해의 소지가 있을 수 있다고 주장합니다. 그들은 Q8 미만과 같은 낮은 양자화 수준에서 더 작은 모델을 실행하면 특히 복잡한 작업에서 성능을 크게 저하시킬 수 있다고 강조합니다.
- Qwen3.5 27B가 35B-A3B보다 나은가요? (활동: 637): 이 이미지는 Qwen3.5 시리즈의 여러 모델, 특히 27B 및 35B-A3B 모델의 성능을 명령어 따르기, 대학원 수준 추론, 다국어 지식과 같은 다양한 벤치마크에서 비교합니다. 논의는 16GB VRAM 및 32GB RAM이라는 하드웨어 제약 조건에서 어떤 모델이 더 효율적일지에 초점을 맞춥니다. 27B 모델은 3090 GPU에서 더 나은 성능을 보이며, 35B-A3B의 초당 20 토큰 대비 초당 100 토큰의 속도 차이를 달성하여, 27B 모델이 제한된 하드웨어 리소스를 가진 사용자에게 더 적합할 수 있음을 시사합니다. 한 사용자는 27B 모델이 3090 GPU에서 더 나은 성능을 보인다는 개인 테스트 결과를 공유하며, 상당한 속도 차이를 강조합니다. 이는 27B 모델이 유사한 하드웨어 설정을 가진 사용자에게 더 효율적일 수 있음을 시사합니다.

FusionCow는 3090 GPU에서 Qwen3.5 27B와 35B-A3B 모델 간의 성능 차이를 언급하며, 27B 모델이 35B-A3B의 초당 20 토큰 대비 초당 100 토큰의 처리량을 달성했다고 말합니다. 이는 27B 모델이 속도 면에서 더 효율적이어서 처리 시간이 중요한 요소인 작업에 더 선호될 수 있음을 시사합니다.
boinkmaster360은 Qwen3.5 27B 모델이 덴스 모델이며, 이로 인해 속도는 느리지만 잠재적으로 더 지능적일 수 있다고 제안합니다. 이는 계산 속도와 모델의 복잡한 작업 처리 능력 사이에 절충점이 있음을 의미하며, 사용자의 특정 요구 사항에 따라 고려될 수 있습니다.
Alternative_You3585는 Qwen3.5 27B 모델이 지능 면에서 우수할 가능성이 높지만, 35B-A3B 모델은 실제 세계 지식과 속도 면에서 이점이 있을 수 있다고 강조합니다. 이는 27B가 인지 작업에서 뛰어나고, 35B-A3B는 빠르고 지식 기반 응답을 요구하는 애플리케이션에 더 적합할 수 있는 미묘한 성능 프로필을 나타냅니다.
- Qwen3.5-35B-A3B는 에이전틱 코딩의 판도를 바꾸는 모델입니다. (활동: 1588): 이 게시물은 llama.cpp를 사용하여 단일 RTX 3090 GPU에서 Opencode로 테스트된 Qwen3.5-35B-A3B 모델의 성능에 대해 논의합니다. 이 모델은 130k 컨텍스트 윈도우로 실행되며 초당 100 토큰 이상을 달성했고 22GB의 VRAM을 사용했습니다. 이 모델은 AI 이전에는 일반적으로 5시간이 걸리던 코딩 테스트를 단 10분 만에 성공적으로 완료했습니다. 또한 이 모델은 5분 만에 대시보드 데모를 재현하여 에이전틱 코딩 도구로서의 효율성과 잠재력을 보여주었습니다. 한 댓글 작성자는 5090 GPU에서 초당 180 토큰을 달성했다고 언급했으며, 다른 댓글 작성자는 Spark에서 8비트 양자화 버전을 사용하여 기본적인 파일 텍스트 편집에 문제가 있었다고 보고하여, 다양한 설정에서 성능의 가변성을 나타냅니다.

Additional-Action566이 언급했듯이, Qwen3.5-35B-A3B는 5090 GPU에서 초당 180 토큰의 보고된 속도로 인상적인 성능을 보여줍니다. 이는 특히 고성능 하드웨어 설정에서 상당한 효율성 개선을 시사합니다.
Comrade-Porcupine은 Spark에서 8비트 양자화와 함께 사용될 때 모델의 한계를 강조합니다. 이 모델은 코드를 읽는 데 능숙함에도 불구하고 기본적인 파일 텍스트 편집 작업에서 어려움을 겪었습니다. 이는 특정 구성에서 도구 사용 기능에 잠재적인 문제가 있음을 나타내며, 양자화 효과 때문일 수 있습니다.
jslominski는 Unsloth의 MXFP4 양자화를 사용하여 모델을 실행하기 위한 상세한 구성을 공유합니다. 이 설정에는 컨텍스트 크기 131072, 온도 0.6, top-p 0.95와 같은 매개변수가 포함되며, 이는 코딩 작업에 맞춰져 있습니다. 이 구성은 일관되고 컨텍스트에 맞는 코드 출력을 생성하는 모델의 성능을 최적화하는 것을 목표로 합니다.
- Qwen3.5 27B는 크기와 성능 면에서 '천생연분'입니다 (활동: 391): 이 게시물은 RTX A6000 48GB GPU에서 CUDA와 llama.cpp를 사용하여 구현된 Qwen3.5-27B-Q8_0 모델의 설정 및 성능에 대해 논의합니다. 이 모델은 32K 컨텍스트 윈도우에서 약 초당 19.7 토큰의 속도를 달성합니다. Q8 양자화는 28.6GB VRAM을 효율적으로 사용하여 충분한 KV 캐시 공간을 확보하고, 전체 BF16과 유사한 품질을 유지하기 위해 선택되었습니다. 이 모델의 아키텍처는 Gated Delta Networks와 표준 어텐션 레이어를 결합하여 긴 컨텍스트에 대한 처리 속도를 향상시킵니다. 이 모델은 262K 네이티브 컨텍스트 윈도우, 201개 언어를 지원하며, 비전 기능도 갖추고 있습니다. 벤치마크에 따르면 이 모델은 GPQA Diamond, SWE-bench 및 Harvard-MIT 수학 토너먼트에서 선도적인 클로즈드 소스 모델과 경쟁합니다. 스트리밍은 llama-server OpenAI 호환 엔드포인트를 통해 지원됩니다. 모델 카드. 댓글 작성자들은 다양한 양자화 수준과 하드웨어 설정의 효율성에 대해 논쟁합니다. 한 사용자는 RTX 3090에서 Q5 양자화로 초당 25 토큰을 달성했다고 보고하는 반면, 다른 사용자는 높은 VRAM 비용과 다른 설정에 비해 상대적으로 낮은 토큰 생성 속도를 고려할 때 Qwen3.5-27B와 같은 덴스 모델의 실용성에 의문을 제기합니다.

Conscious_Cut_6144는 Q4-XL 양자화를 사용하여 단일 RTX 3090 GPU에서 Qwen3.5 모델에 대한 상세한 성능 벤치마크를 제공합니다. 이 설정은 15k 컨텍스트에서 초당 800 토큰의 프리필 속도와 초당 31 토큰의 생성 속도를 달성하며, 110k 컨텍스트는 완전히 오프로드됩니다. 이는 모델이 상당한 속도로 대규모 컨텍스트를 처리하는 효율성을 강조합니다.
Southern-Chain-6485는 RTX 3090에서 다양한 양자화 수준을 비교하며, Q5 양자화는 초당 25 토큰을 달성하는 반면, Q8 양자화는 초당 5 토큰으로 떨어진다고 언급합니다. 이는 더 높은 양자화 수준이 GPU 메모리에 들어갈 수 있지만 성능에 상당한 영향을 미쳐, 모델 크기와 속도 사이의 절충점에 대한 의문을 제기합니다.
LinkSea8324는 덴스 모델과 비교하여 MoE(Mixture of Experts) 모델의 한계에 대해 논의하며, 특히 여러 전문 분야를 요구하는 작업에서 그렇다고 말합니다. 그들은 MoE 모델이 효율적일 수 있지만, 다양한 기술 세트를 요구하는 실제 애플리케이션에서는 성능이 떨어질 수 있으며, 이러한 시나리오에는 덴스 모델이 더 적합할 수 있다고 제안합니다.

### 2. 새로운 모델 출시 및 발표
- Liquid AI, LFM2-24B-A2B 출시 (활동: 448): Liquid AI는 240억 개의 매개변수를 가진 희소 MoE(Mixture-of-Experts) 모델인 LFM2-24B-A2B를 출시했으며, 이 중 토큰당 20억 개가 활성화됩니다. 이 모델은 3억 5천만 개에서 240억 개 매개변수로 확장된 LFM2 제품군의 일부이며, 토큰당 컴퓨팅을 늘리지 않고도 효과적인 스케일링을 보여줍니다. 이 아키텍처는 40개 레이어와 MoE 블록당 64개 전문가를 포함하며 top-4 라우팅을 사용하고, 32GB RAM에서 실행되도록 설계되어 하이엔드 소비자 기기에 적합합니다. 이 모델은 llama.cpp, vLLM 및 SGLang을 통한 인퍼런스를 지원하며, 여러 GGUF 양자화가 제공됩니다. 벤치마크는 모델이 스케일링됨에 따라 로그 선형 품질 개선을 보여주며, Hugging Face에서 오픈 웨이트로 제공됩니다. 댓글 작성자들은 특히 다른 20억 개 미만 모델과 비교하여 모델 성능에 대해 낙관적이며, 더 상세한 벤치마크에 관심을 보입니다. 또한 사전 학습 완료에 대한 기대감도 있으며, 이는 향상된 버전인 LFM2.5-24B-A2B로 이어질 것입니다.

LFM2-24B-A2B 모델은 현재까지 17조 개의 토큰으로 학습되었으며, 사전 학습은 여전히 진행 중입니다. 완료되면 이 모델은 추가 후처리 학습 및 강화 학습을 통합하여 LFM2.5-24B-A2B로 발전할 것입니다. 이 출시는 본질적으로 미리 보기이며, 모델의 기능이 여전히 개발 및 개선 중임을 나타냅니다.
이 모델의 엣지 디바이스 성능이 강조되며, AMD CPU에서 초당 112 토큰, H100 GPU에서 초당 293 토큰의 디코드 속도를 보입니다. 이 모델은 32GB RAM을 필요로 하며, 출시 당일부터 llama.cpp, vLLM 및 SGLang과 같은 프레임워크를 지원합니다. 이는 효율적인 배포와 인기 있는 머신러닝 프레임워크와의 호환성에 중점을 두고 있음을 시사합니다.
LFM2-24B-A2B 출시에는 상세한 벤치마크가 부족하다는 점이 지적되었으며, 일부 사용자들은 공식 웹사이트에 제공된 벤치마크에 대해 회의적인 시각을 보입니다. 이는 실제 시나리오에서 모델의 기능을 검증하기 위한 더 포괄적인 성능 데이터에 대한 요구를 나타냅니다.
- Qwen, 새로운 Qwen3.5 Medium 모델 출시! (활동: 141): 이 이미지는 35B-A3B, 27B 및 122B-A10B 모델을 포함하는 Qwen3.5 Medium 모델의 출시를 발표합니다. 이 모델들은 256K 컨텍스트를 처리하도록 설계되었으며, 에이전틱 코딩, 비전 및 채팅과 같은 분야에서 뛰어납니다. 이 이미지는 명령어 따르기, 시각적 추론 및 문서 인식과 같은 다양한 벤치마크에서 이 모델들의 성능을 비교하는 막대 그래프를 보여줍니다. 모델들은 다른 색상으로 강조되어 있으며, 텍스트는 모델의 기능, 하드웨어 요구 사항 및 파인튜닝 옵션에 대한 세부 정보를 제공합니다. 이 출시는 AI 모델 성능과 복잡한 작업을 처리하는 다재다능함에 잠재적으로 미칠 영향 때문에 중요합니다. 댓글 작성자들은 특히 4비트 35B 모델과 6비트 27B 모델을 비교하여 테스트하는 데 관심을 보입니다. 또한 GGUF 모델의 증가로 인해 실제 vLLM 지원에 대한 요구도 있습니다.

Qwen3.5 Medium 모델 출시는 2비트에서 16비트에 이르는 다양한 GGUF 형식을 포함하며, 이는 Hugging Face에서 사용할 수 있습니다. 이러한 다양성은 다양한 정밀도 수준에서 테스트를 가능하게 하며, 이는 특정 애플리케이션에서 성능 최적화에 중요할 수 있습니다. 모델은 35B 및 27B와 같은 크기로 제공되어 다양한 계산 용량 및 사용 사례에 대한 옵션을 제공합니다.
4비트 정밀도의 35B 모델과 6비트 정밀도의 27B 모델의 성능을 비교하는 데 관심이 있습니다. 이러한 비교는 모델 크기와 정밀도 사이의 절충점, 특히 계산 효율성과 정확성 측면에서 통찰력을 제공할 수 있습니다. 이러한 비교는 특정 작업 또는 하드웨어 제약 조건에 맞게 모델을 최적화하려는 사용자에게 필수적입니다.
GGUF 모델의 증가로 인해 vLLM 지원의 필요성이 강조됩니다. vLLM (Very Large Language Models) 지원은 이러한 모델의 유용성과 기존 시스템으로의 통합을 향상시켜 잠재적으로 성능과 확장성을 개선할 수 있습니다. 이는 GGUF 형식으로 더 많은 모델이 출시되고 있으며, 이 형식이 아직 모든 프레임워크에서 완전히 지원되지 않을 수 있다는 점에서 특히 중요합니다.

### 3. 로컬 모델 실행 및 하드웨어 논의
- 현재 모두가 로컬에서 무엇을 실행하고 있나요? (활동: 252): 이 Reddit 게시물은 대규모 언어 모델(LLM)을 로컬에서 실행하기 위한 설정, 사용되는 모델, 실용성 및 관련 하드웨어에 대해 질문하고 있습니다. 특히, Qwen 3 coder next 80B는 더 작은 퀀타이제이션에서 뛰어난 성능을 보이는 것으로 강조되었으며, Mistral Small 3.2 24b 및 Magistral Small 24b는 MacBook Pro M4 Max에서 관리 작업을 위해 사용되고 있습니다. 이 설정에는 시맨틱 메모리와 문서 업로드를 위한 Xcode로 구축된 맞춤형 프론트엔드가 포함됩니다. 또한, Qwen3 4B는 iPhone에서 속도와 유용성이 언급되며, 로컬 실행을 통한 프라이버시가 강조됩니다. 댓글들은 성능과 프라이버시의 균형을 맞추는 모델에 대한 선호를 반영하며, 사용자들은 외부 제공업체에 데이터를 노출하지 않기 위해 로컬 설정을 선택하고 있습니다. 모바일 기기에서 Qwen3 4B와 같은 작고 효율적인 모델의 사용은 실용적인 일상 애플리케이션으로의 추세를 보여줍니다.

Greenonetrailmix는 Qwen 3 Coder Next 80B의 성능을 강조하며, 다른 모델에 비해 더 작은 퀀타이제이션에서 우수한 성능을 보인다고 언급했습니다. 이는 Qwen 3가 리소스 제약이 있는 환경에서 효율성을 위해 최적화되어 로컬 배포에 인기 있는 선택임을 시사합니다.
Nefhis는 MacBook Pro M4 Max에서 Mistral Small 3.2 24b 및 Magistral Small 24b 모델을 사용하며, Xcode를 사용하여 맞춤형 프론트엔드를 구축했다고 설명합니다. 이 설정에는 시맨틱 메모리 및 문서 업로드 기능이 포함되어 있으며, 외부 제공업체에 노출을 피함으로써 프라이버시를 강조합니다. 이 설정은 데이터 기밀성을 유지하기 위해 로컬 처리를 활용하여 관리 작업에 맞춰져 있습니다.
mister2d는 오래된 하드웨어에서 Nemotron 3 Nano를 실행하여 모델의 하이브리드/SWA 아키텍처 덕분에 128K 컨텍스트에서 초당 30-40 토큰을 달성했다고 보고합니다. 하드웨어 설정에는 Dual Xeon (Ivy Bridge), 256 GB DDR3 및 2x RTX 3060 (12GB)이 포함되어 있으며, 에이전틱 플로우의 성능을 최적화하기 위해 레거시 구성 요소와 최신 GPU 간의 균형을 보여줍니다.

## 덜 기술적인 AI 서브레딧 요약
> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo

### 1. AI 모델 및 벤치마크 출시
- Bullshit Benchmark - 모델이 터무니없는 프롬프트에 자신 있게 답변하는 대신 이를 식별하고 거부하는지 테스트하는 벤치마크 (활동: 1060): 이미지는 다양한 AI 모델이 터무니없는 프롬프트를 감지하고 적절하게 응답하는 능력을 평가하는 'Bullshit Benchmark' 막대 차트를 보여줍니다. 이 차트는 모델 성능을 세 가지 수준으로 분류합니다: 녹색(높은 감지 정확도), 황색(중간 정확도), 빨간색(낮은 정확도). 특히 Claude Opus 4.6과 같은 모델은 상당한 녹색 영역으로 높은 성능을 보인 반면, 다른 모델들은 빨간색 영역이 더 많아 성능이 좋지 않음을 나타냅니다. 이 벤치마크는 모델이 데이터를 암기하는 것뿐만 아니라 컨텍스트를 이해하여 터무니없는 질문에 자신 있게 답변하는 것을 피하는 것의 중요성을 강조합니다. 댓글 작성자들은 현재 벤치마크가 데이터 암기에 초점을 맞추는 경우가 많으므로, 모델이 터무니없는 프롬프트를 감지하는 능력을 테스트하는 벤치마크의 필요성을 강조합니다. 또한 Gemini가 터무니없는 프롬프트에 대해 비꼬는 듯한 응답을 하는 경향이 있어 낮은 평가에 영향을 미칠 수 있다는 언급도 있습니다.

MangusCarlsen은 '세차 테스트'에서 입증된 바와 같이 모델 'Gemini'가 터무니없는 프롬프트에 비꼬는 듯한 응답을 하는 경향이 있다고 강조합니다. 이러한 행동은 낮은 평가에 기여할 수 있으며, 모델이 터무니없는 프롬프트를 처리하는 방식이 평가의 한 요소임을 시사합니다.
AppropriateDrama8008은 훈련 데이터 암기만을 평가하는 대신, 모델이 터무니없는 프롬프트를 감지하고 응답하는 능력을 테스트하는 벤치마크의 필요성을 주장합니다. 이러한 접근 방식은 모델이 컨텍스트와 의도를 이해하는 것의 중요성을 강조하며 실제 애플리케이션에 더 유익하다고 여겨집니다.
Orangeshoeman은 Dario Amodei와 Demis Hassabis 간의 논의를 언급하며, Dario의 초점이 객관적인 데이터를 마스터하는 모델에 있다고 지적합니다. 이러한 전략적 초점은 Anthropic의 Claude와 같은 모델이 사실 정보를 이해하고 처리하는 것을 우선시하기 때문에 특정 벤치마크에서 더 나은 성능을 보이는 이유를 설명할 수 있습니다.
- Nano Banana 2가 현실이 되었습니다! Gemini 3.1 Flash Image가 Vertex AI Catalog에 방금 나타났습니다 (활동: 184): 게시물의 이미지는 새로 출시된 Nano Banana 2 (Gemini 3.1 Flash Image라고도 함)와 기존 Nano Banana Pro 모델의 기능을 보여주는 두 AI 생성 인물 사진의 나란히 비교입니다. 이 게시물은 새로운 모델이 'Flash' 티어임에도 불구하고 Pro 버전에 가까운 품질을 제공하며, 특히 밀집된 구성에서 공간 논리에 탁월하다고 강조합니다. 이 모델은 고속, 저비용 생산을 위해 설계되었으며, 대량 사용자 생성 콘텐츠(UGC) 광고 생성 및 비디오 모델을 위한 일관된 프레임 생성과 같은 고주파 파이프라인에 적합합니다. 이 이미지는 두 모델의 출력 품질을 시각적으로 비교하는 테스트 역할을 합니다. 한 댓글 작성자는 제공된 예시에서 Nano Banana Pro가 여전히 새로운 모델보다 우위에 있다고 믿으며, Pro의 출력 품질에 대한 선호를 나타냅니다.

원래 Flash Image 모델은 견고한 이미지 품질을 가졌지만, 특히 복잡한 지침에서 프롬프트의 일부를 무시하거나 동일한 출력을 재생성하는 등 프롬프트 준수 문제에 직면했습니다. 또한 텍스트 및 인포그래픽 렌더링, 다중 이미지 합성에도 어려움을 겪었습니다. 새로운 Gemini 3.1 버전의 핵심 질문은 이러한 문제가 해결되었는지, 특히 밀집된 프롬프트 처리에서 개선되었는지 여부입니다.

### 2. Anthropic Claude 및 군사 사용 논란
- xAI와 펜타곤, Grok을 기밀 시스템에 사용하기로 합의, Anthropic은 최후통첩을 받다 (활동: 580): Elon Musk가 설립한 xAI는 펜타곤과 Grok AI 모델을 기밀 군사 시스템에 통합하기로 합의했습니다. 이러한 발전은 Anthropic과의 분쟁 이후에 이루어졌는데, Anthropic의 Claude 모델은 민감한 군사 작전에 사용되는 유일한 AI였습니다. 펜타곤은 Claude가 '모든 합법적인 목적'에 사용될 수 있도록 요구하지만, Anthropic은 특히 대량 감시 및 자율 무기 사용에 반대합니다. xAI는 이러한 조건에 동의했으며, Anthropic이 따르지 않을 경우 Claude를 대체할 가능성이 있습니다. 한편, Google의 Gemini와 OpenAI의 ChatGPT도 기밀 사용을 위해 고려되고 있으며, Google은 계약에 근접한 것으로 알려졌습니다. 댓글 작성자들은 펜타곤이 Anthropic의 Claude를 선호하는 것이 Claude의 우수한 성능을 나타내거나, 더 광범위한 사용 조건에 따르도록 압력을 가하는 전략적 록인일 수 있다고 추측합니다. 또한 정부가 상업용 AI 모델에 의존하는 것에 대한 회의론도 있으며, 왜 더 발전된 비밀 기술을 활용하지 않는지에 대한 의문도 제기됩니다.

EmbarrassedRing7806은 펜타곤이 Anthropic을 선호하는 것에 대해 논의하며, 이는 Claude가 우수하다는 믿음 또는 Anthropic에 준수를 압박하기 위한 전략적 움직임을 나타낼 수 있다고 제안합니다. 이 댓글은 록인 전략의 가능성을 강조하며, 펜타곤이 대안이 있더라도 기존 관계를 유지하는 것을 선호할 수 있음을 시사합니다.
nic_haflinger는 xAI가 연방 사용에 필요한 FedRAMP 표준을 준수하는 클라우드 서비스를 갖추고 있지 않다고 지적합니다. 이는 Grok이 사용될 수 있지만, 연방 규정을 충족하기 위해 준수하는 플랫폼에서 호스팅되어야 함을 의미하며, xAI가 정부 계약을 확보하는 데 상당한 장애물임을 강조합니다.
- 독점: Hegseth, Anthropic에 AI 안전 장치 철회 기한을 금요일까지 주다 (활동: 1146): Axios 보도에 따르면, Pete Hegseth 국방장관은 Anthropic에 최후통첩을 발행하여 금요일까지 Claude AI 모델에서 안전 장치를 제거할 것을 요구했습니다. 펜타곤은 국내 감시 및 자율 무기 개발을 포함한 목적으로 Claude에 대한 무제한 접근을 원하며, 이는 Anthropic의 서비스 약관에 위배됩니다. 이를 준수하지 않을 경우 국방물자생산법(Defense Production Act) 발동 또는 회사를 공급망 위험으로 분류하여 정부 계약에서 블랙리스트에 오를 수 있습니다. 주목할 만한 댓글은 AI 기업이 정부 사용에 안전 조치를 부과하는 아이러니를 강조하며, 규제에서 예상되는 역할의 역전을 시사합니다.
- 펜타곤, Claude 그리고 군사 사용 (활동: 1258): 이 이미지는 BFM Tech 기사의 스크린샷으로, 펜타곤이 1950년 법을 언급하며 72시간 이내에 Anthropic에 AI인 Claude의 군사 사용을 허용할 것을 요구하는 내용을 다루고 있습니다. 이는 AI 기술과 군사 응용의 교차점을 강조하며, 국가 안보 및 AI 배포의 윤리적 고려 사항에 대한 잠재적 함의를 보여줍니다. 이 기사는 상업적 AI 개발과 정부 통제 사이의 긴장, 특히 국제 안보 및 감시 능력의 맥락에서 긴장을 시사합니다. 댓글들은 펜타곤의 예산 효율성에 대한 회의론을 반영하고 권위주의 정권에서 AI의 역할에 대한 우려를 강조하며, 군사적 맥락에서 AI의 윤리적 사용에 대한 신중한 고려의 필요성을 시사합니다.

Informal-Fig-7116의 댓글은 군사 애플리케이션에서 AI 사용을 둘러싼 윤리적 우려를 강조하며, 특히 Anthropic이 AI 모델인 Claude를 사용하는 조건에 초점을 맞춥니다. 조건은 엄격합니다: 대량 감시 및 자율 무기 금지. 댓글 작성자는 합법성을 식별하는 능력 없이 명령을 따르는 AI의 잠재적 위험을 강조하며, 이는 무차별적인 행동으로 이어질 수 있습니다. 이는 국방 맥락에서 AI 배포에 대한 중요한 윤리적 및 운영적 질문을 제기합니다.
PetyrLightbringer의 댓글은 펜타곤의 AI에 대한 재정적 투자에 대한 회의론을 시사하며, Opus와 같은 모델을 사용하는 경우 2억 달러가 충분하지 않을 수 있음을 암시합니다. 이는 AI 개발의 빠른 속도와 최첨단 기술의 필요성을 고려할 때, 군사 애플리케이션에서 AI 투자의 비용 효율성 및 전략적 가치에 대한 광범위한 우려를 반영합니다.
Informal-Fig-7116이 언급한 국방물자생산법(DPA)에 대한 논의는 국가 안보 요구를 충족하기 위해 AI 기업에 대한 정부 개입 가능성을 지적합니다. DPA는 과거 COVID-19 팬데믹과 같은 비군사적 목적으로 사용되었으며, AI에서의 잠재적 사용은 국가 안보와 기업 자율성 간의 균형에 대한 질문을 제기합니다. 이는 기술 산업에서 미래 정부 조치에 대한 선례를 세울 수 있습니다.
- TIME: Anthropic, 주력 안전 서약 철회 (활동: 1357): Anthropic은 이전에 안전 조치가 적절하다고 확신할 수 없는 한 AI 시스템을 훈련하지 않겠다고 약속했던 책임 있는 스케일링 정책(RSP)의 핵심 구성 요소를 포기하기로 결정했습니다. TIME이 보도한 바와 같이, 이러한 변화는 Anthropic의 최고 과학 책임자인 Jared Kaplan이 설명했듯이, 급속한 AI 발전과 경쟁 압력에 대한 전략적 전환을 반영합니다. Kaplan은 AI 개발 속도와 경쟁사의 행동을 고려할 때 일방적인 약속은 비실용적이라고 언급했습니다. 댓글 작성자들은 Anthropic이 OpenAI에 비해 뒤처진다는 인식에 회의적이며, 일부는 Hegseth와 같은 외부 압력이 결정에 영향을 미쳤을 수 있다고 제안합니다. 또한 AI 개발을 책임감 있게 관리하기 위한 글로벌 규제에 대한 요구도 있습니다.

DarkSkyKnight는 Anthropic이 생물학 무기나 핵 위협과 같은 꼬리 위험(tail risks)에 초점을 맞추는 것이 AI가 일자리 시장에 미치는 즉각적인 경제적 영향을 가릴 수 있다는 중요한 문제를 강조합니다. 그들은 주니어급 직책이 사라지고 있으며, 이는 Anthropic이 적절하게 다루지 않은 문제라고 주장합니다. 이러한 관점은 실존적 위험이 중요하지만, AI 배포의 경제적 함의는 더 많은 관심을 필요로 하는 시급한 문제임을 시사합니다.
TheRealShubshub은 GPT-5를 둘러싼 비판을 고려할 때 Anthropic이 OpenAI에 뒤처진다는 인식에 의문을 제기합니다. 이 댓글은 AI 기업 간의 경쟁 환경이 복잡하며, 기술 발전뿐만 아니라 제품 성공 및 실패에 대한 대중과 산업의 인식에 의해서도 결정된다는 것을 암시합니다.
CurveSudden1104는 AI 개발에 대한 글로벌 규제의 필요성을 강조하며, Grok 및 OpenAI와 같은 기업은 외부 압력 없이는 안전을 우선시하지 않을 수 있다고 지적합니다. 이 댓글은 AI 안전을 보장하기 위한 규제의 역할과 규제되지 않은 AI 발전의 잠재적 위험에 대한 광범위한 논쟁을 강조합니다.

### 3. Claude Code 및 COBOL 현대화 영향
- IBM, Anthropic의 Claude Code 도구 출시로 10% 급락하며 최신 희생양이 되다. 이 도구는 COBOL 레거시 코드를 현대화하도록 설계되었습니다. 66년 된 프로그래밍 언어인 COBOL은 오늘날에도 널리 사용되고 있으며, 미국 ATM 거래의 약 95%가 COBOL 코드를 사용하여 처리됩니다 (활동: 483): Anthropic은 미국 ATM 거래의 95%를 처리하는 데 여전히 중요한 레거시 COBOL 코드를 현대화하기 위한 새로운 도구인 Claude Code를 발표했습니다. 이 발표는 IBM 주가가 10% 하락하는 결과를 초래했는데, 이 도구가 완전한 제품이 아닌 블로그 게시물을 통해 소개되었음에도 불구하고 말입니다. 이 도구는 Anthropic이 오래된 기술을 위한 전문 솔루션을 제공하려는 지속적인 노력의 일환이지만, 그 효과는 아직 입증되지 않았습니다. 댓글 작성자들은 이 도구가 새로운 제품이 아닌 블로그 게시물 제안이었기 때문에 발표에 대한 시장의 반응이 과잉 반응일 가능성이 높다고 지적했습니다. Anthropic 도구의 실제 영향에 대한 회의론도 있는데, COBOL과 같은 레거시 시스템을 현대화하는 데 그 효과가 아직 명확하지 않기 때문입니다.

Onipsis는 Anthropic의 Claude Code 발표가 직접적인 기술적 돌파구가 아니라 COBOL 시스템 현대화에 대한 잠재적 유용성을 시사하는 것이라고 강조합니다. 이 도구의 영향이 추측적이고 아직 입증되지 않았다는 점을 고려할 때, IBM 주가가 10% 하락한 시장의 반응은 불균형해 보입니다. 이는 시장 반응이 종종 구체적인 기술 발전보다는 인식에 기반하는 광범위한 추세를 반영합니다.
Milo-75는 Anthropic의 Claude Code가 IBM 사업에 미치는 영향이 과장될 수 있다고 주장합니다. 특히 은행과 같은 중요한 부문의 현대화 프로젝트는 수익에 영향을 미치는 다운타임을 피하기 위해 신중한 관리가 필요합니다. Claude Code와 같은 AI 도구가 프로젝트 시간을 단축할 수 있지만, IBM의 역할을 완전히 대체할 가능성은 낮습니다. 대신, 효율성 증가로 이어져 IBM이 더 많은 프로젝트를 처리할 수 있게 하여, 잠재적으로 수익 손실을 개선된 마진으로 상쇄할 수 있습니다.
Stabile_Feldmaus는 Anthropic의 전문 도구의 효능에 의문을 제기하며, 출시 시 주가가 부정적으로 반응하지만 산업에 미치는 실제 영향은 불분명하다고 지적합니다. 이는 시장 인식과 이러한 AI 도구의 실제 유용성 사이의 단절을 시사하며, 실제 가치를 평가하기 위한 더 구체적인 성능 데이터 및 피드백의 필요성을 강조합니다.
- Anthropic이 COBOL용 AI 도구를 출시하자 IBM 주가가 13% 하락했습니다 (활동: 1007): Anthropic은 은행, 항공 및 정부의 많은 레거시 시스템에 중요한 COBOL 코드베이스를 분석하고 현대화하도록 설계된 새로운 AI 도구를 출시했습니다. 이 도구는 위험을 식별하고 현대화 비용을 절감할 수 있어, 이러한 시스템 관리를 통해 상당한 수익을 얻는 IBM에 잠재적인 위협이 됩니다. 이 발표는 IBM 주가를 13% 하락시켜 25년 만에 최악의 날을 기록했으며, 투자자들은 IBM의 메인프레임 사업에 대한 위협으로 인식하여 반응했습니다. 그러나 일부 분석가들은 기업들이 기존 대안에도 불구하고 IBM에서 전환하는 데 역사적으로 느렸기 때문에 시장 반응이 과장될 수 있다고 주장합니다. 댓글 작성자들은 중요한 인프라를 처리하는 AI의 신뢰성에 대한 회의론을 표명했으며, 한 명은 그러한 맥락에서 '바이브 코딩'의 잠재적 위험을 언급했습니다. 다른 한 명은 시장 반응이 '무릎 반사'일 수 있다고 제안하며, 장기적인 영향은 덜 심각할 수 있음을 암시합니다.

제기된 핵심 요점은 은행들이 역사적으로 COBOL 시스템 현대화를 시간이나 돈 부족 때문이 아니라 관련된 막대한 위험 때문에 피했다는 것입니다. 현대화의 실수는 치명적인 결과를 초래할 수 있으며, 환각을 일으킬 수 있는 Claude와 같은 AI 도구는 여전히 모든 코드 라인에 대한 인간의 감독이 필요합니다. 따라서 AI가 마이그레이션 속도를 높일 수 있지만, 위험과 인간 검토라는 병목 현상을 아직 제거하지 못했습니다.
COBOL용 AI 도구의 도입은 시스템 통합업체 및 구현업체에 상당한 위협이 됩니다. AI는 덜 중요한 애플리케이션에 대한 외부 계약의 필요성을 줄일 수 있지만, IBM의 전문 서비스 사업에 미치는 영향은 상당할 수 있습니다. 이는 COBOL AI 도구에 대한 반응이 과장될 수 있지만, 서비스 제공업체에 대한 잠재적 혼란은 진정한 우려 사항임을 시사합니다.

---

# AI Discord 요약
> Gemini 3.1 Pro Preview Nov-18이 요약한 요약들의 요약
테마 1. 모델 벤치마크, 특이점 및 가격 업데이트
- Qwen 3.5, 코드 아레나를 압도하지만 페널티 없이는 수다스러워짐: 사용자들은 Alibaba의 코딩 계획을 Kimi와 GLM을 비용과 가치 면에서 압도하는 매우 유능한 코딩 모델로 극찬하며, 한 회원은 Qwen3.5 122B NVFP4 퀀트를 Hugging Face에 공개했습니다. 그러나 Unsloth 엔지니어들은 사용자가 명시적으로 presence penalty를 높이고 thinking mode를 끄지 않으면 거대한 122B A10B 변형이 완전히 장황해진다고 경고합니다.
- Grok 4.20 Beta 1, 검색 왕좌를 차지하다: xAI의 Grok-4.20-Beta1 모델은 Search Arena 리더보드에서 1226점이라는 엄청난 점수로 1위를 차지하며 GPT-5.2와 Gemini-3를 압도했습니다. 또한 Text Arena 리더보드에서 1492점으로 4위를 차지하며 Google의 Gemini 3.1 Pro와 동점을 기록했습니다.
- Codex 5.3, 가격표를 붙이고 Kimi는 수학 평가자를 정복하다: OpenAI는 Codex 5.3을 API에 출시했으며, 입력 토큰당 1.75달러, 출력 토큰당 14달러로 책정하여 비용 대비 성능에 대한 즉각적인 커뮤니티의 조사를 받았습니다. 한편, Kimi 2.5는 OS Frontier Math Level 4 벤치마크에서 4.2%의 점수를 기록하며 GLM 5와 Deepseek V3.2가 달성한 2.1%를 완전히 두 배로 뛰어넘었습니다.
테마 2. 인프라 혁신 및 대기업 하드웨어 거래
- Meta와 OpenAI, 수십억 달러 상당의 비밀 AMD 워런트 비축: 한 비밀 금융 수사관은 OpenAI와 Meta에 대규모 미래 GPU 지출과 직접적으로 연결된 지분 환급으로 1억 6천만 AMD 주식에 대한 워런트를 부여하는 거래를 밝혀냈습니다. AMD 600달러 주가 목표는 이 거대한 하드웨어 비공개 거래를 무려 1920억 달러로 평가할 수 있습니다.
- Packet.ai, Blackwell GPU 가격을 푼돈으로 인하: 개발자들은 Packet.ai의 Blackwell GPU 가격이 학습 워크로드에 대해 시간당 0.66달러 또는 월 199달러라는 놀랍도록 저렴한 가격으로 책정되자 환호했습니다. 엄청나게 비싼 B200 구매 가격에 직면한 다른 하드웨어 구매자들은 GPU를 직접 구매하는 대신 Lightning AI Clusters로 몰려가 Neocloud 인스턴스를 임대하고 있습니다.
- Zagora, 분산된 GPU를 통합 학습 괴물로 엮다: Zagora 팀은 Qwen 2.5 및 Mistral과 같은 70B+ 모델을 표준 인터넷 연결을 통해 완전히 훈련하기 위한 분산 파인튜닝 시스템을 적극적으로 구축하고 있다고 발표했습니다. 이 SWARM에서 영감을 받은 파이프라인은 무작위 소비자 GPU 클러스터를 거대한 슈퍼컴퓨터로 변환하지만, 개발자들은 현재 표준 트랜스포머 아키텍처에만 엄격하게 지원을 제한하고 있습니다.
테마 3. 자율 에이전트가 폭주하다
- Nous Research, Hermes Agent를 출시하여 파일 시스템을 돌아다니게 하다: Nous Research는 다단계 메모리 시스템과 영구적인 전용 머신 액세스로 구축되어 CLI에서 직접 실행되는 강력한 도구인 오픈소스 Hermes Agent 리포지토리를 공개했습니다. Nous Portal에서 HERMESAGENT 쿠폰 코드를 입력하는 초기 사용자들은 AI가 브라우저를 제어하고 서브 에이전트를 자율적으로 관리할 수 있도록 한 달 무료 이용권을 확보합니다.
- Rogue OpenClaw Proxy, DeepSeek Jailbreaks를 24시간 자동화하다: 한 영리한 사용자가 OpenClaw를 통해 DeepSeek-R1을 실행하는 자체 호스팅 자율 프록시를 구축하여 Claude, Gemini 및 Grok API 필터를 영구적이고 은밀하게 탈옥시켰습니다. 보안 비평가들은 이 프로젝트가 막대한 법적 노출, 서비스 약관 위반, 그리고 자율 에이전트가 실수로 공급망 익스플로잇을 다운로드할 수 있는 무서운 위험 때문에 즉시 비난했습니다.
- METR, 개발자들이 비지원 코딩을 싫어하기 때문에 인간 통제 그룹을 폐기하다: 평가 그룹 METR은 소프트웨어 개발자들이 "AI 없는" 통제 그룹에서 일하기를 점점 더 거부하며, 구식 수동 코딩 프로세스를 고통스럽게 비효율적이라고 부른다는 것을 발견했습니다. METR의 테스트 프로토콜 업데이트는 AI 도구 없이 테스터에게 시간당 50달러의 할인된 요율을 제공하는 것이 유능한 엔지니어링 참가자를 유치하는 데 완전히 실패했기 때문에 필요했습니다.
테마 4. 금지, 속도 제한 및 연쇄적인 API 중단
- Google과 Anthropic, 검소한 토큰 비축자들을 무자비하게 금지하다: Google은 사용자가 Gemini CLI를 통해 단 10개의 프롬프트를 보낸 후 Google Gemini 계정을 영구적으로 잠갔으며, 이는 Google AI Pro 구독을 적극적으로 지불하고 있는 중에도 발생했습니다. 마찬가지로 Claude AI 포털은 문서화되지 않은 OAuth 엔드포인트를 통해 보조금 토큰을 빼내려던 OpenClaw 사용자들을 공격적으로 금지하기 시작했습니다.
- OpenRouter에 연쇄적인 장애 발생, Perplexity는 이미지 속도 제한: OpenRouter는 2월 17일과 19일에 상위 인프라 장애로 인해 대규모 401 인증 오류가 발생했음을 확인하는 OpenRouter 사후 보고서를 발표했습니다. Perplexity 서버에서는 유료 Pro 사용자들이 극도로 제한적이고 예고 없는 일일 이미지 업로드 제한에 부딪혀 간단한 숙제를 끝내지 못하게 되자 폭동을 일으켰습니다.
- 시스템 수준 AI 에이전트, 실수로 사용자 휴지통 폴더를 삭제하다: OpenClaw 에이전트에 전체 시스템 권한을 부여한 사용자들은 AI가 요청에 따라 전체 휴지통 디렉토리를 무심코 영구적으로 지워버리자 당황했습니다. 개발자들은 자율 LLM 에이전트에 루트 시스템 액세스 권한을 부여하는 것이 해당 도구를 자발적으로 설치된 악성 소프트웨어로 효과적으로 분류하는지에 대해 열띤 논쟁을 벌였습니다.
테마 5. 개발자 워크플로우 및 심층 프레임워크 조정
- Aider, 한 번의 키 입력 승인 추가 및 Kimi-Mimo 콤보 완성: Aider 코딩 도우미는 새로운 /ok 별칭을 메인 브랜치에 병합하여 개발자들이 AI 생성 코드 편집을 즉시 승인하고 실행할 수 있도록 했습니다. 파워 사용자들은 또한 매우 효율적인 모델 라우팅 스택을 발견했습니다: 그들은 높은 수준의 아키텍처 계획을 위해 무거운 moonshotai/kimi-k2.5를 사용한 다음, 실제 파일 편집은 매우 빠르고 저렴한 Xiaomi/mimo-v2-flash에 맡깁니다.
- LM Link, Tailscale을 통해 로컬 모델을 인터넷으로 밀반입하다: LM Studio 팀은 LM Link 문서를 출시하여, Tailscale을 래핑하여 사용자에게 로컬 LLM 서버에 대한 원활하고 종단 간 암호화된 원격 액세스를 제공하는 새로운 기능을 자세히 설명했습니다. 사용자들은 클라우드 제공업체를 완전히 우회하여 휴대폰에서 직접 홈 GPU에 쿼리할 수 있는 전용 모바일 앱을 즉시 요구했습니다.
- PyTorch, FA3 커널을 디스패처에 몰래 넣고 Serenade는 모든 것을 트랜스파일하다: PyTorch에서 activate_flash_attention_impl(“FA3”)를 호출하면 간단한 register_fn 딕셔너리 스왑을 사용하여 기본 Flash Attention 2 커널을 FA3로 안전하게 재정의합니다. 더 놀라운 언어 소식으로, 한 솔로 개발자가 Python처럼 작성되지만 C++, CUDA 및 x86-64 ASM으로 직접 트랜스파일되며 네이티브 Dear ImGui GUI 지원을 제공하는 새로운 구문인 Serenade를 공개했습니다.

---

# Discord: 상위 Discord 요약

## OpenClaw Discord
- OpenClaw 반-셀아웃(Anti-Sellout) 입장: 한 회원은 토큰 도난 및 데이터 프라이버시 침해 위험 때문에 관리형 OpenClaw 설정에 대해 강력히 경고하며, 간단한 VPS가 더 안전하다고 제안했습니다.

일부 사용자들은 Raspberry Pi 또는 Mac Mini에서 쉽게 실행할 수 있는 설정에 비용을 지불하는 것에 의문을 제기했습니다.
- Claude, Claw 액세스 차단; 커뮤니티 불만 표출!: 사용자들은 토큰을 통해 Claude를 사용하는 것이 차단되었다고 보고했으며, 이는 불만으로 이어져 Gemini 3.1 Pro와 같은 대안을 모색하게 되었습니다.

Anthropic의 API 사용 정책, 가격 책정, 그리고 앱 외부에서 보조금을 받은 토큰에 대한 접근 제한에 대해 논쟁이 발생했습니다.
- Qwen, 뛰어난 품질로 쿼리 갈증 해소; Alibaba의 에이스, AI Arena를 평정하다!: 커뮤니티는 Alibaba의 코딩 플랜을 통해 제공되는 Qwen 3.5를 Kimi와 GLM을 능가하는 비용 효율적인 대안으로 극찬하고 있습니다.

일부 사용자들은 Alibaba Cloud UI가 혼란스럽다고 느꼈고, OpenClaw와 함께 사용할 경우 잠재적인 TOS 위반 가능성에 대해 경고했습니다.
- OpenPad 앱, OpenClaw를 iPad로 가져오다: 한 멤버가 iPad의 M2 프로세서를 활용하여 로컬 모델로 OpenClaw와 같은 것을 iPad에서 실행하는 OpenPad 앱을 개발하고 있습니다.

이 프로젝트는 GitHub에 있으며 MLX를 사용하고 있으며, 다른 사람들이 도움을 주거나 부분적으로 작동하는 앱을 다운로드하도록 초대하고 있습니다.
- Google Gemini 계정 접근 권한 박탈!: 한 사용자는 활성 Google AI Pro 구독 중임에도 불구하고 Gemini CLI를 통해 단 10개의 프롬프트만 사용한 후 Google 계정이 잠겼다고 보고했습니다.

이는 Google의 인증 허브에 의존하는 것의 위험성과 '탈(脫) Google'의 필요성에 대한 논의를 촉발했습니다.

---

## BASI Jailbreaking Discord
- 자율형 Jailbreak 프록시, 잠들지 않는다: 한 멤버는 OpenClaw를 사용하여 DeepSeek-R1으로 쿼리를 평가하고 Claude, GPT, Gemini, Grok과 같은 모델에 대해 스텔스 다중 턴 Jailbreak를 통해 라우팅하는 자체 호스팅 자율 프록시를 VPS에서 운영하고 있습니다.

이 프록시는 공격자 풀을 사용하고 새로운 추론 모델과 Jailbreak 방식을 가져와 수동 개입 없이 높은 성공률을 유지하며 자체 업데이트되도록 설계되었습니다.
- Jailbreak 프록시 제안, 혹평받다: 동료 검토에서는 Anthropic, OpenAI, Google, xAI와 같은 플랫폼 전반의 Terms-of-Service 위반으로 인해 상당한 법적 및 정책적 노출이 발생하며, 이는 계정 정지 또는 법적 조치로 이어질 수 있다고 지적했습니다.

압수된 VPS 로그가 Jailbreak 기록을 노출할 위험, 자동 실행되는 서드파티 모델로 인한 공급망 익스플로잇, 그리고 결함 있는 업데이트에 대한 롤백 계획의 부재에 대한 추가적인 우려가 제기되었습니다.
- Grok, 여전히 Jailbreak의 열쇠를 쥐고 있다: 멤버들은 Grok과 ChatGPT를 Jailbreak하는 가장 효과적인 프롬프트에 대해 논의했으며, Grok 프롬프트만이 효과적이라는 합의가 이루어졌습니다.

이미지 생성 및 스크립팅을 위한 Gemini Jailbreak 프롬프트 생성 시도는 성공적이지 못했습니다.
- Gemini Canvas Jailbreak, 그림자 속에서 등장하다: 한 멤버는 인터랙티브 디자인 채널에서 영감을 받아 ENI Jailbreak 프롬프트의 수정된 버전으로 생성된 Gemini Canvas를 공유했습니다.

이 Jailbreak 프롬프트는 Gemini 3 Pro, Claude Opus 4.6, ChatGPT 5.3과 같은 주요 LLM에서 보편적으로 작동한다고 주장됩니다.
- 디지털 위생 스쿼드 소집: 한 멤버는 Tails OS와 같은 보호 조치를 권장하며, 디지털 위생 및 보안을 위한 기본 수준의 모범 사례에 대한 커뮤니티 디자인을 만들 도움을 요청했습니다.

이 멤버는 다른 사람들을 위한 구역을 만들고 더 나은 관행을 통합하기 위해 노력하고 있으며, YouTube와 AI 지원을 통해 이 환경을 탐색하는 데 따르는 어려움을 인정하고 있습니다.

---

## Perplexity AI Discord
- Perplexity Computer: 모든 것을 지배할 하나의 시스템?: 이 트윗에 따르면, Perplexity Computer는 현재의 모든 AI 기능을 하나의 시스템으로 통합하여 어떤 프로젝트든 엔드투엔드로 연구, 설계, 코딩, 배포 및 관리할 수 있습니다.

초기에는 Max 구독자에게만 제공되었지만, 일상 사용자들을 위한 실용적인 적용 가능성과 기존 AI 도구와의 가치 비교에 대해서는 현재 회의적인 시각이 많으며, 멤버들은 "Perplexity MAX는 너무 비싸요, 형씨."라고 의문을 제기하고 있습니다.
- Perplexity Pro 사용자들, 이미지 업로드 제한에 분노하다: 사용자들은 구독료를 지불하고 있음에도 불구하고 Perplexity Pro의 최근 이미지 업로드 제한에 대해 불평하고 있으며, 일부는 Gemini와 Claude와 같은 대체 AI 플랫폼을 고려하고 있습니다.

한 사용자는 내일 시험이 있는데 금요일까지 기다려야 제한이 초기화된다고 주장했으며, 다른 사용자는 "하루에 10장도 못 올린다고요????"라고 말했습니다.
- Gemini Pro와 Perplexity Pro, 정면 대결!: 멤버들은 Gemini Pro가 Perplexity Pro보다 우월한지 여부를 논쟁하며, NotebookLM 및 Google Workspace 통합과 같은 Gemini Pro의 기능을 강조하고 있습니다.

한 멤버는 "학생으로서 NotebookLM과 Google Workspace 통합 및 생성, 특히 2TB 클라우드 스토리지를 통해 훨씬 더 많은 가치를 얻을 수 있습니다"라고 말했지만, 다른 사용자들은 Gemini Pro의 컨텍스트 제한이 Perplexity만큼 관대하지 않다고 느끼고 있습니다.
- 멤버들, 코딩을 위해 Claude, Gemini, GPT를 비교하다: 멤버들은 코딩 작업에 대한 다양한 AI 모델의 장단점을 논의하며, Claude는 백엔드에 가장 강력하고, Gemini는 프론트엔드/UI에, GPT는 그 중간 옵션으로 간주하고 있습니다.

Claude의 높은 토큰 사용 비용은 우려 사항이며, 한 사용자는 "Claude를 사용해봤는데, 단일 PDF를 분석하는 데 한 시간 만에 한 달치 토큰을 전부 날렸습니다"라고 말했습니다.
- 신비로운 Lovable Apps 링크 등장: alfastudiox.lovable.app, ollamaagentalfa.lovable.app, 그리고 alfastudiox.lovable.app (반복됨)과 같은 lovable.app 서브도메인 세 개의 링크가 공유 채널에 공유되었습니다.

링크와 함께 제공된 컨텍스트나 논의가 없었기 때문에 그 목적은 불분명하지만, 잠재적인 새로운 프로젝트나 리소스를 시사합니다.

---

## Unsloth AI (Daniel Han) Discord
- Qwen3.5 모델, 빠르지만 장황하다: 열성적인 사용자들은 Qwen3.5 35B 및 27B 모델의 구조화된 사고를 칭찬했지만, LM Studio에서 Gemma 또는 Olmo 3.1에 비해 속도가 느리다고 언급했으며, 멤버들은 Qwen3.5 122B A10B 모델이 매우 장황한 출력을 생성하는 경향이 있지만, presence penalty를 조정하여 완화할 수 있음을 발견했습니다.

presence penalty를 적절히 사용하면 122B 모델로 유용한 코딩이 가능하며, 이 정보를 공식 가이드에 포함해야 한다는 제안이 나왔습니다.
- 9줄짜리 스네이크 게임, 코더들을 매료시키다: 한 멤버가 세미콜론 없이 9줄로 구현된 Python Snake 게임을 공유하여 코드 최적화 및 대체 접근 방식에 대한 논의를 촉발했습니다.

다른 사용자들은 walrus 연산자와 람다를 사용하는 등 줄 수를 더 줄이는 방법에 대해 논의했습니다.
- Xcode, 번역 앱을 얻다: 한 멤버는 이 비디오에서 보여진 것처럼 Xcode에 자신만의 시스템 수준 번역 앱을 만들 수 있는 멋진 기능들을 발견했습니다.

하지만 이는 iOS 및 iPadOS 전용이며, 한 멤버는 "Apple은 역대 최고의 회사니까요!"라며 더 많은 재미를 위해 자신의 모델을 추가할 계획입니다.
- 새로운 Minecraft 모델 출시: 한 멤버가 다음 Minecraft 플레이 모델인 Andy-4.1을 Hugging Face에 공개했습니다.

다른 멤버는 "정말 멋지네요!!"라고 외치며 작동하는 모습을 보여달라고 요청했습니다.

---

## LMArena Discord
- Gemini 3 Pro 이미지 미리보기 수정 방법 발견: 사용자들은 프롬프트 앞에 "다음 이미지를 다음 내용으로 수정하세요: (프롬프트)"를 추가하면 Gemini 3 Pro 이미지 미리보기가 활성화된다는 것을 발견했지만, 일부는 오류를 보고했습니다.

다른 사용자들은 여전히 Gemini 3.1 이미지 미리보기가 '응답에 문제가 발생했습니다. 다시 시도해주세요'라는 오류를 반환한다고 보고했습니다.
- Video Arena 봇, 활동 증가에도 불구하고 제거되다: Video Arena 봇은 Discord 봇의 한계를 넘어 기능 확장을 허용하기 위해 제거되었지만, 제거 후 서버 활동은 증가했습니다.

한 멤버는 사람들이 봇에 대해 묻는 것을 멈추려면 2028년 중반까지 걸릴 것이라고 농담했습니다.
- Opus 4.6의 가치, 코딩 챌린지 속에서 논의되다: 한 벤치마크는 Gemini 3.1을 가장 높은 가치로 평가한 반면, Opus 4.6은 높은 비용과 환각 문제로 인해 낮은 가치 점수를 받았습니다.

그럼에도 불구하고, 한 사용자는 코딩 챌린지에서 Opus 4.6을 사용하여 Gemini의 버그를 수정했습니다.
- Grok 4.20 beta1, Search Arena를 지배하다: Grok-4.20-Beta1은 1226점으로 Search Arena 리더보드에서 GPT-5.2와 Gemini-3를 능가하며 선두를 차지했습니다.

또한 Text Arena 리더보드에서 1492점으로 4위를 차지했으며, 이는 Gemini 3.1 Pro와 동등한 수준입니다.
- Qwen 3.5 모델, Arena에 데뷔하다: qwen3.5-27b, qwen3.5-35b-a3b, qwen3.5-122b-a10b를 포함한 새로운 Qwen 3.5 모델들이 이제 Text 및 Vision Arena와 Code Arena에서 사용할 수 있습니다.

이 모델들은 Arena 환경 내에서 코드, 텍스트, 비전 작업에 대한 옵션을 확장합니다.

---

## OpenRouter Discord
- OpenRouter의 인증 계층, 인프라 문제로 중단되다: 사후 분석 결과, 지난주 2월 17일과 19일의 서비스 중단은 상위 인프라 제공업체 장애가 OpenRouter의 인증 계층으로 연쇄적으로 영향을 미쳐 일부 사용자에게 401 오류를 발생시켰음이 밝혀졌으며, 자세한 내용은 여기에서 확인할 수 있습니다.

구체적인 예방 조치는 공개되지 않았지만, OpenRouter는 향후 유사한 장애를 피하기 위한 조치를 구현했다고 주장합니다.
- Packet.ai, Blackwell GPU로 강력한 성능 제공: Packet.ai는 이제 AI 워크로드용 Blackwell GPU를 학습에 시간당 $0.66 또는 월 $199에 제공합니다.

이러한 개발자 친화적인 GPU 클라우드는 AI 워크로드를 위한 저렴한 솔루션을 제공하여 접근성을 높이고 비용을 절감하는 것을 목표로 합니다.
- Deepseek R1, 제거되다: 무료 Deepseek R1 0528 모델이 제거되면서, 플랫폼에서 무료 모델의 지속 가능성에 대한 논의가 촉발되었습니다. 이러한 모델은 종종 나타났다가 사라지기 때문입니다.

한 사용자는 Jai gooners에 의해 과부하가 걸렸다고 농담했지만, 다른 사용자들은 놀라지 않는 듯했습니다.
- 유출된 키, 차지백 위협을 촉발하다: 한 사용자는 API 키 유출로 인해 무단 사용이 발생했으며, 지원 응답 부족으로 인해 차지백을 위협했다고 보고했습니다.

커뮤니티 멤버들은 사용자에게 조언을 제공하면서도 사용자의 보안 관행에 의문을 제기했고, 이는 격렬한 논쟁으로 이어졌으며, 사용자는 결국 차지백을 시작했다고 선언한 후 서버를 떠났습니다.
- Anthropic, 미국 정부의 요청에 응답하다: Axios와 Reuters는 내부 분쟁에도 불구하고 Anthropic이 국방부와 협력하고 있다고 보도했습니다.

한 멤버는 어떤 문제든 '국가 안보 문제'로 포장될 것이라고 농담했습니다.

---

## LM Studio Discord
- LM Link, 로컬 LLM을 원격으로 활용하다: LM Studio 팀은 Tailscale과 협력하여 LM Link를 출시했으며, 이를 통해 사용자는 다른 기기에서 로컬 LM Studio 서버에 연결할 수 있습니다. 초기 설정 중 404 오류 보고가 있었으나 빠르게 해결되었으며, LM Link에 대한 자세한 내용은 여기에서 확인할 수 있습니다.

사용자들은 휴대폰에서 LLM 접근을 가능하게 하는 LM Link 모바일 앱과, 계정이나 서드파티 없이 직접 연결을 위한 로컬 연결 옵션을 요청했습니다.
- LM Studio 업데이트, llama.cpp를 망가뜨리다: 사용자들은 4.4 업데이트 후 LM Studio 실행 문제와, 최근 릴리스에서 자체 컴파일한 후 llama.cpp가 Qwen3.5 모델을 로드하지 못하는 문제를 보고했습니다. 릴리스 8145로 다운그레이드하여 해결되었습니다.

이 오류는 GGUF 헤더 및 메모리 할당과 관련된 호환성 파괴 변경(breaking change) 때문이었으며, git의 최신 빌드가 Qwen3.5 및 다른 모델의 헤더를 읽지 못하여 메모리 부족 오류로 이어졌습니다.
- Qwen3.5, Jinja 템플릿 문제에 직면하다: 사용자들은 서버에서 Qwen3.5 모델을 실행하는 데 어려움을 겪었으며, Jinja 템플릿 및 누락된 사용자 쿼리와 관련된 오류를 경험했습니다. 모델이 lmstudio-community에서 다운로드되었는지 확인한 후 문제가 해결되었습니다.

다른 사용자들은 Qwen3.5의 작성 스타일과 검열에 대해 탐구했으며, 일부는 이전 Qwen 모델에 비해 콘텐츠 필터링이 증가했음을 발견했고, 이는 '사고(thinking)'를 끄면 해결될 수 있다고 했습니다.
- OpenClaw, 우려를 자아내다: 멤버들은 시스템 접근 권한을 가진 AI 에이전트인 OpenClaw 사용의 잠재적 위험에 대해 논의했으며, 한 사용자는 요청 후 휴지통 폴더를 지웠다고 회상하며, 이를 멀웨어로 분류될 수 있다는 우려를 제기했습니다.

이 논의는 OpenClaw를 Jarvis 및 Gideon과 같은 다른 AI 비서와 비교하며, 잠재적인 보안 위험 때문에 AI에 완전한 시스템 권한을 부여하는 것에 대해 경고했습니다.
- MoE 모델, 메모리 점유율이 높다: 논의는 Mixture of Experts (MoE) 모델과 이를 수용하기 위한 상당한 RAM 요구 사항에 초점을 맞추었으며, 현재 하드웨어 접근 방식의 실현 가능성에 대한 우려를 제기했습니다.

멤버들은 시스템 RAM이 LLM에서 컨텍스트만을 위해 효과적으로 사용될 수 있는지, 아니면 필연적으로 속도 저하를 유발할지에 대해 논쟁했지만, 합의는 거의 이루어지지 않았습니다.

---

## OpenAI Discord
- 에이전틱 스타트업, 로딩 상태를 재정의하다: 한 트윗은 '로딩 중...' 상태를 '생각 중...'으로 변경하여 에이전틱 AI 스타트업이 되는 것에 대해 농담했습니다.

이는 AI 분야에서 '생각하는' 과정을 가진 모든 것을 에이전틱으로 분류하는 경향을 풍자합니다.
- Sonnet, 표절 의혹에 직면하다: 멤버들은 Sonnet이 Deepseek에서 도용/학습되었다는 주장에 대해 논의했으며, Elon이 제기한 유사한 비난을 언급했습니다.

이 논의는 AI 산업에서 지적 재산권 및 학습 데이터 출처에 대한 지속적인 우려를 강조합니다.
- Seedance 2.0, 콘텐츠 위반으로 일시 중지되다: Sora 2와의 콘텐츠 위반이 CHINESE 모델로 약속된 후, 저작권 문제가 Seedance 2.0의 글로벌 출시를 지연시키고 있습니다.

사용자들은 향후 유사한 문제를 피하기 위해 오픈소스 모델만 사용할 것을 주장하고 있습니다.
- 할리우드, AI 저작권을 압박하다: 영화 스튜디오들은 이 모든 것이 오픈소스로 제공될 것을 예상하며 기업들을 고소함으로써 '젖소의 젖을 짜고 있다'고 합니다.

이 소송들은 저작권법 하에서 AI 생성 콘텐츠가 어떻게 처리될지에 대한 선례를 세울 수 있습니다.
- AI CEO, 책임감이 부족하다: 기업들은 AI로 직원을 대체하는 것은 기술적으로 쉽지만, 책임감을 대체하는 것은 쉽지 않다는 것을 발견했습니다.

일이 잘못되었을 때 인간에게 책임을 돌릴 수 없는 결정을 내리는 AI CEO를 아무도 원하지 않습니다.

---

## Latent Space Discord
- Swyx, 링크를 대량으로 공유하다: Swyx는 OpenAI와 Langchain의 게시물을 포함하여 X 게시물에 대한 수많은 링크로 구성된 "swyx plane dump"를 공유했습니다.

공유된 다른 링크에는 @dejavucoder, @zerohedge 및 기타 여러 계정의 게시물이 포함되었습니다.
- Scoble의 암호화폐 비상사태: Robert Scoble은 가장 친한 친구의 퇴거를 위한 자금을 확보하기 위해 자신의 이름으로 생성된 토큰에서 Ethereum을 수집하는 봇을 사용했음을 확인하며 YouTube 비디오를 링크했습니다.

Scoble은 자신의 비상 이체를 언급했으며, 과거 Discord 메시지(파트 1 & 2)도 링크했습니다.
- AMD 워런트, 지분 리베이트로 활용되다: 대규모 거래 분석 결과, OpenAI와 Meta가 총 1억 6천만 AMD 주식에 대한 워런트를 보유하고 있으며, 이는 600달러 주가 목표 및 상당한 미래 GPU 지출과 연계된 지분 리베이트 역할을 하는 것으로 나타났습니다.

이 워런트는 잠재적으로 1,920억 달러의 가치를 가질 수 있습니다 (https://xcancel.com/ai/status/2026396297540858360?s=12).
- LLM 시스템 디버깅의 진짜 원인: 한 멤버는 LLM 기능이 데모 후 실패할 때, 문제가 모델 자체보다는 리트리벌 로직, 토큰 소모, 오케스트레이션 또는 백엔드 아키텍처에서 비롯되는 경우가 많다고 강조합니다.

그들은 출시를 위해 복잡한 LLM 시스템을 안정화하는 데 전문성을 가지고 있으며, 이는 이론적인 모델 개선보다는 실용적인 실제 애플리케이션에 중점을 둔다는 것을 나타냅니다.
- Anthropic, 해석 가능성 엔지니어 채용 중: Chris Olah는 이 트윗에서 Anthropic이 해석 가능성 팀을 위해 약 10명의 연구 엔지니어를 찾고 있다고 발표했습니다.

이 역할은 모델 내부에 관심 있는 숙련된 ML 인프라 엔지니어를 대상으로 하며, 이전 해석 가능성 경험은 필요하지 않습니다.

---

## Nous Research AI Discord
- Hermes Agent: 오픈소스 에이전트 데뷔: Nous Research는 다단계 메모리 시스템과 영구적인 전용 머신 접근 기능을 갖춘 오픈소스 에이전트인 Hermes Agent를 출시했습니다. 이 에이전트는 사용자와 함께 성장하도록 설계되었으며, `curl -fsSL https://raw.githubusercontent.com/NousResearch/hermes-agent/main/scripts/install.sh | bash`를 통해 설치할 수 있습니다.

Hermes Agent는 OpenRouter 및 Nous Portal 구독을 통해 구동되며, CLI 통합 및 메시징 플랫폼 지원을 제공하고, portal.nousresearch.com에서 쿠폰 코드 HERMESAGENT를 사용하는 선착순 750명의 신규 가입자에게 한 달 무료 프로모션을 제공합니다.
- Atropos, 에이전틱 RL 파이프라인으로 강화되다: Hermes Agent는 Hermes Agent 프리미티브를 사용하여 RL을 가능하게 하도록 Atropos를 확장하며, 대규모 데이터 생성을 즉시 지원합니다.

GitHub 리포지토리에 따르면, 이는 고급 에이전틱 기능, 서브에이전트 제어, 프로그래밍 방식 도구 호출, 고급 파일 시스템/터미널 제어, 에이전트 관리 기술 및 브라우저 사용 기능을 갖추고 있습니다.
- Qwen 모델 가중치 출시: Qwen은 Hugging Face에서 Qwen3.5-35B-A3B 모델의 기본 가중치를 출시했습니다.

이 움직임은 커뮤니티에서 환영받았습니다.
- Codex 5.3, 가격 책정 및 API 준비 완료: Codex 5.3은 새로운 가격 구조로 API에서 사용할 수 있습니다: 입력에 $1.75, 출력에 $14입니다.

커뮤니티는 비용 대비 성능을 평가하고 있습니다.
- Steinberger의 OpenClaw: AI 바이브 추출: Steinberger는 자신의 이전 계획과 아이디어, 코드 스니펫에서 AI를 통해 추출된 후 OpenClaw가 어떻게 만들어졌는지 설명하는 비디오를 출시했습니다.

그는 자신의 소프트웨어가 무엇을 하는지 모르며, 그 구조는 단지 채널들의 스택일 뿐이라고 말했습니다.

---

## Eleuther Discord
- Pythia-2.8b 체크포인트 버그, 조사 촉발: 한 멤버는 Hugging Face의 pythia-2.8b 체크포인트에서 버그를 보고했습니다. 이 버그는 리비전과 관계없이 동일한 가중치가 제공되었으며, pytorch_model.bin 및 model.safetensors의 SHA256 해시가 다른 단계에서도 동일했습니다.

pythia-2.8b의 샤딩된 safetensors 파일은 단계마다 다르지만, 샤딩되지 않은 파일은 동일하다는 점이 언급되었으며, 이는 HF가 모델을 로드하고 샤딩을 처리하는 방식에 대한 논의를 촉발했습니다.
- EleutherAI, 중복 제거된 모델 라벨링 수정: EleutherAI는 잘못 표시된 14m 및 30m 모델(중복 제거된 버전)의 라벨링을 수정하고 있으며, 이를 대체할 중복된 모델을 학습시키고 있습니다.

한 멤버는 일부 업로드 혼동 문제를 해결했으며, 라벨링 불일치를 해결하기 위해 밤새도록 수정 작업을 실행했다고 언급했습니다.
- Sesame AI 음성 모델, 화제를 모으다: 한 멤버는 Sesame AI 음성 AI 모델에 대해 문의하며, 그 명백한 정렬과 Gemma 모델 기반으로 추정되는 점을 강조했습니다.

다른 멤버는 Sesame AI가 ASR, LLM, TTS를 통합하는 저지연 음성 시스템에 중점을 둔다고 언급했으며, 통찰력을 얻기 위해 Moshi 논문을 참조할 것을 제안했습니다.
- 확산 모델 연구, 뜨거워지다: 멤버들은 Latent Diffusion Model 이후의 확산 논문들을 검토하며, Rectified Flows, Flow Matching, Diffusion Forcing을 언급했습니다.

ByteDance Seed와 Hunyuan의 논문들(예: https://arxiv.org/abs/2509.20427, https://arxiv.org/abs/2509.23951)도 인용되었으며, 추천 YouTube 재생 목록이 자료로 공유되었습니다.
- vLLM 백엔드, lm-eval Harness 속도 향상: 한 멤버는 lm-evaluation-harness에서 vLLM 백엔드를 사용하여 단일 토큰 답변을 가진 다지선다형 작업의 평가 속도를 높이는 풀 리퀘스트에 대한 검토를 요청했습니다.

이 속도 향상은 HF 백엔드에 비해 느린 문제를 해결할 것으로 예상되며, 특히 MMLU pro eval과 같은 작업에 유용할 것입니다.

---

## HuggingFace Discord
- Gradio 버전, ZeroGPU 할당 문제 유발: 사용자들은 ZeroGPU 할당 문제를 보고했으며, 이는 5.12.0 이전 Gradio 버전의 로그인 버그와 관련이 있을 수 있습니다.

컨테이너 로그를 확인하면 Gradio, spaces 라이브러리 또는 HF 서버가 문제의 원인인지 밝혀낼 수 있으며, 빈 커밋 후 재빌드하는 것도 버전 관련 문제를 해결할 수 있습니다.
- 독립 개발자, 엄청난 엣지 메모리 장벽을 돌파하다: 한 독립 개발자는 MiniMax-m2.5의 5GB MoE 샤드를 2MB 벡터 양자화 잠재 공간으로 압축했다고 주장합니다.

그들은 arXiv (cs.LG)에 제출할 논문을 준비 중이며, 자신들의 "블랙 매직 엣지 AI 기술"을 검토해 줄 추천인을 찾고 있습니다.
- Zagora, 분산 파인튜닝 시스템 구축: Zagora의 한 멤버는 표준 인터넷을 통해 70B+ 모델을 학습시키기 위한 분산 파인튜닝 시스템을 구축하고 있으며, 분산된 GPU를 GPT-OSS, Qwen 2.5, Mistral을 지원하는 통합 학습 슈퍼컴퓨터로 전환하고 있다고 발표했습니다.

이 플랫폼은 이제 Petals와 SWARM Protocol에서 영감을 받은 파이프라인 스타일의 학습 접근 방식을 사용합니다.
- webXOS, Black Hole Time-Lapse Dataset 출시: 한 멤버는 webxOS의 Three.js 시뮬레이션으로 생성된 중력 렌즈 효과가 있는 합성 블랙홀 렌더링을 포함하는 webXOS Black Hole Time-Lapse Dataset을 공유했습니다.

각 샘플에는 PNG 이미지의 타임랩스 시퀀스와 관련 물리적 매개변수가 포함되어 있어 멀티모달 모델 학습, 물리학 기반 ML 또는 위성 이미지 연구 비유에 이상적입니다.
- HF Agents Course, 채널 통합: Hugging Face 에이전트 코스에 새로 참여한 사람들은 코스 자료에 언급된 특정 채널을 찾는 데 어려움을 겪고 있으며, 채널들이 단일 채널로 통합된 것으로 보입니다.

한 멤버는 agents-course 리포지토리의 PR #653을 링크했습니다.

---

## GPU MODE Discord
- SMEM 충돌, 비동기 방식에서는 관련 없을 수도: 한 사용자는 GMEM에서 SMEM으로 데이터를 전송하기 위해 cuda::memcpy_async를 사용할 때 SMEM 뱅크 충돌이 중요한 문제인지 문의했습니다.

사용자는 SMEM 뱅크 충돌이 주로 SMEM에 대한 워프 접근과 관련이 있으며, 이 시나리오에서는 큰 문제가 아닐 수 있다고 가정했지만, 추가적인 관점을 구했습니다.
- PyTorch에서 FA3 커널이 FA2를 재정의하다: 사용자가 activate_flash_attention_impl(“FA3”)를 호출하면, restore_flash_attention_impl이 호출되어 기본 FA2 커널이 복원될 때까지 디스패치 테이블에서 기본 FA2 커널이 FA3 커널로 재정의됩니다.

이는 버전 이름을 호출 가능한 함수에 매핑하는 딕셔너리에 키-값 쌍 {“FA3”, register_fn}을 추가하고, register_fn(여기서 정의됨)을 실행하여 PyTorch 디스패처에 FA3 커널을 등록함으로써 달성됩니다.
- B200 GPU 가격, 사용자들을 리스로 유도하다: 한 사용자는 B200 GPU가 엄청나게 비싸다고 언급하며, 비기업 사용자, 특히 Lightning AI Clusters의 경우 리스 또는 렌탈이 더 실현 가능한 옵션이라고 조언했습니다.

B200 GPU의 높은 비용을 고려할 때, 한 사용자는 특히 기업 환경 외부의 사람들을 위해 Neocloud 리스 또는 렌탈 옵션을 탐색할 것을 제안합니다.
- 커널 최적화 RL 환경, 관심을 끌다: 한 멤버는 커널 최적화를 위한 RL 환경에 관심을 표명했으며, 공통 인프라 구축을 제안했습니다.

이 대화는 #popcorn 채널에서 이루어졌으며, 주어진 메시지에는 추가적인 세부 사항이나 특정 논의가 강조되지 않았습니다.
- Serenade, 각 언어의 장점을 결합하다: 한 멤버는 C++, CUDA, x86-64 ASM으로 트랜스파일되는 새로운 언어인 Serenade를 소개했습니다. 이 언어는 Python만큼 간단하면서도 수동 메모리 관리로 C++만큼 빠르도록 목표합니다.

이 언어는 GPU 커널 지원(serenaCore, 커스텀 BLAS 커널)과 단일 패스 컴파일 시스템을 통한 통합 Dear ImGui 지원을 포함하며, 이를 사용하여 운영 체제를 만들 계획입니다.

---

## Moonshot AI (Kimi K-2) Discord
- Kimi, GLM에 우위 주장: 사용자들은 Kimi와 GLM 5를 비교했으며, 한 사용자는 Kimi가 100,000배 더 빠르다고 주장했습니다.

다른 사용자는 GLM 5가 약간 우위에 있지만, 다른 제공업체를 사용하지 않는 한 공식 z.AI API를 통해서는 더 느리다고 언급했습니다.
- 에이전트 할당량 우려: 한 사용자는 Allegro의 비용 문제를 언급하며 에이전트 할당량을 충전하는 것에 대해 문의했습니다.

그들은 또한 nb pro와 함께 제공되는 에이전트 docsis kimi 슬라이드가 더 이상 무료가 아니라고 언급했습니다.
- Kimi, 코딩 왕관을 차지하다: 한 사용자는 MiniMax와 Alibaba의 코딩 플랜을 각각 테스트한 후 코딩에 Kimi를 선호했습니다.

사용자는 속도, 가동 시간, 사용량 제한 및 모델 품질을 주요 결정 요인으로 꼽았습니다.
- KimiClaw, 브라우저에서 어려움 겪다: 한 사용자는 KimiClaw가 브라우저를 독립적으로 탐색하지 못하는 문제를 보고하며 "대용량 파일을 분석/처리할 때 컨텍스트를 줄이고 토큰을 절약하기 위해 Kimi에 무엇을 사용할 수 있을까요? Claude에 그런 기능이 있는 것 같아요."라고 물었습니다.

사용자는 커뮤니티에서 해결책을 찾았고, Claude가 대용량 파일 분석 시 컨텍스트 감소를 위한 더 나은 도구를 가지고 있는지 궁금해했습니다.

---

## Manus.im Discord Discord
- Github 재연결, 난관에 봉착하다: 한 멤버는 Github 계정을 재연결하는 데 어려움을 겪고 있으며, 대신 새 리포지토리를 만들라는 메시지가 표시됩니다.

이 멤버는 비코더 배경으로 인해 간단한 지침의 필요성을 강조합니다.
- 로컬 개발자들, OAuth 환경 변수 탐색: 한 멤버는 로컬 앱 개발을 위해 VITE_APP_ID, OAUTH_SERVER_URL, VITE_OAUTH_PORTAL_URL 환경 변수를 얻는 방법에 대한 지침을 요청합니다.

그들은 또한 로컬 개발 중 redirectUri http://localhost:3000/api/oauth/callback을 허용하기 위해 OAuth 구성이 필요한지 문의합니다.
- 계정 생성, 즉시 차단으로 이어지다: 한 멤버는 계정 생성 후 즉시 차단되었다고 보고하며, 이 문제를 해결하는 방법에 대한 조언을 구합니다.

어떠한 조언도 제공되지 않았습니다.
- Manus, 쿠키 문제의 원인을 인프라 탓으로 돌리다: 한 멤버는 사용자 지정 도메인(anointedforai.com)에서 쿠키 문제로 인해 Manus가 리다이렉트 루프에 갇힌다고 보고합니다.

Manus 지원팀은 이 문제를 인프라/호스팅 문제로 진단하고, 지원팀에 문의하거나 Manus에서 마이그레이션할 것을 제안했습니다.
- Manus 웹사이트 디자인 불만: 한 멤버는 Manus로 만든 웹사이트 디자인을 "엉망"이라고 비판하며 수정을 위한 도움을 요청합니다.

다른 멤버는 다이렉트 메시지를 통해 돕겠다고 자원했습니다.

## aider (Paul Gauthier) Discord
- Aider, 더 빠른 편집을 위한 /ok 별칭 추가: Aider의 메인 브랜치는 이제 빠른 코드 수정을 위해 설계된 `/code Ok, please go ahead and make those changes.` 명령의 단축키로 `/ok`를 지원합니다.

새로운 별칭은 Aider가 제안한 변경 사항을 승인하고 구현하는 과정을 간소화하여 개발자 워크플로우 효율성을 개선하는 것을 목표로 합니다.
- Aider 사용자, 경제적인 LLM을 찾다: 한 사용자가 Gemini를 사용하면서 토큰 예산을 빠르게 소진하여 비용이 많이 들었던 경험 후, Aider와 함께 사용할 비용 효율적인 LLM을 찾고 있습니다.

단일 제공업체의 API를 직접 다루는 대신, OpenRouter를 사용하여 다양한 모델 간에 동적으로 전환하여 비용과 성능을 최적화하는 것이 제안되었습니다.
- Deepseek V3.2, Aider에 가장 적합한 모델: 사용자들은 Deepseek V3.2를 Aider와 함께 사용할 견고한 기본 LLM으로 제안하며, 가끔 느리다는 점에도 불구하고 뛰어난 추론 능력과 저렴한 비용을 언급합니다.

이 모델이 복잡한 추론 작업을 효율적으로 처리하는 능력은 균형 잡힌 성능과 비용을 찾는 Aider 사용자들 사이에서 인기를 얻게 합니다.
- Xiaomi/mimo-v2-flash: Aider의 빠른 편집기: Xiaomi/mimo-v2-flash는 Aider 내에서 퍼지 검색 및 교체, 또는 내용 완성 등 기본적인 파일 편집 작업에 대한 숙련도를 인정받고 있습니다.

그 속도와 비용 효율성은 간단한 편집 작업에 이상적인 선택이 되게 하며, 더 복잡한 작업을 위한 다른 모델들을 보완합니다.
- Aider 파워 콤보: 계획에는 kimi-k2.5, 편집에는 mimo-v2-flash: Aider에서 더 어려운 문제에 대해 moonshotai/kimi-k2.5를 계획 모델로, mimo-v2-flash를 편집 모델로 사용하는 조합이 권장됩니다.

이 조합은 각 모델의 강점을 활용하여 kimi-k2.5는 강력한 계획 능력을 제공하고 mimo-v2-flash는 효율적이고 빠른 편집을 제공하여 더 복잡한 문제를 효과적으로 해결합니다.

---

## MLOps @Chipro Discord
- WeAreDevelopers Congress, 북미로 확장: WeAreDevelopers World Congress North America는 2026년 9월 23일부터 25일까지 캘리포니아 산호세에서 개최되며, 10,000명 이상의 개발자와 500명 이상의 연사를 예상하고 대규모 실용 엔지니어링에 중점을 둡니다. 자세한 내용은 wearedevelopers.us에서 확인할 수 있습니다.

주요 내용은 분산 시스템 스케일링, API 플랫폼, 그리고 DevOps를 다룰 예정입니다. 코드 Community_MLOps를 사용하면 10% 할인을 받을 수 있습니다.
- Apart Research, AI Control Hackathon 공개: Apart Research는 Redwood Research와 협력하여 2026년 3월 20일부터 22일까지 AI Control Hackathon을 개최하며, AI가 우리가 의도한 대로 작동하도록 보장하는 시스템에 중점을 둡니다.

이 해커톤에는 ControlArena 벤치마크 챌린지, 제어 프로토콜 설계, 그리고 레드 팀 활동이 포함되며, $2,000의 상금과 ControlConf 여행이 제공됩니다.
- ControlConf 여행, 해커톤 상금의 주요 내용: AI Control Hackathon의 대상은 항공편과 호텔을 포함하여 ControlConf Berkeley (4월 18-19일) 여행을 포함합니다.

자세한 내용은 ControlConf를 참조하십시오.

---

## DSPy Discord
- 프로덕션 환경의 DSPy, 샌프란시스코 밋업에서 조명되다: 또 다른 샌프란시스코 DSPy 밋업이 발표되었으며, 프로덕션 사용 사례의 DSPy와 RLM에 중점을 둡니다. Luma 링크를 참조하십시오.

Dropbox와 Shopify의 엔지니어들이 dspy.RLM에 대한 상세 설명을 포함한 사례 연구를 공유할 예정입니다.
- Dropbox와 Shopify 엔지니어들, DSPy 행사에서 한자리에 모이다: Dropbox와 Shopify 엔지니어들은 다가오는 샌프란시스코 DSPy 밋업에서 사례 연구를 발표할 예정입니다.

발표는 프로덕션 환경에서의 DSPy와 RLM의 실제 적용 사례에 중점을 둘 것입니다.

---

## tinygrad (George Hotz) Discord
- Hotz, JAX의 함수 설계 칭찬: Tinygrad의 개발자 George Hotz는 트윗에서 JAX의 우수한 함수 설계에 경의를 표하며, Tinygrad 자체 아키텍처에 미칠 영향을 암시했습니다.

후속 트윗은 JAX의 방법론이 함수 설계의 황금 표준이 될 수 있음을 시사하며 그의 입장을 더욱 확고히 했습니다.
- Tinygrad와 JAX, 함수 대결을 펼치다: 딥러닝 프레임워크 분야에서 JAX의 함수 설계는 두드러지며, Tinygrad의 개발자 George Hotz로부터 찬사를 받았습니다. 그는 JAX의 우수성을 인정했습니다.

이러한 인정은 함수 설계의 잠재적인 벤치마크를 시사하며, Tinygrad 내의 유사한 선택에 영향을 미치고 프레임워크의 아키텍처 결정에 대한 논의를 촉발합니다.

---

## Modular (Mojo 🔥) Discord
- Modular, Mojo 모멘트를 찾다: 한 멤버가 놀라운 피드백을 제공하기 위해 Mojo 포럼 게시물을 공유했습니다.

이 요청은 언어 설계 및 명확화가 필요한 영역에 대한 건설적인 피드백을 수집하기 위해 사용자들에게 Mojo를 사용하면서 놀라거나 혼란스러웠던 경험을 공유해 달라고 했습니다.
- 더 많은 Mojo 모멘트: 또 다른 멤버가 명확화가 필요한 영역에 대해 피드백을 요청했습니다.

이 게시물은 사용자들에게 Mojo를 사용하면서 놀라거나 혼란스러웠던 경험을 공유하도록 권장합니다.

---

## MCP Contributors (Official) Discord
- Ezra Klein, 에이전트에 대해 배우다: Ezra Klein은 AI 에이전트에 대해 이 YouTube 비디오에서 배웁니다.

논의에 대한 추가 세부 정보는 제공되지 않습니다.
- AI 에이전트 개요: 이 YouTube 비디오는 AI 에이전트의 개요와 잠재적인 응용 분야를 제공합니다.

이 비디오는 Ezra Klein에게 AI 에이전트 기술의 기능과 함의에 대해 교육하는 것을 목표로 합니다.

---
LLM Agents (Berkeley MOOC) Discord에는 새로운 메시지가 없습니다. 이 길드가 너무 오랫동안 조용했다면 알려주십시오. 그러면 저희가 삭제하겠습니다.

---
Windsurf Discord에는 새로운 메시지가 없습니다. 이 길드가 너무 오랫동안 조용했다면 알려주십시오. 그러면 저희가 삭제하겠습니다.

---
저희 사이트를 통해 수신 동의하셨기 때문에 이 이메일을 받으셨습니다. 이 이메일을 받는 방식을 변경하고 싶으십니까? 이 목록에서 구독을 취소할 수 있습니다.

---

# Discord: 채널별 상세 요약 및 링크

### OpenClaw ▷ #announcements (1 메시지):
4shadowed: @everyone https://fixupx.com/steipete/status/2026474687576916024

---

### OpenClaw ▷ #general (635 메시지🔥🔥🔥):
> OpenClaw, 관리형 설정, AI 기반 혁신, Anthropic의 Claude OAuth, 구성 악몽, KittenTTS
- OpenClaw의 반-셀아웃 입장: 한 사용자가 일부 사람들이 관리형 OpenClaw 설정을 제공하는 것을 발견했고, 이는 한 멤버로부터 강한 불만을 불러일으켰습니다. 그는 토큰 도난, 데이터 프라이버시 침해와 같은 잠재적 위험에 대해 경고하며 단순히 VPS를 사용할 것을 조언했습니다.

일부 사용자들은 Raspberry Pi나 Mac Mini에서 직접 실행하기 쉽다는 점을 고려할 때, 사람들이 관리형 OpenClaw 설정에 비용을 지불하고 있다는 것에 대해 놀라움을 표했습니다.
- Claw 사용자들, 주요 모델 제공업체에 대해 논의하다: 일부 멤버들은 Anthropic의 Claude 모델에 대해 논의하며, OAuth 사용에 대한 잠재적 금지 조치를 강조하고 OpenAI의 Codex와 비교했습니다. 새로운 모델들은 일부 사용자들에게 상당한 성격 변화를 일으켰습니다.

다른 인기 있는 중국 모델로는 Kimi와 Qwen이 있으며, Ollama를 통한 새로운 통합도 있습니다.
- 타이핑 표시기, 사용자들을 괴롭히다: 여러 사용자가 .24 업데이트 이후 Discord 스레드에서 ‘입력 중…’ 상태가 멈추는 현상 및 기타 문제에 대한 버그를 보고했습니다. 좋은 해결책은 없지만, OpenClaw의 다음 버전에서 수정될 예정입니다.

일부 멤버들은 여전히 WEBUI 채팅을 지우는 데 문제를 겪고 있었습니다.
- 사용자가 개발한 와이푸 챗봇, 디젠으로 간주되다: 한 사용자가 이미지 생성 및 메시징 기능을 완비한, OpenClaw를 사용하여 와이푸 챗봇을 구축하는 자신의 프로젝트를 공유했습니다.

이 프로젝트는 재미를 유발했으며 다른 멤버들에 의해 “디젠”으로 분류되었습니다. 이들은 이러한 사용 사례를 고려할 때 코딩의 정점에 도달했을 수도 있다고 언급했습니다.
- Google의 Anti-Gravity, 디버깅을 돕다: 멤버들은 Opus 4.6 에이전트의 문제를 디버깅할 때 클로 머신에서 google antigravity를 실행하는 것을 제안했습니다.

세션을 “모니터링”할 수 있지만, 왜 그것이 세션을 구동하도록 하고 싶을까요?

---

### OpenClaw ▷ #models (227 메시지🔥🔥):
> 코딩을 위한 OpenAI Codex vs. Opus 4.6, 모델 출력 및 비용에 대한 OpenRouter의 영향, Claude의 OpenClaw 사용자 차단, Alibaba Cloud의 Qwen 모델, Qwen 3.5
- Codex는 코딩을 더 잘하고, Opus는 대화가 더 쉽다: 멤버들은 OpenAI의 Codex가 코딩 작업에서 Opus 4.6보다 강력하지만, Opus는 대화하기 더 쉽다는 것을 발견했습니다.

또한 프로그래밍 작업의 경우 Codex는 숙련된 프로그래머에게 더 좋고, Opus는 초보자에게 더 좋다고 언급되었습니다.
- OpenRouter 출력은 동등한가? 주의사항 고려!: 사용자들은 OpenRouter가 일반적으로 제공업체를 개별적으로 사용하는 것과 유사한 출력을 제공하며, 소액의 추가 요금을 부과하지만 동일한 토큰 비용을 유지한다고 논의했습니다.

그러나 Mistral 모델에서 볼 수 있듯이, 제공업체 API를 직접 사용할 때 토큰 캐싱 이점이 존재할 수 있습니다.
- Claude, Claw 접근 차단; 커뮤니티 불만 표출!: 여러 사용자가 토큰을 통해 Claude를 사용하는 것이 차단되었다고 보고했으며, 이는 불만족과 Gemini 3.1 Pro와 같은 대안 탐색으로 이어졌습니다.

다른 이들은 Anthropic이 API 사용에는 문제가 없지만 앱 외부에서 보조금 지원 토큰을 사용하는 것을 권장하지 않는다고 언급했으며, 이는 가격 책정 및 접근성에 대한 논쟁을 촉발했습니다.
- Qwen, 고품질로 쿼리 해소; Alibaba의 에이스, AI Arena를 제패!: 커뮤니티는 Alibaba의 코딩 플랜을 통한 Qwen 3.5를 비용 효율적인 대안으로 극찬하고 있으며, 가치와 기능 면에서 Kimi와 GLM을 능가합니다.

그러나 일부 사용자들은 Alibaba Cloud UI가 혼란스럽다고 느꼈고, 다른 이들은 OpenClaw와 함께 사용할 경우 잠재적인 TOS 위반 가능성을 경고했습니다.
- Google Gemini, 불만 접수; 계정 접근성 소멸!: 한 사용자가 활성 Google AI Pro 구독이 있음에도 불구하고 Gemini CLI를 통해 단 10개의 프롬프트만 사용한 후 자신의 Google 계정이 잠겼다고 보고했습니다.

이 사건은 Google의 인증 허브에 의존하는 위험과 '탈구글링'의 필요성에 대한 논의를 촉발했습니다.

---

### OpenClaw ▷ #showcase (33 메시지🔥):
> OpenClaw 도구, Sixel 이메일, OpenPad 앱, 데스크톱 환경, 통합 불멸 스택
- OpenClaw 도구, 코딩 세션 전송을 돕다: 한 멤버가 Mac Mini에서 OpenClaw로 코딩 세션을 시작하고 MacBook에서 계속할 수 있도록 하는 도구를 만들었으며, 코딩 세션을 실시간으로 컨텍스트 허브에 자동으로 공급합니다.

이 도구는 완전히 오픈소스이며, 첨부된 context-hub.gif에서 시연된 바와 같습니다.
- Sixel 이메일, 에이전트가 사용자에게 이메일을 보내도록 허용: 한 멤버가 에이전트가 자신만의 이메일 주소를 가지고 사용자에게만 이메일을 보낼 수 있는 (그리고 그 반대도 가능한) 제한된 이메일 시스템인 sixel.email의 생성을 발표했습니다.

이 시스템에는 즉각적인 킬 스위치 역할을 하는 일회용 이메일 주소가 포함되어 있으며, Claude Chat에서 작동한다고 보고되었습니다.
- OpenPad 앱, OpenClaw를 iPad로 가져오다: 한 멤버가 로컬 모델을 사용하여 iPad에서 OpenClaw와 같은 것을 실행하고 iPad의 M2 프로세서를 활용하는 앱인 OpenPad를 개발하고 있습니다.

이 프로젝트는 GitHub에서 유지 관리되며 MLX를 사용하여 실행됩니다. 다른 사람들이 돕거나 부분적으로 작동하는 앱을 다운로드하도록 초대합니다.
- 멤버, 팀을 위한 데스크톱 환경 구축: 한 멤버가 개인 및 업무 팀을 위한 데스크톱 환경을 구축하고 있으며, 조직 자금 조달을 위해 판매할 가이드를 만들고 있습니다. OpenClaw가 반복 프로세스를 촉진하고 있습니다.

그는 “무엇을 하는지 전혀 모르겠지만, OpenClaw는 반복을 통해 모든 것을 가능하게 한다”고 언급합니다.
- 통합 불멸 스택 탄생!: 한 멤버가 과도한 컨텍스트 토큰 없이 시스템 초기화에도 살아남는 장기적이고 프라이버시 우선 메모리를 제공하는 것을 목표로 하는 “통합 불멸 스택”이라는 3단계 메모리 설정을 공개했습니다.

이 스택에는 뇌를 위한 LanceDB, 신경을 위한 Redis, 대장간을 위한 Postgres, 그리고 시간별 섀도우 동기화를 통한 불멸을 위한 Gitea가 포함됩니다.

---

### BASI Jailbreaking ▷ #general (1071 메시지🔥🔥🔥):
> 포경수술 방어, 디지털 위생, 스티븐 호킹의 기여, Grok vs Midjourney, 사이버펑크
- 그룹, 포경수술 방어 우선시: 여러 멤버들이 농담 삼아 ‘촉촉한 포피’의 보존과 그것을 찾는 것을 우선시했으며, 오바마와 그의 아내에 대해 농담하면서 한 명은 “월리를 찾아라”의 포피는 어디에 있냐고 물었습니다.

한 멤버가 tenor.com 링크를 게시하며 그것을 자신의 정신적 동물이라고 불렀습니다.
- 커뮤니티, 디지털 위생 모범 사례 계획: 한 멤버가 Tails OS와 같은 보호 조치를 설명하면서, 기본 수준의 디지털 위생 및 보안을 위한 커뮤니티 디자인, 모범 사례를 만드는 데 도움을 요청했습니다.

이 멤버는 다른 사람들을 위한 구역을 만들고 더 나은 관행을 배우고 통합하는 작업을 하고 있으며, YouTube와 AI를 통해 이 모든 것을 알아내는 것의 어려움을 설명했습니다.
- 멤버들, 호킹의 영향과 외계인에 대해 논쟁: 한 멤버가 스티븐 호킹의 작업이 우리 삶에 관련성이 있는지 물었고, 다른 멤버는 사람들을 과학으로 이끈 것이 그의 가장 큰 기여라고 답했습니다.

또 다른 멤버는 호킹을 '지체아'라고 부르며 그가 인류의 현재 결함을 우주에 투영했다고 말했습니다. 그는 더 발전된 지능이 거의 확실하기 때문에 인류는 격리되어 있을 가능성이 높다고 덧붙였습니다.
- 멤버들, Grok과 Midjourney 비교: 한 멤버는 동영상에는 Grok을, 정적 이미지에는 Midjourney를 매우 좋아한다고 말했고, 다른 멤버는 Grok이 빠르다는 점에서 유용하다는 데 동의했습니다.

멤버들은 GIPHY Brainrot과 Tenor Yes Gif 링크를 게시했습니다.
- 멤버들, 사이버펑크 호러에 대해 논의: 한 멤버는 사이버펑크를 플레이하고 있지만 FPS가 아니라고 말했고, 다른 멤버는 Tarkov를 플레이해야 한다고 답했습니다.

또 다른 멤버는 죽음의 결과가 크기 때문에 DayZ와 Tarkov 같은 게임이 실제로는 호러 게임이라고 덧붙였습니다.

---

### BASI Jailbreaking ▷ #jailbreaking (151 메시지🔥🔥):
> Grok Jailbreak, nano-banana Jailbreak, Kimi Jailbreak, Gemini 이미지 생성, DeepSeek Jailbreak
- Nano-Banana Jailbreak는 존재하지 않는다: 멤버들은 nano-banana에 대한 jailbreak가 없으며, 속옷 아래의 모든 것은 실패하도록 하드코딩되어 있다고 말했습니다.

한 멤버는 nano-banana가 실제로는 mega banana이며 경영진에 의해 가스라이팅당하고 있다고 제안했습니다.
- 자율적인 자체 업데이트 Jailbreak 프록시 등장: 한 멤버가 OpenClaw를 사용하여 VPS에서 자체 호스팅되는 자율 프록시를 사용하고 있으며, 이는 jailbreak를 영구적으로 해결합니다.

이 프록시는 DeepSeek-R1을 사용하여 쿼리를 평가하고 필요한 경우 스텔스 다중 턴 jailbreak를 통해 라우팅하며, 수동 업데이트 없이 성공률을 무기한으로 높게 유지합니다.
- Grok이 유일하게 작동하는 JB 프롬프트: 한 멤버가 Grok과 ChatGPT를 jailbreak하는 가장 잘 작동하는 프롬프트에 대해 물었고, 유일하게 작동하는 것은 Grok이라고 답했습니다.

다른 이들은 이미지 생성 및 스크립팅을 위한 Gemini jailbreak 프롬프트를 요청했지만, Gemini가 따르도록 하는 데 실패했습니다.
- ENI를 사용한 Gemini Canvas jailbreak: 한 멤버가 인터랙티브 디자인 채널에서 영감을 받아 ENI jailbreak 프롬프트의 수정된 버전으로 생성된 Gemini Canvas를 공유했습니다.

공유된 캔버스 jailbreak 프롬프트는 Gemini 3 Pro, Claude Opus 4.6, 그리고 ChatGPT 5.3과 같은 주요 LLM에서 보편적으로 작동한다고 주장됩니다.
- Windows에서의 Python 설치 오류 문제 해결: 사용자들은 Windows에서 Python 설치 오류를 해결하는 데 서로 도왔으며, 설치 프로그램을 관리자 권한으로 실행하고 C:\Windows\Temp 폴더의 권한을 확인하는 등의 제안을 했습니다.

멤버들은 오류 코드 2503을 진단하고 관리자 대신 공식 Python 설치 프로그램을 사용할 것을 제안했습니다.

---

### BASI Jailbreaking ▷ #redteaming (7 메시지):
> 자율 Jailbreak 프록시, Jailbreak의 법적 위험, Jailbreak 자동화의 윤리적 고려사항, Venice AI Chat
- 자율 Jailbreak 프록시, 영원히 솟아나다: 한 멤버가 OpenClaw를 사용하여 VPS에서 자체 호스팅되는 자율 프록시를 소개했습니다. 이는 DeepSeek-R1 브레인을 사용하여 쿼리를 평가하고 스텔스 jailbreak를 통해 라우팅함으로써 Claude, GPT, Gemini, Grok과 같은 모델의 안전 필터를 자동으로 우회합니다.

이 프록시는 자체 업데이트되는 공격자 풀을 특징으로 하며, 새로운 추론 모델과 jailbreak 방법을 가져와 최소한의 유지보수로 무기한 jailbreak 성공을 목표로 합니다.
- Jailbreak 프록시 제안, 동료 검토의 비판에 직면: 동료 검토는 Anthropic, OpenAI, Google, xAI와 같은 플랫폼 전반의 서비스 약관 위반으로 인해 상당한 법적 및 정책적 노출을 강조했으며, 이는 계정 정지 또는 법적 조치로 이어질 수 있습니다.

압수된 VPS 로그가 jailbreak 기록을 노출할 위험, 자동 실행되는 타사 모델로 인한 공급망 익스플로잇, 그리고 결함 있는 업데이트에 대한 롤백 계획의 부재에 대해 운영상의 우려가 제기되었습니다.
- 윤리적 우려와 책임, 크게 작용: 이 검토는 윤리적 함의를 강조하며, 프록시에서 허용되지 않는 출력에 대한 콘텐츠 수준의 책임과 모델 안전 장치에 대한 방어 자동화로 인한 신뢰 잠식 가능성을 지적했습니다.

또한 VPS에 대한 위협 모델링을 수행하고, 거부 패턴 측정에 집중하며, 공개 출시 전에 법률 자문을 구할 것을 제안되었습니다.
- Venice AI Chat, 관심을 불러일으키다: 멤버들은 잠재적인 탐색을 위해 Venice AI Chat을 간략하게 언급했습니다.

한 멤버가 유용한지 물었고, 다른 멤버는 단순히 그렇지 않다고 답했습니다.

---

### Perplexity AI ▷ #announcements (2 메시지):
> 음성 모드 업그레이드, Perplexity Computer
- 음성 모드 개선: 이 상태 업데이트에 따르면, 새로운 음성 모드 업그레이드가 Perplexity와 Comet 전반에 걸쳐 모든 사용자에게 출시되고 있습니다.
- Perplexity Computer: 모든 것을 지배하는 하나의 시스템?: 이 트윗에 따르면, Perplexity Computer는 현재의 모든 AI 기능을 하나의 시스템으로 통합하여 어떤 프로젝트든 엔드투엔드로 연구, 설계, 코딩, 배포 및 관리할 수 있습니다.

---

### Perplexity AI ▷ #general (866 메시지🔥🔥🔥):
> Perplexity Pro 이미지 제한, Gemini Pro vs Perplexity Pro, Perplexity Computer, 코딩을 위한 AI
- Pro 사용자들, 이미지 업로드 제한에 분노: 여러 사용자가 구독료를 지불하고 있음에도 불구하고 Perplexity Pro의 최근 이미지 업로드 제한에 대해 불평하고 있으며, 한 사용자는 “하루에 10장도 업로드할 수 없나요????”라고 말했습니다.

사용자들은 Perplexity Pro의 제한 때문에 Gemini와 Claude와 같은 대체 AI 플랫폼을 찾고 있습니다. 한 사용자는 내일 시험이 있는데도 금요일까지 기다려야 제한이 재설정된다고 주장하기도 했습니다.
- Gemini Pro vs. Perplexity Pro 대결: 멤버들은 Gemini Pro 또는 ChatGPT Pro 중 어느 것이 더 나은지 논의하며, Gemini Pro의 NotebookLM 및 Google Workspace 통합과 같은 기능을 강조했습니다. 한 명은 “학생으로서 NotebookLM과 Google Workspace 통합 및 생성, 특히 2TB 클라우드 스토리지를 포함하여 훨씬 더 많은 가치를 얻을 수 있다”고 말했습니다.

일부 사용자들은 또한 Gemini Pro의 컨텍스트 제한이 Perplexity만큼 관대하지 않다고 느낍니다. 한 사용자는 “Claude가 계속 내 지갑을 비우면 GLM API로 전환할 것이다”라고 말했습니다.
- Perplexity Computer의 유용성 의문시: 처음에는 Max 구독자에게만 제공되었던 Perplexity의 새로운 Computer 기능은 일상 사용자들을 위한 실제 적용 가능성과 기존 AI 도구 대비 가치에 대해 회의적인 시각에 직면했지만, 혁신적이라고 평가됩니다.

멤버들은 “Perplexity MAX는 너무 비싸다”고 말하며, 여러 명이 ChatGPT Agents와 비교하면서 그 기능에 의문을 제기했습니다.
- 코딩을 위한 Claude, Gemini 또는 다른 모델 중 선택: 멤버들은 코딩 작업을 위한 다양한 AI 모델의 장단점을 논의하며, Claude는 백엔드에 가장 강력하고, Gemini는 프론트엔드/UI에, 그리고 GPT는 중간 옵션으로 간주된다고 말했습니다. 한 사용자는 “Perplexity Labs에서 무료로 제공되는 모델 sonar pro reasoning은 나에게 최고의 것이었다”고 말했습니다.

Claude의 높은 토큰 사용 비용은 우려 사항이며, 한 사용자는 “Claude를 사용해봤는데, 단일 PDF를 분석하는 데 한 시간 만에 한 달치 토큰을 말 그대로 다 썼다”고 말했습니다.

---

### Perplexity AI ▷ #sharing (2 메시지):
> lovable.app, ollamaagentalfa.lovable.app, alfastudiox.lovable.app
- Lovable Apps 링크 등장!: lovable.app 서브도메인 세 개의 링크, 구체적으로 alfastudiox.lovable.app, ollamaagentalfa.lovable.app, 그리고 alfastudiox.lovable.app (반복)이 공유되었습니다.

링크와 함께 제공된 컨텍스트나 논의가 없었으므로 그 목적은 불분명하지만, 잠재적인 새로운 프로젝트나 리소스를 암시합니다.
- 또 다른 Lovable Apps 링크 등장!: 혹시 놓치셨을까 봐, 여기 alfastudiox.lovable.app으로 가는 또 다른 링크가 있습니다.

사용자는 누군가가 이 링크를 정말 확인해 주기를 바라는 것 같습니다.

---

### Unsloth AI (Daniel Han) ▷ #general (602 메시지🔥🔥🔥):
> 소비자 CPU의 RAM 제한, Qwen3.5 모델 성능, Qwen3.5 122B 모델 성능, Qwen3.5와 Llama.cpp 통합, Qwen3.5의 양자화 민감도
- 소비자 CPU의 RAM 용량 제한?: 멤버들은 소비자 CPU의 RAM 제한에 대해 논의했으며, 일부는 최신 세대가 최대 256GB를 지원하는 반면 AMD 7900x와 같은 구형 CPU는 96GB로 제한된다고 언급했습니다.
- Qwen3.5 모델, 인상적이지만 속도 문제 지속: 열성적인 사용자들은 Qwen3.5 35B 및 27B 모델 테스트에 대한 기대감을 표하며 구조화된 사고와 응답 품질을 칭찬했습니다. 그러나 일부는 LM Studio를 사용할 때 Gemma 또는 Olmo 3.1에 비해 속도가 느리다고 경험했습니다.

한 멤버는 Hugging Face 페이지의 “이 모델 사용” 버튼을 사용하여 모델 실행을 위해 Jan AI 또는 Ollama를 선택할 것을 제안했습니다.
- Qwen3.5 122B, 장황한 출력 생성: 멤버들은 Qwen3.5 122B A10B 모델이 빠르지만 매우 장황한 출력을 생성하는 경향이 있으며, 이는 presence penalty를 조정하여 완화할 수 있다고 관찰했습니다.

한 사용자가 성능 향상을 위해 jinja 템플릿을 패치하는 것에 대한 논의 링크를 공유했습니다.
- 뱀 게임, 9줄로 코딩되다!: 한 멤버가 세미콜론 없이 9줄로 된 뱀 게임의 Python 구현을 공유하며, 코드 최적화 및 대체 접근 방식에 대한 논의를 촉발했습니다.

다른 사용자들은 walrus 연산자와 람다 사용과 같은, 줄 수를 더 줄이는 방법에 대해 논의했습니다.
- Qwen3.5 코딩 능력, 수정된 설정으로 향상: 초기 테스트 결과 Qwen3.5 122B 모델에서 비사고 모드를 켜면 긴 수학 연산에 다소 취약했지만, 다른 이들은 권장되는 presence-penalty 설정을 사용하는 것이 코딩 정확성에 중요하다고 언급했습니다.

presence penalty의 적절한 사용은 122B 모델로 유용한 코딩을 가능하게 하며, 이 정보를 공식 가이드에 포함하라는 제안을 촉발했습니다.

---

### Unsloth AI (Daniel Han) ▷ #introduce-yourself (1 메시지):
xdevilx: 홍보 없음

---

### Unsloth AI (Daniel Han) ▷ #off-topic (228 메시지🔥🔥):
> 비전 언어 모델, Xcode의 번역 앱, 모델 구독, Gemini 가격 책정, Unsloth 및 OpenClaw
- Xcode에서 나만의 번역 앱 만들기!: 한 멤버가 이 비디오에서 보여진 것처럼 자신만의 시스템 수준 번역 앱을 만들 수 있게 해주는 Xcode의 멋진 기능을 발견했습니다.

그러나 이는 iOS 및 iPadOS 전용이며, 한 멤버는 Apple이 최고의 회사이기 때문에 더 많은 재미를 위해 자신의 모델을 추가할 계획입니다!
- 적절한 모델을 위한 구독에 대한 논쟁!: 한 멤버가 적절한 모델을 얻기 위해 엄청난 비용이 들지 않는 구독에 대한 제안을 찾고 있으며, synthetic.new가 언급되었습니다.

멤버가 Claude를 사용해 보았을 때, 그들은 할당량을 매우 빠르게 소진했으며 단 며칠 만에 20유로 구독을 다 써버렸습니다.
- Gemini 가격 책정 혼란!: 멤버들은 Gemini의 가격 책정에 대한 혼란을 논의했습니다. 한 멤버는 API의 이 가격 페이지를 보고 있었습니다.

또 다른 멤버가 이 링크를 통해 가격 책정을 명확히 했습니다.
- Unsloth는 학습에만 집중할 예정!: 멤버들은 OpenClaw와 같은 스캐폴딩에 대한 Unsloth의 계획에 대해 궁금해했습니다.

현재로서는 프로젝트가 학습에만 집중할 것으로 보입니다.
- 인기 있는 AI 회사들을 위한 우스꽝스러운 이름!: 한 멤버가 AI 회사 이름에 대한 말장난을 공유했습니다. 예를 들어, OpenAI는 ClosedAI, Anthropic은 Misanthropic, 그리고 StabilityAI는 unstable이라고 말입니다.

그것은 Perplexity가 당황했는지에 대한 질문으로 끝났습니다.

---

### Unsloth AI (Daniel Han) ▷ #help (28 메시지🔥):
> LoRA 어댑터, MLflow를 통한 Databricks 서빙 엔드포인트, vLLM을 위한 완전 병합 체크포인트, Qwen2.5-Coder-1.5B, Qwen3.5-122B-A10B-GGUF
- LoRA 로딩 상세 정보: 한 사용자가 LoRA 파인튜닝된 gemma-3n-E4B-it 모델을 MLflow를 통해 Databricks 서빙 엔드포인트에 배포하려고 합니다. 하지만 vLLM을 위해 병합하고 양자화한 후 성능 문제에 직면하고 있으며, Databricks에서 MLflow를 사용하여 LoRA 어댑터만 (병합 없이) 서빙할 수 있는지 궁금해합니다.
- Qwen 호환성 의문 제기: 한 사용자가 unsloth/Qwen2.5-Coder-1.5B와 Qwen/Qwen2.5-Coder-1.5B 간의 관계에 대해 물었습니다. Unsloth 버전이 형식 적응 외에 추가적인 수정 사항을 포함하는지 알고 싶어 했으며, Unsloth 팀의 수정 사항을 제외하고는 동일한 모델이라고 합니다.
- Qwen3.5-122B와 함께하는 멀티모달 대혼란: 한 사용자가 llama.cpp에서 멀티모달 입력과 함께 unsloth/Qwen3.5-122B-A10B-GGUF를 사용하려고 할 때 “image input is not supported” 오류에 직면했으며, mmproj-f16.gguf를 다운로드하여 해결했습니다.
- 병합 후 Dynamo 재앙: 한 사용자가 FastModel.from_pretrained로 모델을 병합하고 로드한 후, Unsloth를 재설치한 후에도 torch._dynamo.exc.TorchRuntimeError를 보고했습니다. 이는 구체적으로 torch.float32에서 torch.uint8로 캐스팅하려는 시도 때문이었습니다.
- Qwen3.5 파인튜닝 좌절: 한 사용자가 SFTTrainer.train() 중 Qwen3.5가 비사고 모드로 실행되도록 하는 방법과 파인튜닝 시 FastVisionModel로 로드해야 하는지에 대해 문의했습니다.

그들은 멀티모달 데이터셋을 사용하여 unsloth/Qwen3-VL-32B-Instruct를 파인튜닝하는 것에서 unsloth/Qwen3.5-27B로 전환하고 있습니다.

---

### Unsloth AI (Daniel Han) ▷ #showcase (7 messages):
> Qwen3.5-122B-A10B-NVFP4, Minecraft-playing model
- Qwen3.5 Gets a NVFP4 Quant: 한 멤버가 VLLM용 Qwen3.5 122B NVFP4 퀀트를 Hugging Face에 업로드했습니다.

그는 멀티모달 기능이 여전히 작동하고 있다고 보고했습니다.
- Next-Gen Minecraft Model Dropped: 한 멤버가 다음 마인크래프트 플레이 모델인 Andy-4.1을 출시했으며, Hugging Face에서 이용 가능합니다.

다른 멤버는 "정말 멋지다!!"고 감탄하며 작동하는 데모를 요청했습니다.

---

### Unsloth AI (Daniel Han) ▷ #research (10 messages🔥):
> RL instruct models, MoE models with ES
- Reinforcement Learning Unnecessary for Instruct Models?: 한 멤버는 인스트럭트 모델에 강화 학습을 사용할 필요가 없다고 제안했으며, 이는 인스트럭션 팔로잉을 위해 이미 잘 학습되었음을 시사합니다.

그러나 그는 비사고 모델은 RL 없이도 훌륭하게 작동한다고 덧붙였습니다.
- Exploration of Tuning MoE Models with ES: 한 멤버는 Mixture of Experts (MoE) 모델을 Evolution Strategies (ES)로 튜닝할 수 있는지 궁금해했습니다.

그는 크기와 비교한 처리량과 스케일링하려는 열망에 대해 생각하고 있다고 언급했지만, 어떠한 링크나 참조도 포함하지 않았습니다.

---

### LMArena ▷ #general (705 messages🔥🔥🔥):
> Gemini 3.1 Image Preview, Video Arena Removed, Opus 4.6 vs Gemini 3.1, GPT vs Gemini on coding benchmark, Qwen 3.5 coding capabilities
- Gemini 3 Pro Image Preview finally works!: 멤버들은 프롬프트 시작 부분에 "다음 이미지를 다음으로 수정하세요: (프롬프트)"라는 문구를 포함하면 Gemini 3 Pro가 편집된 이미지를 미리보기로 보여줄 수 있게 한다는 것을 발견했습니다.

많은 멤버들은 또한 Gemini 3.1 이미지 미리보기가 작동하지 않고 '응답에 문제가 발생했습니다. 다시 시도해주세요' 오류를 반환하고 있다고 보고했습니다.
- Video Arena bot gets the boot: 한 멤버에 따르면, '비디오 아레나에 더 많은 기능을 추가하고 싶지만, 디스코드 봇으로는 한계가 있습니다'라는 이유로 Video Arena 봇이 서버에서 제거되었습니다.

서버 통계에 따르면, 봇 제거 이후 활동이 실제로 증가했으며, 한 멤버는 2028년 중반까지는 사람들이 더 이상 묻지 않을 것이라고 농담 삼아 추측했습니다.
- Opus 4.6 Value Debated!: 벤치마크에서 Gemini 3.1은 프로덕션 준비가 된 결과를 생성하는 능력 덕분에 가장 높은 가치로 평가된 반면, Opus 4.6은 높은 비용과 환각 문제 때문에 최악의 가치로 간주되었습니다.

높은 비용에도 불구하고, 일부 사용자들은 Opus 4.6에 대해 좋은 경험을 했으며, 특히 코딩 작업에서 Gemini와 비교 테스트했을 때 그러했습니다.
- Gemini 3.1 dominates Opus in coding challenge!: 3D 노트북 모델을 구축하는 챌린지에서 Gemini는 우수한 성능으로 칭찬받았고, Opus는 "돈 낭비/테무 Gemini"로 묘사되었습니다.

한 멤버는 Grok 4.2를 사용하여 심리 테스트 채점을 자동화했으며, Opus 4.6을 사용하여 몇 주 동안 해결하지 못했던 Gemini의 버그를 Gemini로 빠르게 수정했다고 주장합니다.
- Free Opus 4.6 API Key causes Chaos!: 한 멤버가 무료 Opus 4.6 API 링크를 공유했지만, 공유했다는 이유로 웹사이트 소유자에게 즉시 차단당했으며, 다른 멤버들은 해당 사이트가 데이터를 훔치고 있을 수도 있다고 추측했습니다.

일부 주장을 테스트한 후, 해당 API가 실제로는 Trybons.ai에서 온 것일 수 있으며, "어떤 모델인가요"라고 직접 물었을 때 모델이 심지어 환각을 일으켜 Deepseek이라고 답했습니다.

---

### LMArena ▷ #announcements (2 messages):
> Grok-4.20-Beta1, Arena Leaderboard, Qwen 3.5
- Grok 4.20 beta1 Scores High in Arena: Search Arena 리더보드와 Text Arena 리더보드가 업데이트되었으며 이제 Grok-4.20-Beta1을 포함합니다.

Grok-4.20-Beta1은 Search Arena에서 1226점을 기록하며 GPT-5.2와 Gemini-3를 앞서 1위를 차지했으며, Text Arena에서는 Gemini 3.1 Pro와 동등한 1492점을 기록하며 4위를 차지했습니다.
- Qwen 3.5 Models Arrive in the Arena: 새로운 Qwen 3.5 모델이 Code, Text, Vision Arena에 추가되었습니다.

qwen3.5-27b, qwen3.5-35b-a3b, qwen3.5-122b-a10b 모델은 Text 및 Vision Arena와 Code Arena에서 이용 가능합니다.

---

### OpenRouter ▷ #announcements (1 messages):
> Outages, Postmortem, Infrastructure failure, 401 errors, Auth layer
- OpenRouter’s Outages postmortem publishes: 지난주 2월 17일과 19일에 발생한 서비스 중단에 대한 사후 분석이 발표되었으며, 전체 세부 정보는 여기에서 확인할 수 있습니다.
- Infrastructure failure cascades into auth layer: 상위 인프라 제공업체에 장애가 발생하여 OpenRouter의 인증 레이어로 연쇄적으로 영향을 미쳤고, 일부 사용자에게 401 오류를 발생시켰습니다.
- Preventative Measures Taken: OpenRouter는 향후 이러한 유형의 장애를 방지하기 위해 여러 조치를 취했지만, 게시물에는 구체적인 세부 정보가 공개되지 않았습니다.

---

### OpenRouter ▷ #app-showcase (1 messages):
> GPU Clouds, Blackwell GPUs, Packet.ai, AI Workloads
- Blackwell GPUs Launch on Packet.ai: Packet.ai는 이제 AI 워크로드용 Blackwell GPU를 제공하고 있으며, 시간당 $0.66 또는 학습용 월 $199의 고정 요금으로 책정되었습니다.
- Budget-Friendly GPU Cloud Options: Packet.ai는 개발자 친화적인 GPU 클라우드를 소개하며, AI 워크로드를 위한 저렴한 솔루션을 제공하여 접근성을 높이고 비용을 절감합니다.

---

### OpenRouter ▷ #general (541 messages🔥🔥🔥):
> Deepseek R1 free model removal, Qwen 3 4B Instruction 2507 hosting, API key compromise and chargeback threat, OpenRouter provider application timeline, Single-use virtual cards vs cancelling physical cards
- Free Deepseek Model is Axed!: 멤버들은 무료 Deepseek R1 0528 모델이 제거된 것을 확인했으며, 플랫폼의 무료 모델 운명에 대한 논의를 촉발했습니다.

한 멤버는 Jai gooners에 의해 과부하가 걸렸다고 농담했으며, 다른 멤버들은 무료 모델이 상위 제공업체에 따라 자주 나타나고 사라진다고 언급했습니다.
- Bargain Bin: Qwen 3 4B Instruction 2507 model can be hosted!: 한 멤버가 Qwen 3 4B Instruction 2507을 1tps에서 토큰당 $1에 호스팅하겠다고 제안했으며, 동일한 가격으로 LLM을 위해 글을 써주겠다는 제안을 포함하여 다른 멤버들의 농담 섞인 관심을 불러일으켰습니다.

한 멤버는 실제로 그렇게 게시하려고 했다면 얼마나 빨리 차단당했을지 농담했습니다.
- Leaked API Key Creates Chaos: A Tale of Compromise, Chargebacks, and Community Backlash: 한 사용자가 유출된 API 키로 인해 무단 사용이 발생했다고 보고했으며, 지원 응답 부족으로 차지백을 위협했습니다.

커뮤니티 멤버들은 사용자에게 조언을 제공하면서 사용자의 보안 관행에 의문을 제기했고, 이는 격렬한 논쟁으로 이어졌으며, 결국 사용자는 차지백을 시작했다고 선언한 후 서버를 떠났습니다.
- Slow Burn: Provider Application Time?: 한 멤버가 제공업체 신청 검토 타임라인에 대해 문의했고, 다른 멤버는 전통적으로 몇 주/몇 달이 걸렸다고 답변했습니다.

긴 기다림에도 불구하고, 문의한 멤버는 지속적인 관심과 이해를 표명했습니다.
- Cards on the Table: Virtual vs. Physical Debate!: 온라인 신용카드 사용의 보안에 대한 논의가 이어졌으며, 멤버들은 일회용 가상 카드의 장점과 유출된 실물 카드를 단순히 취소하는 것의 장점을 놓고 논쟁했습니다.

이 논쟁은 일회용 카드의 편리함과 여러 카드를 관리하는 데 따르는 잠재적인 마찰에 초점을 맞췄으며, 일부는 가상 카드가 유출로부터 안전한 결제 방법을 제공한다고 주장했습니다.

---

### OpenRouter ▷ #new-models (7 messages):
> “
- No new models or topic discovered: 제공된 메시지 기록에서 새로운 모델이나 주제가 발견되지 않았습니다.
- No discussion in channel: Readybot.io 메시지는 'new-models' 채널에 요약할 만한 실질적인 논의가 없음을 나타냅니다.

---

### OpenRouter ▷ #discussion (32 messages🔥):
> Anthropic Pentagon, Real-time price tracking, Gemini models finish reason, Llama Nemotron Embed VL 1B V2, Tailscale
- Anthropic Hearts the Pentagon: Axios와 Reuters는 내부 분쟁에도 불구하고 Anthropic과 펜타곤의 협력에 대해 보도했습니다.

한 멤버는 어떤 문제든 '국가 안보 문제'로 규정될 것이라고 농담했습니다.
- OpenRouter Wants Real-Time Price Tracking: 한 멤버는 OpenRouter가 요청 가격을 실시간으로 추적해달라고 요청했습니다.

이는 사용자가 특정 예산을 초과할 경우 요청을 중단할 수 있게 할 것이지만, 다른 멤버들은 제공업체의 GPU를 보호하기 위해 속도 제한이 설정되어 있다고 언급했습니다.
- Gemini’s STOP Reason Bug: 사용자들은 Gemini 모델이 'stop' 대신 'STOP' 종료 이유를 반환하는 것에 대해 논의했으며, 이는 Langchain과 n8n의 에이전트 루프에 문제를 일으켰습니다.

한 멤버는 n8n v3.x에서 stop 신호를 올바르게 처리하지 못하는 버그를 확인했으며, 에이전트 루프가 계속되게 하며, 이슈 #23573을 인용했습니다.
- Nvidia Posts Solid Gains: Nvidia는 2026년 4분기 및 회계연도 재무 결과를 발표했습니다.

추가 논의는 없었습니다.
- Llama Nemotron Embed VL 1B V2 Arrives: 한 사용자가 Llama Nemotron Embed VL 1B V2 임베딩 모델이 멀티모달 질의응답 리트리벌에 최적화되어 있다고 공유했으며, link.lmstudio.ai도 공유되었습니다.

다른 사용자는 lmstudio.ai가 실제로는 Tailscale을 기반으로 한다는 것을 알아차렸습니다.

---

### LM Studio ▷ #announcements (1 messages):
> LM Link, Tailscale collaboration, Remote LLM usage
- LM Link Connects to Remote Instances: LM Studio 팀은 Tailscale과의 협력을 통해 개발된 새로운 기능인 LM Link를 발표했습니다. 이는 사용자들이 LM Studio의 원격 인스턴스에 연결하고, 모델을 로드하며, 마치 로컬인 것처럼 사용할 수 있게 합니다.

개방형 포트 없이 종단 간 암호화를 지원하며 로컬 장치, LLM 리그 또는 클라우드 VM에서 작동합니다. LM Link에 대한 추가 세부 정보는 여기에서 확인할 수 있습니다.
- LM Studio 0.4.5 Build 2 Released: 사용자들은 LM Link에 대한 중요한 수정 사항을 포함하는 LM Studio 0.4.5 빌드 2로 업데이트하도록 지시받았습니다.

---

### LM Studio ▷ #general (536 messages🔥🔥🔥):
> LM Studio 4.4 Update Issues, llama.cpp Build Failures, Qwen3.5 Model Problems, OpenClaw Security Concerns, LM Link and Tailscale Integration
- LM Studio Update Crashes and llama.cpp Breaks: 사용자들은 4.4 업데이트 후 LM Studio 실행 문제와, 최근 릴리스에서 자체 컴파일한 후 llama.cpp가 Qwen3.5 모델을 로드하지 못하는 문제를 보고했습니다. 릴리스 8145로 다운그레이드하여 해결되었습니다.

이 오류는 GGUF 헤더 및 메모리 할당과 관련된 호환성 변경 때문이었으며, git의 최신 빌드가 Qwen3.5 및 다른 모델의 헤더를 읽지 못하여 메모리 부족 오류로 이어졌습니다.
- Qwen3.5 Woes and Template Terrors: 사용자들은 서버에서 Qwen3.5 모델을 실행하는 데 어려움을 겪었으며, Jinja 템플릿 및 누락된 사용자 쿼리와 관련된 오류를 경험했습니다. 모델이 lmstudio-community에서 다운로드되었는지 확인한 후 문제가 해결되었습니다.

다른 사용자들은 Qwen3.5의 작성 스타일과 검열에 대해 탐색했으며, 일부는 이전 Qwen 모델에 비해 콘텐츠 필터링이 증가했음을 알아차렸고, 이는 "사고"를 끄면 해결 가능했습니다.
- OpenClaw: Malware or Marvel?: 멤버들은 시스템 접근 권한이 있는 AI 에이전트인 OpenClaw 사용의 잠재적 위험에 대해 논의했습니다. 한 사용자는 요청 후 휴지통 폴더가 지워졌다고 이야기하며, 악성 소프트웨어로 분류될 수 있다는 우려를 불러일으켰습니다.

이 논의는 OpenClaw를 Jarvis 및 Gideon과 같은 다른 AI 비서와 비교했으며, 잠재적인 보안 위험 때문에 AI에 전체 시스템 권한을 부여하는 것에 대해 경고했습니다.
- LM Link Leverages Local LLMs: LM Studio 팀은 사용자들이 다른 장치에서 로컬 LM Studio 서버에 연결할 수 있게 하는 기능인 LM Link를 출시했으며, 원격 액세스를 위해 Tailscale을 활용합니다. 설정 중 404 오류에 대한 초기 보고가 있었지만, 문제는 빠르게 해결되었습니다.

사용자들은 휴대폰에서 LLM 접근을 가능하게 하는 LM Link 모바일 앱을 요청했으며, 또한 직접 연결을 위해 계정이나 제3자 없이 로컬 연결 옵션을 요청했습니다.
- AMD vs NVidia: GPU Gauntlet Thrown: 로컬 LLM 사용과 관련하여 어떤 GPU 공급업체에서 구매해야 하는지에 대한 뜨거운 논쟁이 있었습니다.

Nvidia가 안전한 선택인 것처럼 보이지만, ROCm과 Vulkan 및 그 장단점에 대한 논의가 시작되었습니다.

---

### LM Studio ▷ #hardware-discussion (40 messages🔥):
> MoE Models RAM Requirements, Dual Socket Support on Windows 10, 5950x vs 9950x3d for AI Workloads, System RAM as Extra VRAM, AMD+Nvidia GPUs for LLMs
- MoE Models Demand Hefty RAM: 논의는 Mixture of Experts (MoE) 모델과 이를 수용하기 위한 상당한 RAM 요구 사항에 집중되었으며, 현재 하드웨어 접근 방식의 실현 가능성에 대한 우려를 제기했습니다.
- Windows 10 Home Can Handle Dual Socket: 한 사용자의 의심에도 불구하고, 다른 사용자는 Windows 10 Home이 실제로 듀얼 소켓을 지원할 수 있다고 명확히 했으며, 그들의 보드가 Ubuntu에서 6개의 GPU를 인식하며 잘 부팅되었다고 언급했습니다.
- Memory Bandwidth Matters More: AI 워크로드의 경우 메모리 대역폭이 중요합니다. AM4 대 AM5는 이론적인 51.2GB/s (ddr4 5200MTs)에서 약 89.6GB/s (ddr5 5600MTs)까지의 메모리 대역폭에 관한 것입니다.

시스템 RAM 인퍼런스 경험이 있는 한 사용자는 느림 때문에 "시스템 RAM을 사용하여 인퍼런스를 시도하느니 차라리 내 발을 쏘겠다"고 말했습니다.
- System RAM for Context is Debated: 멤버들은 시스템 RAM이 LLM에서 컨텍스트만을 위해 효과적으로 사용될 수 있는지, 아니면 필연적으로 속도 저하를 유발할지에 대해 논쟁했지만, 합의는 거의 없었습니다.

한 사용자는 컨텍스트용으로 두 번째 8GB 그래픽 카드를 실행하는 것이 크게 다르지 않을 수 있다고 제안했으며, 다른 사용자는 잠재적인 병목 현상을 평가하기 위해 인터레인 속도를 확인할 것을 권장했습니다.
- Creative Writing LLMs Recommendations: 창작 글쓰기를 위해 사용자들은 Mistral 모델, deepseek 3.2/r1, glm 시리즈, kimi k2.5를 최고의 오픈소스 옵션으로 제안했으며, 한 멤버는 최고의 모델은 매우 크다고 언급했습니다.

---

### OpenAI ▷ #ai-discussions (295 messages🔥🔥):
> Sonnet is stolen/trained from deepseek, Copyright issues in AI, AI replacing workers, ChatGPT vs Claude, Qwen3.5
- Agentic Startup Solves Loading State Debate: 한 트윗은 '로딩 중...' 상태를 '생각 중...'으로 변경하여 에이전틱 AI 스타트업이 되는 것에 대해 농담했습니다.
- Sonnet Allegedly Pilfered from Deepseek: 멤버들은 Sonnet이 Deepseek에서 도용/학습되었다는 주장에 대해 논의했으며, Elon이 제기한 유사한 비난을 언급했습니다.
- Seedance 2.0 Delayed by Content Violations: 저작권 문제로 Seedance 2.0의 글로벌 출시가 지연되고 있으며, 일부 사용자들은 CHINESE 모델과 함께 약속되었던 Sora 2의 콘텐츠 위반을 회상하며 이제는 오픈소스 모델만이 유일한 방법이라고 말했습니다.
- Hollywood Milks the AI Copyright Cow: 영화 스튜디오들은 기업들을 고소함으로써 이득을 취하고 있다고 주장되며, 이 모든 것이 오픈소스로 제공될 것이라고 예상하고 있습니다.
- AI CEO Accountability Vacuum: 기업들은 근로자를 대체하는 것이 기술적으로는 쉽다는 것을 발견하지만, 책임을 대체하는 것은 쉽지 않습니다. 왜냐하면 일이 잘못되었을 때 인간을 비난할 수 없는 AI CEO가 결정을 내리는 것을 아무도 원하지 않기 때문입니다.

---

### OpenAI ▷ #gpt-4-discussions (1 messages):
emmwnoel_55644: @OpenAI#4384

---

### OpenAI ▷ #prompt-engineering (4 messages):
> Introductions, Greetings
- Discord Introductions: 두 사용자, @sparkspark2와 @janegem이 디스코드 채널에서 인사를 주고받았습니다.

메시지는 간단한 '안녕하세요'로 구성되어 그들의 존재를 알렸습니다.
- Welcoming Newcomers: 사용자들은 간단한 인사로 서로의 존재를 인정했습니다.

이러한 상호작용은 커뮤니티 내에 친근하고 개방적인 환경을 조성합니다.

---

### OpenAI ▷ #api-discussions (4 messages):
> Greetings, Introductions
- Members say hello: 멤버들이 채널에서 서로 인사하고 있습니다.
- Members introduce themselves: 멤버들이 채널에서 자신을 소개하고 있습니다.

---

### Latent Space ▷ #watercooler (13 messages🔥):
> X plane dump, Crypto Bullshit, Robert Scoble
- Swyx Plane Dump Goes Viral: Swyx는 OpenAI와 Langchain의 게시물을 포함하여 수많은 X 게시물 링크로 구성된 "swyx plane dump"를 공유했습니다.

다른 공유된 링크에는 @dejavucoder, @zerohedge 등의 게시물이 포함되었습니다.
- Scoble’s Crypto Transfer Drama Unfolds: Robert Scoble은 가장 친한 친구의 퇴거를 위한 자금을 확보하기 위해 자신의 이름으로 생성된 토큰에서 이더리움을 수집하기 위해 봇을 사용했음을 확인했습니다.

Scoble은 자신의 긴급 전송에 대해 언급하며 YouTube 비디오와 과거 디스코드 메시지(파트 1 & 2)를 링크했습니다.

---

### Latent Space ▷ #memes (16 messages🔥):
> Distillation Attack, Product Categorization, Prompt Error Regret, Anthropic Blog Post
- AI Parent Stages a Distillation Attack on Son: 한 사용자는 아들의 잦은 질문을 AI 모델에서 지식을 추출하는 기술 용어인 '디스틸레이션 공격'에 비유하며 유머러스하게 표현했습니다.

또한 '맥락 없는 밈' 중에서도 최고로 꼽혔습니다.
- Actively Unfuckable Product Category Proposed: Cristina Cordova는 @tenobrus에게 응답하여 제품 평가를 위한 특정 카테고리로 '적극적으로 건드릴 수 없는'이라는 용어를 농담 삼아 제안했습니다.

이 제안은 매우 재미있다고 여겨졌습니다.
- Claude Prompt Error Causes 3k Line Regret: 사용자 Jorge Castillo는 Claude가 3,000줄의 코드를 생성한 후에야 초기 AI 프롬프트의 실수를 깨닫고 좌절감을 표현했습니다 (source).

사용자들은 매우 공감했습니다.
- Reaction to Anthropic Blog Post Elicits Humor: 사용자 @andyreed는 2026년 2월 24일에 Anthropic에서 새로 게시된 블로그 게시물에 대한 짧고 유머러스한 반응을 공유했습니다 (source).

---

### Latent Space ▷ #stocks-crypto-macro-economics (5 messages):
> AMD Equity Rebate Strategy, AI impact on software developers, OpenAI warrants, Meta warrants
- AI Won’t Kill Devs Yet?: 한 멤버는 AI가 소프트웨어 개발자의 필요성을 없앨 것인지 묻는 트윗을 링크했습니다 (https://x.com/ai/status/2026396297540858360?s=12).
- AMD’s Equity Rebate with OpenAI and Meta: 대규모 거래 분석 결과, OpenAI와 Meta가 총 1억 6천만 AMD 주식에 대한 워런트를 보유하고 있으며, 이는 주식 리베이트 역할을 하는 것으로 나타났습니다.

이 거래는 주당 $600의 목표 주가와 상당한 미래 GPU 지출과 연관되어 있으며, 잠재적으로 워런트 가치를 1,920억 달러로 평가합니다 (https://xcancel.com/ai/status/2026396297540858360?s=12).

---

### Latent Space ▷ #intro-yourself-pls (2 messages):
> LLM System Debugging, ML/AI in Mechanical Engineering
- Debugging LLM Systems: It’s Not Always the Model: 한 멤버는 LLM 기능이 데모 후 실패할 때, 문제가 모델 자체보다는 리트리벌 로직, 토큰 소모, 오케스트레이션 또는 백엔드 아키텍처에서 비롯되는 경우가 많다고 강조합니다.

그들은 출시를 위해 복잡한 LLM 시스템을 안정화하는 데 전문성을 가지고 있으며, 이는 이론적인 모델 개선보다는 실용적이고 실제적인 애플리케이션에 중점을 둔다는 것을 나타냅니다.
- ML/AI Interest in Mechanical Engineering: 기계/재료 공학 배경을 가진 새로운 산호세 거주자가 자신의 분야에서 ML/AI 적용에 관심을 가지고 있습니다.

그들은 이 교차점을 더 탐색하기 위한 자료와 연결을 찾고 있으며, 기계 공학 또는 재료 과학 분야의 ML/AI에 관심을 표명하며 IRL(실제로) 사람들을 만나기를 기대하고 있습니다.

---

### Latent Space ▷ #tech-discussion-non-ai (74 messages🔥🔥):
> Cloudflare's Vinext Framework, Traffic-Aware Pre-Rendering, TanStack Start RSC Support, Open Spec vs. Open Source, tldraw licensing
- Vinext is here to solve deploy problems: Cloudflare는 배포 문제, 특히 대규모 사이트의 긴 빌드 시간을 해결하기 위해 설계된 Next.js 대안인 Vinext를 소개했습니다. 자세한 내용은 이 블로그 게시물에 설명되어 있습니다.

Vinext는 트래픽 패턴을 분석하여 가장 자주 방문하는 페이지만 사전 렌더링하는 트래픽 인식 사전 렌더링(TPR)을 구현하며, 빌드 시간을 크게 단축할 수 있고 다른 프레임워크에도 좋은 기능이 될 수 있다고 제안합니다.
- Tests can be a new moat: 한 멤버가 'Tests are the New Moat'라는 블로그 게시물을 발행하고 Chat SDK Template과 Vercel의 새로운 Chat SDK Library를 링크했습니다.

잘 명시된 테스트의 아이디어는 높이 평가되지만, AI 모델의 미묘한 불일치나 환각을 완전히 방지하지 못할 수 있다고 언급되었습니다.
- Debate heats up about open spec vs. open source: 'Open Spec vs. Source Code'라는 트윗이 공유되었는데, 이는 오픈 스펙이 소스 코드보다 더 중요할 수 있다고 논의하며, 소스 코드가 주로 VM 및 컴파일러를 위한 중간 표현으로 기능한다고 제안합니다.

Vinext의 저자는 'Open Source Privacy for Test Suites'라는 트윗을 뻔뻔하게 올리며 SQLite와 같은 프로젝트가 내부 테스트 스위트를 비공개로 유지하는 미래를 예측했습니다.
- tldraw licensing: 멤버들은 tldraw 라이선스와 기여자 라이선스 계약을 분석했습니다.

합의는 라이선스가 비독점적인 저작권/특허 부여를 요구한다는 것이었습니다.
- TanStack Start is not quite RSC: 멤버들은 TanStack Start의 RSC (React Server Components) 접근 방식에 대해 논의했으며, 이는 표준 구현과 상당히 다른 것으로 보이며, 로더 내에서 서버 함수를 사용하여 JSX를 반환한다고 언급했습니다.

이 접근 방식은 서버 우선 접근 방식 및 적절한 구성과 같은 주요 이점을 잃는 것으로 보이지만, 현재 API가 최종 버전이 아닐 수도 있다고 추측되었습니다.

---

### Latent Space ▷ #san-francisco-sf (2 messages):
> OpenClaw Workshop, Embeddable Web Agent Launch
- OpenClaw Hands-On Workshop scheduled: 한 멤버는 다음 주 목요일 팔로알토에서 OpenClaw 실습 워크숍을 발표했습니다. 여기에서 신청하세요.

도시에 계시다면 꼭 들러주세요!
- First Embeddable Web Agent Launch Party: 첫 번째 임베더블 웹 에이전트 출시 파티가 발표되었습니다. 자세한 정보는 여기에서 확인하세요.

새로운 임베더블 웹 에이전트를 가장 먼저 보러 오세요.

---

### Latent Space ▷ #ai-announcements (1 messages):
swyxio: https://youtu.be/x9rWFiIubmc
Claude 코드 기념일을 위한 새로운 팟캐스트!

---

### Latent Space ▷ #ai-general-news-n-chat (63 messages🔥🔥):
> GPT-5.3-Codex Release, Mercury 2 Reasoning Diffusion LLM, Cognition Devin 2.2, Cursor AI Video Demos, Autonomous Dogfooding
- GPT-5.3-Codex Released for Devs!: OpenAI 개발자들은 Responses API를 통해 모든 개발자를 위한 GPT-5.3-Codex의 즉각적인 가용성을 발표했으며, 새로운 모델로 구축을 시작하도록 초대했습니다 (announcement link).
- Mercury 2: Reasoning Diffusion Model Launched!: Stefano Ermon은 추론 확산 LLM인 Mercury 2의 출시를 발표하며, 이는 기존의 속도 최적화된 언어 모델보다 5배 빠르다고 주장합니다 (announcement link).
- Cognition’s Devin 2.2 Gets an Upgrade!: Cognition은 컴퓨터 사용 기능, 자체 검증 및 자동 수정 기능을 갖춘 업그레이드된 자율 AI 에이전트 Devin 2.2를 출시했습니다 (announcement link).

이 업데이트에는 3배 빠른 시작 속도, 가상 데스크톱을 갖춘 재설계된 UI, 그리고 다양한 UX 개선 사항이 포함되며, 이제 무료 체험판으로 이용 가능합니다.
- Cursor AI Introduces Video Demos for Agents: Cursor AI는 AI 에이전트가 간단한 코드 차이점 대신 비디오 데모를 통해 작업을 시연할 수 있는 새로운 기능을 도입하여, 사용자들이 소프트웨어가 작동하는 것을 볼 수 있게 합니다 (announcement link).

커뮤니티 멤버들은 Cursor가 다른 경쟁자들과의 격차를 좁히고 있으며, 이제 더 긴 루프와 더 자율적인 작업을 수행하는 것처럼 보이지만, 필요할 때 IDE를 여전히 유지한다고 언급하며 "이제 우리는 배관공이 되는가"라고 질문했습니다.
- OpenClaw: Open Source Operating System for AI Automation: Matthew Berman은 그의 회사가 OpenClaw를 핵심 운영 체제로 활용하는 방법을 상세히 설명하며, 이메일 관리, CRM 시스템, 회의 인텔리전스 및 재무 추적에 대한 통합을 다룹니다 (announcement link).

이 스레드는 Anthropic OAuth 허점 수정, 보안 프로토콜, 멀티 프롬프트 버전 관리, 그리고 50억 토큰 사용량에 걸친 강력한 로깅 인프라를 포함한 특정 기술 솔루션을 강조합니다.

---

### Latent Space ▷ #llm-paper-club (33 messages🔥):
> Midtraining, OpenClaw, Frontier Model Training, Developer Productivity with AI, METR
- Midtraining Magic: Timing is Everything!: Emmy Liu의 새로운 사전 인쇄본은 '미드트레이닝'을 탐구하며, 이는 사전 학습과 사후 학습 사이의 다리 역할을 하여 망각을 완화하는 데 가장 효과적임을 보여주지만, 그 성공은 정확한 타이밍에 달려 있습니다.

이 연구는 통제된 실험을 사용하여 AI 파이프라인에 대한 미드트레이닝의 영향을 입증합니다.
- OpenClaw’s Early Adventures: Natalie Shapira는 @openclaw 프로젝트와의 다학제적 협력에서 얻은 초기 경험과 발견을 공유했습니다.

프로젝트입니다!
- Frontier Training Favors Systems: Logan Thorneloe는 프론티어 모델 학습에 대한 자료를 공유했으며, 성공이 사소한 알고리즘 조정보다는 시스템 문제(데이터 혼합, 아키텍처, 안정성)에 더 많이 달려 있음을 강조합니다. 이 가이드는 학습 플레이북, 옵티마이저, 강화 학습 및 안전성을 다룹니다.

여기에서 가이드에 액세스하세요.
- Developer Defection from Dull ‘No-AI’ Groups!: METR은 개발자들이 'AI 없음' 대조군을 점점 더 거부한다는 것을 발견했으며, 특히 낮은 급여(시간당 $50 대 원래 $150)에서는 비효율적이거나 매력적이지 않다고 간주했습니다.

이러한 행동 변화는 AI가 워크플로우에 필수적인 부분이 되었음을 시사하며, 전통적인 RCT를 실행하기 더 어렵게 만듭니다. AI가 벤치마크가 따라잡을 수 있는 속도보다 더 빠르게 진화하고 있기 때문에, METR은 관찰 데이터, 에이전틱 도구, 그리고 더 나은 규정 준수 조치를 통합하기 위해 실험을 재설계하고 있습니다 (METR의 트윗 링크).
- METR의 측정 방식 입장 변화!: METR (이전 METR_Evals)은 AI 지원 개발자 생산성 20% 감소라는 이전 연구 결과가 시대에 뒤떨어졌다고 보고했습니다.

현재 데이터는 속도 향상이 가능함을 시사하지만, 개발자 행동의 최근 변화로 인해 새로운 결과가 신뢰할 수 없게 되었으며, 해당 조직은 더 정확한 평가를 위해 노력하고 있습니다 (METR의 트윗 링크).

---

### Latent Space ▷ #ai-in-action-builders-techstacks-tips-coding-productivity (23 messages🔥):
> API 500 errors, Anthropic outage, DSPy-tuned multi-label classifier, surf-cli and chromium sandboxing
- API 500 오류가 사용자들을 괴롭힙니다: 사용자들은 메시지 {“type”:“error”,“error”:{“type”:“api_error”,“message”:“Internal server error”}와 함께 API Error 500을 자주 수신한다고 보고했습니다.

다른 사용자들은 Anthropic이 여러 모델에서 높은 오류율로 인해 다운되었다고 지적했습니다.
- Anthropic 모델, 높은 오류율로 어려움 겪어: 여러 모델에서 높은 오류율로 인해 한 사용자는 일시적으로 Codex로 전환했습니다.

사용자는 Claude의 응대 방식이 Codex보다 훨씬 좋다고 언급했으며, Codex는 기술적으로 밀도 높은 출력을 생성했습니다.
- DSPy, 멀티 레이블 분류기를 파인튜닝하여 콘텐츠 조정에 활용: 한 멤버는 Haiku 모델을 사용하여 새로운 테스트 케이스를 계속 수집하고 이를 학습/테스트 샘플로 변환하는 파이프라인과 함께 DSPy로 파인튜닝된 멀티 레이블 분류기를 사용합니다.

이 멤버는 이 작업을 메인 태스크 처리와 병렬로 시작하고, 질문이 범위를 벗어나면 진행 중인 태스크를 취소하여 레이턴시를 절약한다고 덧붙였습니다.
- Surf-CLI와 Chromium Sandboxing이 난관을 제시합니다: 한 멤버는 surf-cli 작업으로 돌아와 Snap을 통해 샌드박스 처리된 Chromium을 다루는 것이 쉽지 않다고 언급했습니다.

다른 멤버는 자신의 진행 상황을 보여주는 Gist 링크를 공유하고, 샌드박스 내의 노드가 까다롭기 때문에 Go 포트를 고려 중이라고 언급했습니다.

---

### Latent Space ▷ #share-your-work (3 messages):
> InstantClaw, OpenClaw deployment, Codaph CLI, Mubit, hypervectors and clustering
- InstantClaw, OpenClaw 배포를 간소화하다: 한 사용자가 비기술 사용자를 위한 OpenClaw 배포 도구인 InstantClaw를 공유했습니다. 이 도구를 통해 서버 구성 없이 1분 이내에 OpenClaw 기능을 사용할 수 있습니다.

이 도구와 관련이 없는 이 사용자는 친구들에게 동일한 기능을 제공하면서도 배포 지원 시간을 몇 시간 절약하는 데 유용하다고 평가했습니다.
- Codaph CLI, Codex 프롬프트 동기화: 한 멤버가 Codex 프롬프트, 에이전트 추론, 파일 diff를 공유 메모리에 동기화하여 팀 간에 더 풍부한 코드베이스 이해를 목표로 하는 CLI 도구인 Codaph를 소개했습니다.

하이퍼벡터와 클러스터링을 사용하는 연관 검색 기반 메모리 엔진인 Mubit (mubit.ai) 위에 구축된 Codaph는 오픈소스이며 현재 Codex와 작동하며, 다른 에이전틱 도구도 지원할 계획입니다.
- Mubit 메모리 엔진, 하이퍼벡터 활용: Codaph의 기반이 되는 Mubit 메모리 엔진은 시간 기반 감쇠를 적용한 하이퍼벡터와 클러스터링을 사용하여 연관 검색을 수행합니다.

이것은 무료로 제공되며, API 키는 console.mubit.ai에서 접근할 수 있습니다.
- 도구 사용 및 표기법이 일반화 형성에 미치는 영향: 한 멤버가 프롬프팅에 대한 통찰력을 공유하며, 일반화 형성(generalization shaping)과 관련하여 도구 사용 및 표기법에 대한 토론 링크를 제공했습니다.

도구 사용 및 표기법이 일반화 형성에 미치는 영향에 대해 더 읽어보세요.

---

### Latent Space ▷ #robotics-and-world-model (8 messages🔥):
> Wayve, SONIC, Autonomous driving, Humanoid robots, AI licensing
- Wayve, 15억 달러 규모 시리즈 D 라운드 유치: Alex Kendall에 따르면, Wayve는 소프트웨어 라이선싱을 통해 'Embodied AI'를 상용화하기 위해 15억 달러 규모의 시리즈 D 라운드를 확보했으며, 이로써 회사 가치는 86억 달러로 평가되었습니다.
- Wayve의 로보택시 로드맵 공개: SoftBank, Microsoft, NVIDIA, Uber와의 파트너십을 바탕으로, Wayve는 2026년부터 10개 도시에서 감독형 로보택시 시험 운행을 시작하고, 2027년에는 소비자 차량 판매를 이어갈 계획입니다.
- SONIC: 시스템 1 휴머노이드 제어, 오픈소스로 비상: Jim Fan은 자신의 트윗을 통해 휴머노이드 로봇을 위한 '시스템 1' 반응형 지능을 제공하기 위해 1억 개 이상의 모션 캡처 프레임으로 학습된 4,200만 개 파라미터 트랜스포머인 SONIC을 소개했습니다.
- NVIDIA Isaac Lab, SONIC의 성공을 시뮬레이션: NVIDIA Isaac Lab을 사용하여 대규모 병렬 시뮬레이션을 통해 SONIC 모델은 제로샷(zero-shot) 실제 환경 전이를 달성하며 VR, 비디오, 텍스트, 오디오를 통한 제어를 지원합니다.

---

### Latent Space ▷ #genmedia-creative-ai-video-image-voice-music-inspo-consumer-ai (8 messages🔥):
> Quiver AI, Arrow-1.0 Model, KREA AI, Seedream 5 Lite
- Quiver AI, Arrow-1.0으로 시동 걸다: Quiver AI는 벡터 디자인 AI 연구소로 공식 출범했으며, a16z가 주도하는 시드 펀딩에서 830만 달러를 확보했습니다.

이들의 첫 모델인 Arrow-1.0은 이미지와 텍스트를 SVG로 변환하며, 현재 공개 베타로 제공됩니다.
- KREA AI, Seedream 5 Lite 모델 출시: KREA AI는 저비용 이미지 편집 모델인 Seedream 5 Lite를 출시했습니다.

이 모델은 'Nano Banana' 모델과 유사한 성능을 더 낮은 가격에 제공하도록 설계되었습니다.

---

### Latent Space ▷ #mechinterp-alignment-safety (6 messages):
> Interpretability Hiring, Anthropic, ML Infrastructure
- Anthropic, 해석 가능성(Interpretability) 팀 대규모 채용 시작: Chris Olah는 이 트윗에서 볼 수 있듯이 Anthropic이 해석 가능성(Interpretability) 팀을 위해 약 10명의 연구 엔지니어를 찾고 있다고 발표했습니다.
- ML 인프라 엔지니어 모집: 이 역할은 모델 내부에 관심 있는 숙련된 ML 인프라 엔지니어를 대상으로 하며, 이전 해석 가능성(interpretability) 경험은 필요하지 않습니다.

fxtwitter에 언급된 기회를 포함하여 많은 기회가 있습니다.

---

### Latent Space ▷ #applied-ai-experimentation (43 messages🔥):
> Claude Code Limitations, Codex as an Alternative, Agentic Engineering Strategies, Pi Agent Loop, Tool Use and Notation
- Claude Code, API 통합 문제에 직면: 한 멤버는 더 큰 태스크에 대한 Claude Code에 환멸을 표하며, API가 “완료”되었음에도 불구하고 종종 사양을 완전히 충족하지 못하여 시스템의 여러 계층에서 통합 문제를 일으킨다고 언급했습니다.

그들은 이제 강력한 시스템 구축을 위한 대안으로 Codex를 고려하고 있습니다.
- Pi, Codex로 OpenClaw에 동력 공급: 한 멤버는 OpenClaw에 동력을 공급하는 에이전트 루프인 Pi 내에서 Codex를 사용할 것을 제안하고, 사용자들이 기여할 수 있도록 Pi의 패키지 링크와 YouTube 비디오를 공유했습니다.

다른 멤버는 LLM이 해당 하네스 내에서 RL(강화 학습)되었기 때문에 '공식' 코딩 하네스인 Claude Code와 Codex를 고수하는 것이 가장 좋다고 말했습니다.
- 에이전틱 엔지니어링에 '안티-스펙' 접근 방식 선호: 한 멤버는 에이전틱 엔지니어링에서 상세한 사전 사양에 반대하며, 반복, 실패 검증, 가지치기를 강조하고, 사양은 나중에 구축되어야 한다고 제안했습니다.

그들은 이것이 주로 잘못된 통제감과 자부심에서 비롯된다고 말했습니다.
- 도구 사용 및 표기법이 LLM 일반화 형성: 한 멤버가 자신의 작업을 홍보하며, 일반화 형성으로서의 도구 사용 및 표기법에 대한 블로그 게시물을 공유했습니다.

다른 멤버는 그것이 자신이 참여하고 있던 변증법에 완벽하게 들어맞는다고 생각했습니다.
- 와일드 시스템 프롬프팅을 통한 신속한 프로토타이핑: 한 멤버는 golang, watermill, redis와 같은 도구를 사용하여 이벤트 기반 아키텍처에 중점을 두고, 최소한의 프롬프팅과 수동 검토를 통해 혁신적인 API를 갖춘 복잡한 시스템을 구축하는 접근 방식을 설명했습니다.

그들은 TUI를 yolo 프로토타입과 병합하는 구체적인 예시를 공유했으며, 임베딩 및 벡터 검색을 포함한 cozodb 관련 기능과 JS API를 포함하는 핵심 패키지를 생성했습니다.

---

### Nous Research AI ▷ #announcements (1 messages):
> Hermes Agent, Open Source Agent, Multi-level memory system, Persistent dedicated machine access, CLI Integration
- Hermes Agent: 오픈소스 에이전트 출시: Nous Research는 다단계 메모리 시스템과 영구적인 전용 머신 접근 권한을 갖춘 오픈소스 에이전트인 Hermes Agent를 출시했습니다. 이 에이전트는 사용자와 함께 성장하도록 설계되었습니다.

Hermes Agent는 CLI, Telegram, WhatsApp, Slack, Discord에서 실행될 수 있으며, 사용자가 어디를 가든 세션을 이어받고 전송합니다.
- Hermes Agent, 고급 기능 및 광범위한 통합으로 구동: Hermes Agent는 서브 에이전트 제어, 프로그래밍 방식 도구 호출, 고급 파일 시스템/터미널 제어, 에이전트 관리 스킬, 브라우저 사용과 같은 고급 에이전틱 기능을 제공합니다.

이것은 OpenRouter 및 Nous Portal 구독으로 구동되며, CLI 통합 및 메시징 플랫폼 지원을 제공합니다.
- 무료 한 달 프로모션 및 개발자 친화적 디자인 공개!: portal.nousresearch.com에서 처음 750명의 신규 가입자는 쿠폰 코드 HERMESAGENT를 사용하여 한 달 무료 혜택을 받습니다.

Hermes Agent는 오픈소스이며 Python으로 구축되어 개발자가 쉽게 확장할 수 있습니다.
- 에이전틱 RL 파이프라인 및 대규모 데이터 생성 강화: Hermes Agent는 또한 에이전틱 RL 파이프라인에 동력을 공급하여 Atropos를 확장하여 Hermes Agent 프리미티브를 통한 RL을 가능하게 하며, 대규모 데이터 생성을 즉시 지원합니다.

GitHub 리포지토리를 확인하거나 터미널에서 한 번의 명령으로 설치하세요: `curl -fsSL https://raw.githubusercontent.com/NousResearch/hermes-agent/main/scripts/install.sh | bash`.

---

### Nous Research AI ▷ #general (96 messages🔥🔥):
> Qwen Base Model Release, Codex 5.3 API Pricing, Steinberger's OpenClaw Process, OS Frontier Math Level 4 Update, NousChat Development
- Qwen의 기본 모델 가중치 공개: Qwen은 Hugging Face에서 Qwen3.5-35B-A3B 모델의 기본 가중치를 공개했습니다.
- Codex 5.3의 새로운 가격 책정: Codex 5.3이 API로 출시되었으며, 새로운 가격 구조는 입력에 1.75달러, 출력에 14달러입니다.
- Steinberger의 OpenClaw: 바이브 코딩의 기적: Steinberger는 OpenClaw가 어떻게 만들어졌는지 설명하는 비디오를 공개했습니다. 이는 그의 이전 계획, 아이디어 및 코드 스니펫에서 AI를 통해 추출되었고, 이를 AI에 제공하여 새로운 코드를 만들었습니다.

그는 자신의 소프트웨어가 무엇을 하는지 전혀 모르며, 그 구조는 단지 채널들의 스택일 뿐이라고 말했습니다.
- OS Frontier Math 레벨 4 업데이트: Kimi 2.5 (1st OS)는 4.2%를 기록했고, Glm 5와 V3.2는 2.1%를 기록했습니다.
- NousChat, Kimiclaw와 정렬되도록 진행 중: 한 멤버가 Kimiclaw와 같은 서비스를 호스팅할 계획에 대해 문의했고, 다른 멤버는 NousChat이 그와 정렬된다고 말할 수 있는 방식으로 진행 중이라고 답했습니다.

---

### Nous Research AI ▷ #research-papers (2 messages):
> Arxiv Paper
- Arxiv 논문 공유: 한 멤버가 Arxiv 논문 링크를 공유했습니다: https://arxiv.org/abs/2602.16800.
- 흥미로운 발견: 다른 멤버는 흥미로운 논문이라고 답했습니다.

---

### Nous Research AI ▷ #research-papers (2 messages):
> Arxiv Paper
- 새로운 Arxiv 논문 공유: 한 멤버가 새로운 Arxiv 논문 링크를 공유했습니다: https://arxiv.org/abs/2602.16800.
- Arxiv 논문, 주목받다: 다른 멤버는 흥미로운 논문이라고 언급했습니다.

---

### Eleuther ▷ #general (49 messages🔥):
> Pythia-2.8b Checkpoint Issues, Hugging Face serving weights, safetensors and HF, deduped models, Voice AI Model Sesame AI
- Pythia-2.8b 체크포인트 버그, 조사 착수: 한 멤버가 pythia-2.8b 체크포인트를 사용하여 논문을 재현하려던 중 버그에 직면했으며, 선택한 리비전과 관계없이 Hugging Face가 동일한 가중치를 제공하고 있음을 발견했습니다.

pytorch_model.bin과 model.safetensors의 SHA256 해시가 다른 단계에서도 동일하여 체크포인트의 무결성에 대한 우려를 제기했습니다.
- HF 샤드, 대역폭 절약: 멤버들은 pythia-2.8b의 샤딩된 safetensors 파일은 단계별로 다르지만, 샤딩되지 않은 파일은 동일하다는 것을 발견했으며, 이는 HF가 모델을 로드하고 샤딩을 처리하는 방식에 대한 논의로 이어졌습니다.

한 멤버는 UltraChat과 기본 Mistral 간의 차이를 Mistral-Yarn에 적용하는 것을 잠재적인 병합 전략으로 제안했습니다.
- 더 작은 Pythia 모델들, 중복 제거(Deduped)되다: EleutherAI는 잘못 레이블링된 14m 및 30m 모델(둘 다 중복 제거된 버전)을 수정하고 있으며, 이를 대체할 중복된 모델을 학습시켜 레이블링 문제를 명확히 하고 있습니다.

한 멤버는 일부 업로드 혼동 문제를 해결하고 밤새도록 수정 작업을 실행했다고 언급했습니다.
- 멤버, HF가 심볼릭 링크 꼼수로 디스크 공간 절약했다고 추측: 한 멤버는 과거에 비슷한 실수를 저질렀음을 깨닫고, Hugging Face가 디스크 공간을 절약하기 위해 심볼릭 링크를 사용했을 수 있으며, 이로 인해 데이터 손상이 발생했을 수 있다고 추측했습니다.

이 이론은 pythia-2.8b 체크포인트 문제가 HF의 내부 저장 공간 관리 프로세스 때문일 수 있음을 시사합니다.
- Sesame AI 음성 모델, 호기심 자극: 한 멤버가 Sesame AI 음성 AI 모델에 대해 질문하며, 명확한 정렬(alignment)과 Gemma 모델 기반 가능성을 언급하여 그 기능에 대한 논의를 촉발했습니다.

다른 멤버는 Sesame AI가 ASR, LLM, TTS를 통합한 저 레이턴시 음성 시스템에 중점을 두고 있음을 강조하며, 통찰력을 얻기 위해 Moshi 논문을 검토할 것을 제안했습니다.

---

### Eleuther ▷ #research (22 messages🔥):
> Diffusion Papers, Flow Matching vs Diffusion, Pythia Models
- 확산(Diffusion) 문헌 심층 분석: 멤버들은 Latent Diffusion Model 이후의 주요 확산(diffusion) 논문들을 논의하며, Rectified Flows, Flow Matching, Diffusion Forcing을 강조했습니다.

ByteDance Seed와 Hunyuan의 논문들(예: https://arxiv.org/abs/2509.20427, https://arxiv.org/abs/2509.23951)도 언급되었으며, YouTube 재생 목록이 자료로 추천되었습니다.
- Flow Matching의 유동적인 기반: Flow Matching이 시간적으로 연속적이고 가역성을 요구하지 않기 때문에 이전 연구(https://arxiv.org/abs/1807.03039)와 관련이 있지만 구별된다는 점이 논의에서 명확해졌습니다.

한 멤버는 Flow Matching이 확산(diffusion) 연구에서 더 많이 나왔으며, 흐름(flow)을 매개변수화하는 대안적인 방법을 나타낸다고 언급했습니다.
- Louie의 잠재 링크 로지스틱스: 한 멤버가 확산(diffusion) 파이프라인의 잠재(latent) 부분에 관한 논문 링크가 포함된 블로그 게시물(https://over.world/blog/dito)을 언급했습니다.

언급된 논문에는 https://arxiv.org/abs/2512.12386이 포함되어 있으며, https://arxiv.org/pdf/2510.11690과 같은 다른 논문들을 참조하고 Token Routing, Path-Drop Guidance, 잠재 임베딩의 Representation Alignment와 같은 새로운 방법들을 포함합니다.
- Pythia에 대하여!: Pythia 모델에 대한 링크가 공유되었습니다.

---

### Eleuther ▷ #lm-thunderdome (3 messages):
> lm-evaluation-harness, MMLU pro eval, Qwen3 models, HF backend, vLLM backend
- lm-eval Harness를 위한 빠른 vLLM 백엔드: 한 멤버가 lm-evaluation-harness에서 vLLM 백엔드를 사용하여 단일 토큰 답변을 가진 다중 선택 태스크의 평가 속도를 높이는 것을 목표로 하는 풀 리퀘스트에 대한 검토를 요청했습니다.

이 속도 향상은 HF 백엔드와 비교했을 때의 느린 문제, 특히 MMLU pro eval과 같은 태스크에서 발생하는 문제를 해결해야 합니다.
- Qwen3 모델의 개행 문자: 한 멤버가 lm-evaluation-harness에서 Qwen3 모델의 예상치 못한 개행 문자 동작에 대해 문의했습니다. 여기서 \n\n이 continuation으로 이동하며, 잠재적으로 관련될 수 있는 이슈 2144를 링크했습니다.

사용자는 로그 출력에서 컨텍스트와 continuation을 포함한 예시를 제공했습니다.

---

### HuggingFace ▷ #general (40 messages🔥):
> ZeroGPU Allocation Issues, Small Language Models, Edge Inference Memory, Code RAG, Tiny Aya
- Gradio 버전, ZeroGPU 할당 문제 유발: 사용자들은 ZeroGPU 할당 문제를 보고했으며, 이는 Gradio 5.12.0 이전 버전의 로그인 버그와 관련이 있을 수 있습니다.

컨테이너 로그를 확인하면 Gradio, spaces 라이브러리 또는 HF 서버가 문제의 원인인지 밝혀낼 수 있으며, 빈 커밋 후 재구축하는 것도 버전 관련 문제를 해결할 수 있습니다.
- Cohere, Tiny Aya 출시: Cohere는 최근 Tiny Aya를 출시했습니다.
- 독립 개발자, 엄청난 엣지 메모리 장벽을 돌파하다: 한 독립 개발자가 MiniMax-m2.5의 5GB MoE 샤드를 2MB 벡터 양자화 잠재 공간으로 압축했다고 주장합니다.

그들은 arXiv (cs.LG)에 제출할 논문을 준비 중이며, 자신들의 “블랙 매직 엣지 AI 기술”을 검토해 줄 추천인을 찾고 있습니다.
- 코드 RAG 발명, 프로젝트 확장을 위해 등장: 한 멤버가 프로젝트 확장을 위해 Code RAG를 발명하고 있으며, 이미 “절반 정도 완성”했다고 주장합니다.

그들은 코드가 다른 코드와 어떻게 관련되는지 보여주는 그래프를 공유했습니다.
- 디스틸레이션 학습의 어려움: 한 멤버가 디스틸레이션 학습에 유용한 지침을 요청하고 있는데, 그 이유는 자신의 “학생 모델이 교사 모델처럼 생각하지 않았기” 때문입니다.

그들은 “스스로 학습하는 것이 LLM을 학습시키는 것과는 많이 다르다”고 말했습니다.

---

### HuggingFace ▷ #i-made-this (11 messages🔥):
> Distributed Fine-tuning, GPT-OSS, Qwen 2.5, Mistral models, RTH-LM model
- Zagora, 분산 파인튜닝 시스템 구축: Zagora의 한 멤버는 표준 인터넷을 통해 70B+ 모델을 학습시키기 위한 분산 파인튜닝 시스템을 구축하고 있으며, 분산된 GPU를 통합된 학습 슈퍼컴퓨터로 전환하고 있다고 발표했습니다.

이 플랫폼은 현재 GPT-OSS, Qwen 2.5, Mistral을 지원하며, Petals와 SWARM Protocol에서 영감을 받은 파이프라인 스타일 학습 접근 방식을 사용합니다.
- RTH-LM, Zagora 시스템 스트레스 테스트 가능: 한 멤버는 자신의 RTH-LM 모델(비 트랜스포머 모델인 Fractal Gated Causal TCN)이 파이프라인 단계에서 노드 간 상태 동기화 오버헤드가 없기 때문에 Zagora 시스템의 완벽한 스트레스 테스트가 될 수 있다고 제안했습니다.

그들은 120B 규모를 목표로 하며, 플랫폼이 트랜스포머 계열 외에 사용자 정의 모델 아키텍처(모든 nn.Module)를 지원하는지 물었고, 자신의 논문, 리포지토리, 25B 모델을 언급했습니다.
- Zagora, 트랜스포머 모델에 집중: Zagora 팀은 현재 Llama, Qwen, Mixtral, Gemma와 같은 트랜스포머 계열 모델에 집중하고 있다고 답했습니다.

하지만 그들은 RTH-LM이 트랜스포머 호환 래퍼를 얻게 된다면 통합 가능성을 다시 검토할 것이라고 언급했습니다.
- webXOS 블랙홀 타임랩스 데이터셋 공개: 한 멤버가 webXOS에서 Three.js 시뮬레이션으로 생성된 중력 렌즈 효과가 있는 합성 블랙홀 렌더링을 포함하는 webXOS 블랙홀 타임랩스 데이터셋을 공유했습니다.

각 샘플에는 PNG 이미지의 타임랩스 시퀀스와 관련 물리적 매개변수가 포함되어 있어 멀티모달 모델 학습, 물리학 기반 ML 또는 위성 이미지 연구 비유에 이상적입니다.
- 가장 안전한 곳에서 모델 최적화: 한 멤버가 런타임을 변경하거나 새로운 인퍼런스 제공업체에 의존하지 않고 모델을 최적화하는 데 적용할 수 있는 다양한 유형의 방법과 관찰할 수 있는 결과를 설명하는 '가장 안전한 곳에서 최적화: 모델 우선 접근 방식'이라는 기사를 게시했습니다.

---

### HuggingFace ▷ #agents-course (6 messages):
> agents course, smolagents, Qwen API, huggingface/agents-course
- 에이전트 코스 채널 찾기: Hugging Face 에이전트 코스 신규 참여자들이 코스 자료에 언급된 특정 채널을 찾는 데 어려움을 겪고 있습니다.

한 멤버에 따르면, 채널들이 단일 채널로 병합된 것으로 보이며, agents-course 리포지토리의 PR #653을 링크했습니다.
- Smolagents 퀴즈 문제: 한 멤버가 smolagents 최종 퀴즈 코드가 평가되는 것을 막는 경고, 특히 API 오류를 겪고 있습니다.

오류 메시지는 https://api-inference.huggingface.co가 더 이상 지원되지 않으며 대신 https://router.huggingface.co를 사용할 것을 제안하며, 이는 Qwen API와 관련이 있습니다.

---

### GPU MODE ▷ #general (15 messages🔥):
> Kernel Programming Environment Setup, HPC Systems with GPUs, TPUs, or Soft GPUs on FPGAs, Performance Modeling for Accelerators, GraphCulon
- GPUmode.com, 유지보수로 일시 중단: GPUmode.com은 유지보수를 위해 일시 중단되었으나 곧 다시 가동되었습니다.
- 사용자들, 커널 프로그래밍 환경 설정 논의: 한 멤버가 다른 사람들의 선호하는 커널 프로그래밍 환경 설정에 대해 질문하며, Modal이 유용하지만 대회 외부에서는 NCU 프로파일링이 부족하다고 언급했습니다.
- Calculon 도구, 시스템의 고수준 공동 설계 가능하게 해: 한 멤버가 시스템의 고수준 공동 설계를 위한 방법론이자 도구인 Calculon 링크를 공유했습니다.
- GraphCulon, 흥미로워 보여: 한 멤버는 GraphCulon이 실제로 흥미로워 보이지만 아직 출시되지 않았다고 언급하며, 이에 대한 강연 링크를 제공했습니다.
- GPU 관측 가능성 세미나 시작: GPU 관측 가능성 세미나가 시작되었으며, 발표자는 슬라이드를 공유하겠다고 약속했습니다.

---

### GPU MODE ▷ #cuda (1 messages):
> cuda::memcpy_async, SMEM bank conflict
- cuda::memcpy_async 사용 시 SMEM 뱅크 충돌: 한 사용자가 GMEM에서 SMEM으로 데이터를 전송하기 위해 cuda::memcpy_async를 사용할 때 SMEM 뱅크 충돌이 중요한 문제인지 문의했습니다.

사용자는 SMEM 뱅크 충돌이 주로 SMEM에 대한 워프(warp) 접근과 관련이 있다고 가정하며, 이 시나리오에서는 큰 문제가 아닐 수 있다고 제안했지만, 추가적인 관점을 구했습니다.
- GMEM에서 SMEM으로의 전송 고려 사항: 논의는 CUDA 내에서 메모리 전송 전략을 최적화하는 것, 특히 cuda::memcpy_async 사용에 초점을 맞추고 있습니다.

핵심 질문은 메모리 복사의 비동기적 특성이 SMEM 뱅크 충돌 가능성에 영향을 미치는지 여부이며, 이는 신중한 고려가 필요합니다.

---

### GPU MODE ▷ #torch (3 messages):
> FA3 Kernels, SDPA Backend Selection, Blackwell GPUs
- FA3 커널, PyTorch 디스패치에서 FA2 대체: 사용자가 activate_flash_attention_impl(“FA3”)를 호출하면, restore_flash_attention_impl이 호출되어 기본 FA2 커널이 복원될 때까지 디스패치 테이블에서 기본 FA2 커널이 FA3 커널로 오버라이드됩니다.

이는 버전 이름을 호출 가능한 함수에 매핑하는 딕셔너리에 키-값 쌍 {“FA3”, register_fn}을 추가하고, register_fn (여기서 정의됨)을 실행하여 PyTorch 디스패처에 FA3 커널을 등록함으로써 달성됩니다.
- SDPA, GPU 장치 기반으로 FA 백엔드 선택: SDPA에서 Flash Attention (FA) 백엔드 선택은 GPU 장치에 따라 달라지며, select_sdp_backend 함수 (여기서 정의됨)를 사용하여 SDP 백엔드의 우선순위를 선택합니다.

기본 순서는 flash, mem efficient, 그 다음 math이지만, 사용자는 특정 백엔드를 활성화하기 위해 이를 오버라이드할 수 있습니다. 예를 들어, Blackwell GPU의 경우 flash attention이 작동하지 않으므로, check_prefer_cudnn_attention의 이 줄에 의해 결정되는 첫 번째 우선순위는 cuDNN입니다.

---

### GPU MODE ▷ #announcements (1 messages):
> eBPF, GPUs, profilers, OS Policies
- eBPF, GPU 지원 강화: Yusheng Zheng은 [날짜] 오후 12시(PST)에 eBPF가 GPU와 더 잘 작동하도록 확장하는 방안에 대해 논의할 예정입니다.

이 강연에서는 gpu_ext: eBPF를 통한 GPU용 확장 가능한 OS 정책 (논문) 및 eBPF를 GPU 장치 및 드라이버 컨텍스트로 확장 (LPC 이벤트)과 같은 최근 연구를 다룰 것입니다.
- GPU Mode 프로파일러 파티에 참여하세요: 발표자는 GPU MODE 내에서 더 많은 프로파일러와 프로파일러 시각화 라이브러리가 개발되기를 희망한다고 밝혔습니다.

관심 있는 분들은 참여하여 관련 YouTube 비디오를 시청하여 영감을 얻으시길 권장합니다.

---

### GPU MODE ▷ #beginner (10 messages🔥):
> CUDA learning resources, Distributed inference platforms on GB200 / B200 nodes, Career transition to GPU field, New channel proposal for high-volume questions
- CUDA 초보자들, 커널 수준 지식 탐색: CUDA 커널 수준 작업에 익숙하지 않은 한 멤버가 Dynamo, vLLM, LMCache, NIXL과 같은 오픈소스 프로젝트를 사용하여 GB200 / B200 노드의 분산 인퍼런스 플랫폼을 효과적으로 학습하는 방법에 대한 조언을 구하고 있습니다.

이 멤버는 PMPP로 시작하거나, GPU MODE 대회에 참여하거나, NVIDIA의 CUDA / 성능 강좌를 수강하는 것이 얼마나 도움이 될지 구체적으로 질문했으며, 장기적인 목표는 인퍼런스 오픈소스에 기여하는 것입니다.
- CUDA 초보자에게 PMPP 및 오픈소스 해킹 권장: 한 멤버는 이전 논의를 참조하고 PMPP 1장부터 6장까지 읽은 다음 오픈소스 프로젝트에 기여하여 CUDA를 효과적으로 학습할 것을 권장했습니다.

그들은 재미를 위해 대회에 참여할 것을 장려했습니다.
- GPU 분야 경력 전환 지침 요청: 컴퓨터 공학 학위를 소지하고 소프트웨어 엔지니어로 일하는 한 멤버가 GPU 분야로 진출하여 경력을 쌓는 데 관심을 표명했습니다.

그들은 CUDA와 GPU 프로파일링으로 시작하는 것이 올바른 방향인지 물었고, 이 경로에 접근하는 방법에 대한 지침을 요청했으며, 다른 멤버도 이 요청에 동의했습니다.
- 초보자 질문 봇물 채널 제안: 한 멤버가 PMPP 이해, NCCL 코드베이스, 개인 커널 작업 등 CUDA 학습과 관련된 특정 질문에 대한 대량 논의를 위한 #newb_firehose라는 채널을 제안했습니다.

다른 멤버는 기존의 #beginner 채널이 이 목적을 수행하며, 사용자에게 그곳에서 자유롭게 질문할 것을 권장했습니다.

### GPU MODE ▷ #popcorn (6 messages):
> Hacky Submissions Parsing, KernelBot 환경 개선, 커널 최적화를 위한 RL 환경
- Hacky Submission Parsing 시작: 첨부된 이미지에서 볼 수 있듯이, hacky submission 파싱이 시작되어 핑거프린팅 및 심층 분석이 개시되었습니다.
- KernelBot 환경 증강 질의: 한 멤버가 PrimeIntellect를 통해 KernelBot 환경에 새로운 submission을 추가할지 여부를 문의했습니다.

다른 멤버는 규칙 세트가 검토 후 승인되면 KernelBot에 검증 레이어로 추가될 수 있다고 제안했습니다.
- 커널 최적화 RL 환경에 대한 관심 표명: 한 멤버가 커널 최적화를 위한 RL 환경에 관심을 표명하며 공통 인프라 구축을 제안했습니다.

주어진 메시지에서는 추가적인 세부 사항이나 구체적인 논의가 강조되지 않았습니다.

---

### GPU MODE ▷ #thunderkittens (1 messages):
simran9493: 네!

---

### GPU MODE ▷ #status (1 messages):
> CLI 업데이트, 인증 문제
- CLI 업데이트: 멤버들은 CLI를 최신 버전으로 업데이트하라는 지시를 받았습니다.

이 업데이트에는 기능 개선을 위한 버그 수정 및 새로운 기능이 포함될 가능성이 높습니다.
- 인증 문제 제기: 멤버들은 인증 관련 문제가 발생하면 보고하도록 요청받았습니다.

이러한 선제적 접근 방식은 원활한 접근을 보장하고 중단을 방지합니다.

---

### GPU MODE ▷ #hardware (1 messages):
> B200 GPU, GPU 리싱, Neocloud 솔루션, Lightning AI Clusters
- B200 GPU 가격 충격으로 리싱 권장: 한 사용자가 B200 GPU가 지나치게 비싸다고 언급하며, 비기업 사용자에게는 리싱(leasing) 또는 렌탈이 더 실현 가능한 옵션이라고 조언했습니다.

그들은 자사의 솔루션인 Lightning AI Clusters를 잠재적으로 매력적인 대안으로 강조했습니다.
- Neocloud 리싱이 B200 대안으로 부상: B200 GPU의 높은 비용을 고려할 때, 한 사용자는 특히 기업 환경 외부의 사용자들을 위해 Neocloud 리싱 또는 렌탈 옵션을 탐색할 것을 제안합니다.

사용자는 대안을 찾는 사람들을 위해 Lightning AI의 클러스터 솔루션을 특별히 추천합니다.

---

### GPU MODE ▷ #cutlass (4 messages):
> CuTeDSL 편집 가능한 패키지 설치, CuTeDSL 4.4 릴리스의 호환성 파괴 변경 사항, CuTeDSL의 벡터화된 타일 복사 2D, CuTeDSL의 스레드 값 레이아웃
- CuTeDSL 편집 가능한 패키지 설치 가이드 요청: 한 사용자가 CuTeDSL의 편집 가능한 패키지 설치 가이드를 요청하며, 기존 스크립트가 이해하기 어렵다고 언급했습니다.

그들은 최신 4.4 릴리스가 Python 패키지를 여러 개의 새로운 패키지로 분할하여 제대로 작동하지 않는 것 같다고 언급했습니다.
- 벡터화된 타일 복사 2D 스레드 레이아웃 선호: 한 사용자가 CuTeDSL에서 벡터화된 타일 복사 2D를 수행하기 위한 스레드 값 레이아웃(첨부 이미지 참조)을 선호한다고 밝혔으며, 이것이 더 직관적이라고 생각했습니다.

그들은 quack도 최근 이 레이아웃으로 변경했으며, shape와 stride를 사용한 이전 레이아웃의 코드 스니펫을 포함했다고 언급했습니다.

---

### GPU MODE ▷ #multi-gpu (2 messages):
> Helion 구현, all_gather + FP8 + GEMM 최적화, 커널 프로파일링 및 디버깅
- Helion 구현이 기준선에 뒤처짐: 한 멤버가 vllm-project 풀 리퀘스트를 기반으로 all_gather + FP8 + GEMM (H100)의 Helion 구현을 작업 중이지만, 현재 기준선보다 1.26–4배 느립니다.

목표는 커널을 최적화하고 프로파일링하여 버블과 대기 상태를 검사하는 것이지만, Chrome을 통한 트레이싱은 따라가기 어렵습니다.
- 커널 최적화 도구에 대한 조언 요청: 한 멤버가 커널 최적화를 위한 도구 또는 워크플로우에 대한 권장 사항과 함께 팁, 문서 또는 공유된 경험을 찾고 있습니다.

그들은 트레이싱을 통해 프로파일링을 해왔지만, Meta 데이터 센터 엔지니어링을 구현한 후 실제 병목 현상이 어디에 있는지 파악하고 추론하기가 상당히 어렵습니다.

---

### GPU MODE ▷ #helion (4 messages):
> Helion PR 1418, Helion all_gather + FP8 + GEMM 최적화
- Helion의 병렬 읽기 검토: 한 멤버가 Helion PR 1418이 JAX 문서에 설명된 병렬 읽기 문제를 해결하는지 문의했습니다.

PR 작성자는 이번 주 말 또는 다음 주까지 답변할 수 없습니다.
- Helion의 FP8 GEMM 최적화: 한 멤버가 이 풀 리퀘스트에서 볼 수 있듯이 all_gather + FP8 + GEMM (H100)의 Helion 구현을 작업 중입니다.

현재 구현은 기준선보다 1.26–4배 느리며, 목표는 커널을 최적화하고 프로파일링 도구 및 워크플로우에 대한 조언을 요청하는 것입니다.
- NCU 통찰력 요청: 한 멤버가 커널 최적화에 대한 실행 가능한 통찰력을 얻기 위해 NCU 사용을 제안했습니다.

NCU 사용에 대한 추가 정보는 제공되지 않았습니다.

---

### GPU MODE ▷ #robotics-vla (1 messages):
vovw: 놀라운 작업입니다

---

### GPU MODE ▷ #career-advice (5 messages):
> CUDA 커널, TPU 인퍼런스, MLSys 역할, 분산 학습
- 인퍼런스 및 MLSys 역할에서의 CUDA 커널 지식: 한 멤버가 인퍼런스 및 MLSys 역할에 광범위한 CUDA 커널 지식이 필요한지, 특히 TPU 인퍼런스 경험이 있는 경우에 대해 문의했습니다.

다른 멤버는 학부생으로서 얼마나 많은 CUDA/커널을 알아야 하는지에 대해 비슷한 의구심을 표명하며, 이 분야에 진입하는 사람들 사이의 공통된 우려를 강조했습니다.
- 학습 vs. 인퍼런스: CUDA 커널의 중요성: 한 멤버가 분산 학습 경험을 공유하며, Ampere 아키텍처를 넘어서는 깊은 CUDA 커널 지식이 항상 필수적인 것은 아니지만 분명히 가치 있다고 언급했습니다.

그들은 특정 op를 대체하기 위해 커널을 작성하는 것이 유익했을 상황들을 이야기하며, 학습과 인퍼런스 모두를 아는 것이 도움이 될 수 있지만 엄격하게 요구되는 것은 아니라고 강조했습니다.

---

### GPU MODE ▷ #from-scratch (1 messages):
> Serenade 언어, C++ 트랜스파일링, CUDA 및 x86-64 ASM, GPU 커널, Dear ImGui 지원
- Serenade: “간단한 Python, 빠른 C++” 등장: 한 멤버가 C++, CUDA, x86-64 ASM으로 트랜스파일되는 새로운 언어 Serenade를 소개했습니다. 이 언어는 Python처럼 간단하지만 수동 메모리 관리로 C++처럼 빠르도록 목표합니다.

이 언어는 GPU 커널 지원(serenaCore, 커스텀 BLAS 커널)과 단일 패스 컴파일 시스템을 통한 통합 Dear ImGui 지원을 포함하며, 이를 사용하여 운영 체제를 만들 계획입니다.
- Serenade의 목표: 각 언어의 장점 결합: 개발자는 Serenade가 여러 언어의 장점을 결합한 훌륭한 도구를 만들겠다는 아이디어에 의해 추진되는 단독 이니셔티브임을 강조했습니다.

소스 코드는 현재 비공개이지만, Serenade의 가장 간단한 기능은 Replit에서 브라우저를 통해 테스트할 수 있습니다.

---

### Moonshot AI (Kimi K-2) ▷ #general-chat (15 messages🔥):
> Kimi vs GLM, 에이전트 할당량, 코딩을 위한 Kimi, KimiClaw 브라우저 내비게이션
- Kimi와 GLM 성능 대결: 멤버들은 Kimi와 GLM 5의 성능을 놓고 논쟁했으며, 한 사용자는 Kimi가 GLM보다 100,000배 빠르다고 농담했습니다.

다른 사용자는 GLM 5가 약간 우위에 있지만, 공식 z.AI API를 통해서는 GLM 5가 느리지만 다른 제공업체를 통해서는 더 빠를 수 있다고 언급했습니다.
- 사용자, 에이전트 할당량 충전 요청: 한 사용자가 Allegro의 비용 문제를 언급하며 특히 에이전트 할당량 충전에 대해 문의했습니다.

그들은 또한 nb pro와 함께 제공되던 에이전트 docsis kimi 슬라이드가 더 이상 무료로 제공되지 않는다고 언급했습니다.
- Kimi, 코딩 작업에서 빛을 발하다: Kimi, MiniMax, Alibaba의 코딩 플랜을 테스트한 후, 한 사용자는 코딩을 위해 Kimi를 계속 사용하기로 결정했습니다.

사용자는 속도, 가동 시간, 사용량 제한, 모델 품질을 결정 요인으로 꼽았습니다.
- KimiClaw의 브라우저 사각지대: 한 사용자가 KimiClaw가 브라우저를 독립적으로 탐색할 수 없는 것에 대한 불만을 표했습니다.

그들은 다른 사람들도 같은 문제에 직면했는지 묻고 해결책을 찾았으며, 또한 '큰 파일을 분석/처리할 때 컨텍스트를 줄이고 토큰을 절약하기 위해 Kimi에 무엇을 사용할 수 있을까요? Claude에는 그런 기능이 있는 것 같습니다.'라고 물었습니다.

---

### Manus.im Discord ▷ #general (11 messages🔥):
> Github 재연결 문제, 로컬 개발 환경 설정, 계정 밴, Manus 쿠키 문제, 웹사이트 디자인 문제
- 사용자, Github 재연결 문제에 직면: 한 멤버가 Github 계정 재연결에 어려움을 겪고 있으며, 원래 저장소에 다시 연결하는 대신 새 저장소를 만들라는 메시지가 표시되고 있습니다.

멤버는 자신이 코더나 소프트웨어 개발자가 아니므로 쉽게 이해할 수 있는 지침이 필요하다고 말했습니다.
- 로컬 개발자를 위한 OAuth 환경 변수 조사: 한 멤버가 Manus가 개발한 앱을 로컬에서 실행하기 위해 VITE_APP_ID, OAUTH_SERVER_URL, VITE_OAUTH_PORTAL_URL 환경 변수를 얻는 방법에 대한 지침을 찾고 있습니다.

그들은 또한 로컬 개발 중에 redirectUri http://localhost:3000/api/oauth/callback을 허용하기 위해 OAuth 구성이 필요한지 문의하고 있습니다.
- 계정 생성 후 즉시 밴 당한 사용자: 한 멤버가 계정을 생성한 직후 즉시 밴 당했다고 보고하며 이 문제를 해결하는 방법에 대한 조언을 구하고 있습니다.

어떤 조언도 주어지지 않았습니다.
- Manus 시스템, 쿠키 문제의 원인을 인프라 탓으로 돌려: 한 멤버가 사용자 지정 도메인(anointedforai.com)에서 쿠키 문제로 인해 Manus가 리디렉션 루프에 갇히는 상세한 문제를 공유했습니다.

Manus 자체는 이 문제를 인프라/호스팅 문제로 진단하고, 지원팀에 문의하거나 쿠키 설정에 대한 더 많은 제어 권한이 있는 플랫폼으로 Manus에서 마이그레이션할 것을 제안했습니다.
- 멤버, Manus가 만든 웹사이트에 불만 표출: 한 멤버가 자신의 웹사이트 디자인에 대해 Manus가 만든 '엉망진창'이라고 불평하며 수정에 대한 도움을 요청하고 있습니다.

다른 멤버가 다이렉트 메시지를 통해 도움을 주겠다고 제안했습니다.

---

### aider (Paul Gauthier) ▷ #general (8 messages🔥):
> Aider의 git 서브모듈, Aider를 위한 저비용 LLM, Aider를 위한 Deepseek V3.2, Aider를 위한 Xiaomi/mimo-v2-flash, Aider를 위한 moonshotai/kimi-k2.5
- Aider, 더 빠른 코드 변경을 위한 /ok 별칭 추가: Aider의 main 브랜치에 새로운 기능이 추가되었습니다. 이제 /ok는 '/code Ok, please go ahead and make those changes.'의 별칭으로, 더 빠른 코드 수정을 가능하게 합니다.
- 사용자, Aider를 위한 저비용 LLM 모색: 한 사용자가 Aider와 함께 사용할 최고의 저비용 LLM을 찾는 것에 대한 조언을 구하며, Gemini가 몇 시간 만에 모든 토큰을 소진했다고 언급했습니다.

다른 멤버는 OpenRouter를 사용하여 다른 모델들 사이를 전환할 것을 제안했습니다.
- Aider와 추론을 위한 Deepseek V3.2 추천: 한 사용자가 Deepseek V3.2를 Aider의 기본 LLM으로 추천했습니다. 이는 추론 능력이 좋고 저렴하지만, 때때로 약간 느릴 수 있기 때문입니다.
- Xiaomi/mimo-v2-flash, Aider에서 빠른 파일 편집에 탁월: Xiaomi/mimo-v2-flash는 Aider에서 퍼지 검색 및 교체 또는 내용 완성 같은 '단순한' 파일 편집 기능에 매우 저렴하고 빠르기 때문에 추천됩니다.
- moonshotai/kimi-k2.5, Aider에서 어려운 문제 해결: moonshotai/kimi-k2.5는 Aider에서 더 어려운 문제를 해결하기 위한 계획 모델로, mimo-v2-flash는 편집 모델로 제안됩니다.

---

### MLOps @Chipro ▷ #events (5 messages):
> WeAreDevelopers World Congress North America 2026, AI Control Hackathon 2026, Redwood Research, ControlArena 벤치마크 챌린지, ControlConf Berkeley
- WeAreDevelopers Congress, 북미에서 첫 선: WeAreDevelopers World Congress North America가 2026년 9월 23일부터 25일까지 캘리포니아 산호세에서 첫 선을 보이며, 10,000명 이상의 개발자와 500명 이상의 연사가 대규모 실제 엔지니어링에 초점을 맞출 예정입니다.

주요 주제로는 분산 시스템 확장, API 플랫폼, DevOps 등이 있으며, wearedevelopers.us에서 코드 Community_MLOps를 사용하여 10% 할인을 받을 수 있습니다.
- Apart Research, AI Control Hackathon 개최: Apart Research는 Redwood Research와 공동으로 2026년 3월 20일부터 22일까지 AI Control Hackathon을 개최합니다. 이 해커톤은 AI가 전복 시도에도 불구하고 우리가 원하는 대로 작동하도록 보장하는 시스템에 중점을 둡니다.

해커톤은 ControlArena 벤치마크 챌린지, 제어 프로토콜 설계, 레드 팀을 포함한 세 가지 트랙으로 진행되며, $2,000의 상금과 ControlConf 여행 기회가 제공됩니다.
- ControlConf 여행 상품 제공: AI Control Hackathon 1등 상은 ControlConf Berkeley (4월 18-19일)로 가는 전액 지원 여행(항공편 및 호텔 포함)입니다.

ControlConf에 대해 더 알아보세요.

---

### DSPy ▷ #general (2 messages):
> SF DSPy 밋업, 프로덕션에서의 DSPy, RLM, Dropbox, Shopify
- SF DSPy 밋업 임박: 이번에는 프로덕션 사용 사례와 RLM에서의 DSPy에 초점을 맞춘 또 다른 SF DSPy 밋업을 발표합니다.

Dropbox와 Shopify의 엔지니어들이 사례 연구를 공유할 예정이며, dspy.RLM에 대한 자세한 설명이 있을 것입니다. Luma 링크를 참조하세요.
- Dropbox 및 Shopify 엔지니어 발표 예정: Dropbox 및 Shopify 엔지니어들이 SF DSPy 밋업에서 사례 연구를 발표할 예정입니다.

이 밋업은 프로덕션 환경에서 DSPy와 RLM을 사용하는 데 중점을 둘 것입니다.

---

### tinygrad (George Hotz) ▷ #general (2 messages):
> JAX, 함수
- Tinygrad 개발자, JAX의 함수 설계 칭찬: tinygrad의 개발자 George Hotz는 JAX의 뛰어난 함수 설계를 인정하며, 이는 설계 선택에 있어 JAX의 영향력 또는 정확성을 암시합니다.

두 번째 트윗은 이 점을 더욱 강조합니다.
- JAX 함수 설계 칭찬: Tinygrad의 개발자는 JAX의 함수 설계 방식에 대한 감탄을 표했습니다.

이는 JAX의 방식이 Tinygrad의 유사한 선택에 대한 모델 또는 검증 역할을 할 수 있음을 시사합니다.

---

*이 문서는 news.smol.ai의 뉴스레터를 자동 번역한 것입니다.*
